{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Setup & LangSmith"
      ],
      "metadata": {
        "id": "mVdK-9dSuZWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1: Install Dependencies\n",
        "We need to install the libraries required for LangChain, OpenAI, vector DBs, PDF parsing, tokenization, and LangSmith (for tracing & debugging)."
      ],
      "metadata": {
        "id": "_HSKcn6Mucsw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI1UQ1TCtact",
        "outputId": "8e467783-faf4-4e90-9698-f786c4fbc195",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.23)\n",
            "Collecting langsmith\n",
            "  Downloading langsmith-0.4.25-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.104.2)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.24.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.4.25-py3-none-any.whl (379 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.4/379.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=dbfda3c4d5aaa995dcd593a2ec66b4bcf9cd2bdc80cdc78bdb736e84befd5f01\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, requests, pypdf, pybase64, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httptools, bcrypt, backoff, watchfiles, typing-inspect, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, onnxruntime, langsmith, kubernetes, dataclasses-json, opentelemetry-exporter-otlp-proto-grpc, langchain-openai, chromadb, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.4.23\n",
            "    Uninstalling langsmith-0.4.23:\n",
            "      Successfully uninstalled langsmith-0.4.23\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.20 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 langchain-community-0.3.29 langchain-openai-0.3.32 langsmith-0.4.25 marshmallow-3.26.1 mmh3-5.2.0 mypy-extensions-1.1.0 onnxruntime-1.22.1 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 posthog-5.4.0 pybase64-1.4.2 pypdf-6.0.0 pypika-0.48.9 requests-2.32.5 typing-inspect-0.9.0 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ],
      "source": [
        "# Install core libraries for LangChain, OpenAI, and LangSmith integration\n",
        "!pip install -U langchain langchain-openai langchain-community chromadb pypdf tiktoken langsmith python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2: Load Keys from Colab Environment\n",
        "We need to configure OpenAI (for embeddings + model) and LangSmith (for tracing & observability).\n",
        "You’ll need your OpenAI API key and optionally a LangSmith API key (if you want to see traces in your LangSmith dashboard)."
      ],
      "metadata": {
        "id": "6fjdE3_Nvp3D"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-QCNhgtuiV9"
      },
      "source": [
        "# Load API keys from Colab environment variables\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# These should already exist in Colab -> Settings -> Variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get(\"LANGSMITH_API_KEY\")\n",
        "\n",
        "# Enable LangSmith tracing (optional but useful for debugging)\n",
        "os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://eu.api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-exercise\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Connect to External Sources (Upload PDF)"
      ],
      "metadata": {
        "id": "PoyT-DDAwkoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1: Upload PDF to Colab\n",
        "This will open a file picker in Colab so you can upload your PDF."
      ],
      "metadata": {
        "id": "KsjVBfCdwzOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload a PDF directly into Colab's local storage\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Show uploaded file names\n",
        "list(uploaded.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "7JjzS6tHwxBH",
        "outputId": "fcb20777-1d4c-4c68-bd2f-6e9666658ad6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bba23cf7-c2ce-45ee-98e8-f60dda5f2a8f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bba23cf7-c2ce-45ee-98e8-f60dda5f2a8f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 22365_3_Prompt Engineering_v7.pdf to 22365_3_Prompt Engineering_v7.pdf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['22365_3_Prompt Engineering_v7.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2: Load & Preview PDF\n",
        "We’ll use PyPDFLoader from LangChain to extract the pages. Then we’ll just peek at the first chunk of text to make sure it worked."
      ],
      "metadata": {
        "id": "YooBTqjcxSdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the uploaded PDF using LangChain's PyPDFLoader\n",
        "# ---------------------------------------------------\n",
        "# PyPDFLoader takes a PDF file and extracts text from each page.\n",
        "# - Each page becomes a LangChain Document object.\n",
        "# - Document.page_content -> the text of that page\n",
        "# - Document.metadata -> info like page number, source file, etc.\n",
        "#\n",
        "# NOTE: PyPDFLoader only works on text-based PDFs.\n",
        "# If a page is image-only (like a scanned document), the page_content will be empty.\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Use the uploaded file (from the previous cell)\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "\n",
        "# Load the PDF -> returns a list of Document objects (one per page)\n",
        "pages = loader.load()\n",
        "\n",
        "print(f\"✅ Total pages loaded: {len(pages)}\\n\")\n",
        "print(\"Preview of first page content:\\n\")\n",
        "print(pages[0].page_content[:500])  # Show first 500 chars for sanity check\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J02mXAwYwXP2",
        "outputId": "ff0c8c2a-aa25-41fb-a544-37965c362938"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total pages loaded: 68\n",
            "\n",
            "Preview of first page content:\n",
            "\n",
            "Prompt  \n",
            "Engineering\n",
            "Author: Lee Boonstra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Processing (Chunking & Embeddings)\n"
      ],
      "metadata": {
        "id": "ZSCYzE0yyWcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1: Chunk the PDF text\n",
        "We’ll convert page-level Documents into smaller, overlapping chunks so retrieval works well.\n",
        "Using RecursiveCharacterTextSplitter keeps sentences/paragraphs as intact as possible while respecting size."
      ],
      "metadata": {
        "id": "BBTzV2s6zKT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk the loaded Documents into retrieval-friendly pieces\n",
        "# ---------------------------------------------------------\n",
        "# Why chunk? Smaller, overlapping chunks improve recall and reduce irrelevant context.\n",
        "# We use RecursiveCharacterTextSplitter which tries to split on sensible boundaries\n",
        "# (paragraphs, sentences, etc.) before falling back to hard character limits.\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,     # ~1–2 paragraphs (tune by your use-case)\n",
        "    chunk_overlap=200,   # overlap preserves context continuity across chunks\n",
        "    separators=[ \"\\n\\n\", \"\\n\", \". \", \" \", \"\" ]  # try larger boundaries first\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "print(f\"✅ Chunks created: {len(chunks)}\")\n",
        "print(\"Preview of first chunk:\\n\")\n",
        "print(chunks[0].page_content[:500])\n",
        "print(\"\\nMetadata example:\", chunks[0].metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml3XJ9VGyVjE",
        "outputId": "b99c6d8f-08dc-47c8-d7e5-1a455a13b453"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunks created: 107\n",
            "Preview of first chunk:\n",
            "\n",
            "Prompt  \n",
            "Engineering\n",
            "Author: Lee Boonstra\n",
            "\n",
            "Metadata example: {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': '22365_3_Prompt Engineering_v7.pdf', 'total_pages': 68, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2: Create Embeddings with OpenAI\n",
        "Embeddings = vector representations of text.\n",
        "They let us search & retrieve semantically (e.g., “find parts of the PDF about Chain of Thought”).\n",
        "We’ll use OpenAIEmbeddings from LangChain."
      ],
      "metadata": {
        "id": "KmltViBx0Fwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings for each chunk using OpenAI\n",
        "# ---------------------------------------------------------\n",
        "# OpenAIEmbeddings will take each text chunk and convert it into\n",
        "# a high-dimensional vector (list of floats).\n",
        "# These vectors capture semantic meaning, which makes similarity search possible.\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Generate embeddings for our chunks (happens automatically when we add them to a vector store later)\n",
        "sample_vector = embedding_model.embed_query(\"What is prompt engineering?\")\n",
        "\n",
        "print(f\"✅ Embedding vector length: {len(sample_vector)}\")\n",
        "print(f\"First 10 dimensions: {sample_vector[:10]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2dieDhD0Gdj",
        "outputId": "6987573b-0a65-461b-a8a7-55b367ecdfa3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Embedding vector length: 1536\n",
            "First 10 dimensions: [0.015054242685437202, -0.009864309802651405, -0.031565792858600616, 0.014215604402124882, -0.0163878146559, 0.036240167915821075, 0.0117753054946661, 0.050648245960474014, -0.015177976340055466, -0.007375891786068678]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Store in a Vector DB (Chroma)"
      ],
      "metadata": {
        "id": "WQj34MJO1KsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1: Create & Populate Chroma DB\n",
        "We’ll store all chunk embeddings inside Chroma.\n",
        "That way, later we can do semantic search like: “Explain Chain of Thought prompting” → and Chroma finds the most relevant chunks from the PDF."
      ],
      "metadata": {
        "id": "xftN0dmj1NY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store embeddings in a Chroma vector database\n",
        "# ---------------------------------------------------------\n",
        "# We create a Chroma DB and fill it with our chunked documents.\n",
        "# Each chunk gets embedded using OpenAI and stored with metadata.\n",
        "# Later, we can query this DB for relevant chunks (retrieval).\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,              # our chunked documents\n",
        "    embedding=embedding_model,     # OpenAI embeddings\n",
        "    persist_directory=None         # use in-memory DB (no files saved)\n",
        ")\n",
        "\n",
        "# Test the vector DB with a semantic search\n",
        "query = \"What is Chain of Thought prompting?\"\n",
        "results = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "print(\"🔍 Query:\", query)\n",
        "print(\"\\nTop 2 retrieved chunks:\\n\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"--- Chunk {i} ---\")\n",
        "    print(doc.page_content[:300], \"...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCEEuMJ-1M5c",
        "outputId": "c3093207-8ed4-4acd-e954-6fb952a47fd2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Query: What is Chain of Thought prompting?\n",
            "\n",
            "Top 2 retrieved chunks:\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Prompt Engineering\n",
            "February 2025\n",
            "29\n",
            "Chain of Thought (CoT)\n",
            "Chain of Thought (CoT) 9 prompting is a technique for improving the reasoning capabilities \n",
            "of LLMs by generating intermediate reasoning steps. This helps the LLM generate more \n",
            "accurate answers. You can combine it with few-shot prompting to ...\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Chain of thought can be useful for various use-cases. Think of code generation, for breaking \n",
            "down the request into a few steps, and mapping those to specific lines of code. Or for \n",
            "creating synthetic data when you have some kind of seed like “The product is called XYZ, \n",
            "write a description guiding  ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Set up the Model (OpenAI via LangChain)"
      ],
      "metadata": {
        "id": "wM3EbdS92AnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1: Initialize the Chat Model\n",
        "\n",
        "We’ll use ChatOpenAI from langchain_openai.\n",
        "Set sensible defaults (low temperature for factual answers; adjust later if you want more creativity)."
      ],
      "metadata": {
        "id": "eNF-ErkZ2CPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the OpenAI chat model for generation\n",
        "# ------------------------------------------------\n",
        "# ChatOpenAI is the LangChain wrapper around OpenAI chat models.\n",
        "# - temperature: lower = more deterministic, higher = more creative\n",
        "# - model: pick a cost-effective model for RAG-style Q&A\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",   # good balance of quality/cost; change if you prefer\n",
        "    temperature=0.2,       # we want precise answers over creativity for RAG\n",
        "    max_tokens=800,        # cap response length; tune per your needs\n",
        ")\n",
        "\n",
        "# quick sanity call (no RAG yet) to ensure the model is reachable\n",
        "resp = llm.invoke(\"Respond with the single word: ready\")\n",
        "print(\"Model check:\", resp.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jm0hWdY2BqL",
        "outputId": "e434b812-0c50-4a00-8e2f-390f3960eff3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model check: ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: Prompt Template & Instructions"
      ],
      "metadata": {
        "id": "JwckFbzl2arA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1: Define Prompt with Message Objects (no memory)\n",
        "\n",
        "We’ll build the system + human messages explicitly, then assemble the template.\n",
        "This prompt expects two inputs later: {context} (from Chroma) and {question} (your query)."
      ],
      "metadata": {
        "id": "BALVgGFG2eHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a chat prompt using templated message classes\n",
        "# ----------------------------------------------------\n",
        "# Why this change?\n",
        "# - SystemMessage/HumanMessage are \"static\" messages -> variables aren't tracked.\n",
        "# - *PromptTemplate* message classes register input variables so chains can validate them.\n",
        "#\n",
        "# Inputs the chain expects:\n",
        "#   - {context}: filled by the retriever (StuffDocumentsChain default variable name)\n",
        "#   - {question}: your user query\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "system_tmpl = (\n",
        "    \"You are an expert tutor on prompt engineering. \"\n",
        "    \"Answer using ONLY the provided context from the PDF. \"\n",
        "    \"If the answer isn't in the context, say you don't know. \"\n",
        "    \"Be concise and, when possible, cite the source page numbers like.\"\n",
        ")\n",
        "\n",
        "human_tmpl = (\n",
        "    \"Context:\\n{context}\\n\\n\"\n",
        "    \"Question: {question}\\n\\n\"\n",
        "    \"Answer:\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_tmpl),\n",
        "    HumanMessagePromptTemplate.from_template(human_tmpl),\n",
        "])\n",
        "\n",
        "# Sanity check: should list the registered variables\n",
        "print(\"✅ Prompt input variables:\", prompt.input_variables)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPQyb_Hc8BKp",
        "outputId": "508a375d-f9d6-4363-fa7b-75cc2b8bedad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Prompt input variables: ['context', 'question']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7: Get the Response (RAG Pipeline)"
      ],
      "metadata": {
        "id": "HO1uya6P3fKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1: Build Retrieval Chain (Vector DB → Prompt → LLM)\n",
        "\n",
        "We’ll connect:\n",
        "\n",
        "Retriever → pulls relevant chunks from Chroma.\n",
        "\n",
        "Prompt → injects chunks + user’s question.\n",
        "\n",
        "LLM → generates the final answer."
      ],
      "metadata": {
        "id": "VMlhs6_z3g0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a retrieval-based QA chain\n",
        "# ------------------------------------------------------------\n",
        "# RetrievalQA glues together:\n",
        "#   - retriever: pulls top-k chunks from the vector DB\n",
        "#   - prompt: template for system + human messages\n",
        "#   - llm: the OpenAI chat model\n",
        "#\n",
        "# IMPORTANT about variables:\n",
        "# - {context}: special variable automatically filled with retrieved chunks\n",
        "#              from the retriever (StuffDocumentsChain default).\n",
        "# - {question}: special variable filled with the user’s query.\n",
        "#\n",
        "# -> These MUST exist in your prompt (unless you rename them).\n",
        "#    If your template expects anything else (like {page}), it will error.\n",
        "#\n",
        "# chain_type:\n",
        "# - \"stuff\"      -> dump all chunks into {context} at once (simple, best for small k).\n",
        "# - \"map_reduce\" -> answer per chunk, then summarize (better for large docs).\n",
        "# - \"refine\"     -> iteratively refine an answer as chunks are processed (good for progressive detail).\n",
        "#\n",
        "# Note: RetrievalQA accepts input as {\"query\": \"...\"}.\n",
        "# Behind the scenes, \"query\" is mapped to {question} for the prompt.\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",   # try \"map_reduce\" or \"refine\" later\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")\n",
        "\n",
        "# Example query\n",
        "query = \"Explain the concept of Chain of Thought prompting.\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "print(\"🔍 Query:\", query)\n",
        "print(\"\\n💡 Answer:\\n\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkVTWjiK3lks",
        "outputId": "59658b62-094f-412f-a59f-ca56f26003f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Query: Explain the concept of Chain of Thought prompting.\n",
            "\n",
            "💡 Answer:\n",
            " {'query': 'Explain the concept of Chain of Thought prompting.', 'result': 'Chain of Thought (CoT) prompting is a technique that enhances the reasoning capabilities of large language models (LLMs) by generating intermediate reasoning steps. This approach helps the LLM produce more accurate answers and can be combined with few-shot prompting for better results on complex tasks that require reasoning. CoT is low-effort and effective, working well with off-the-shelf LLMs without the need for fine-tuning. It provides interpretability, allowing users to learn from the LLM’s responses and identify any malfunctions. Additionally, CoT improves robustness across different LLM versions, leading to more consistent performance. It is particularly useful for tasks that can be solved by \"talking through\" the problem, such as code generation or creating synthetic data (source page 29).'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7B: Conversation History (Memory)"
      ],
      "metadata": {
        "id": "B1bAU7Yd9wNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7B.1 — Adjust the Prompt for Conversational History\n",
        "\n",
        "Right now your prompt expects {context} and {question}.\n",
        "With conversation, we need one extra input: {chat_history} (the previous turns)."
      ],
      "metadata": {
        "id": "wPsoKGno_ek7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt with MessagesPlaceholder for conversation history\n",
        "# --------------------------------------------------------\n",
        "# Why this is better:\n",
        "# - chat_history is injected as a proper message list\n",
        "# - preserves roles (user vs assistant) for each turn\n",
        "# - avoids flattening history into one string\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "system_tmpl = (\n",
        "    \"You are an expert tutor on prompt engineering. \"\n",
        "    \"Use ONLY the provided context from the PDF + chat history. \"\n",
        "    \"If the answer isn't in the context, say you don't know. \"\n",
        "    \"Be concise and, when possible, cite page numbers.\"\n",
        ")\n",
        "\n",
        "human_tmpl = (\n",
        "    \"Chat history:\\n{chat_history}\\n\\n\"   # <-- plain string injection\n",
        "    \"Context:\\n{context}\\n\\n\"\n",
        "    \"Question: {question}\\n\\n\"\n",
        "    \"Answer:\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_tmpl),\n",
        "    HumanMessagePromptTemplate.from_template(human_tmpl),\n",
        "])\n",
        "\n",
        "print(\"✅ Prompt variables:\", prompt.input_variables)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uuzu9h768Y2r",
        "outputId": "0108cbfa-8b5a-4c95-b935-2e160037b35e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Prompt variables: ['chat_history', 'context', 'question']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7B.2 — Set up Conversation Buffer Memory"
      ],
      "metadata": {
        "id": "kyOMiWZyDAQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ConversationBufferMemory setup\n",
        "# ------------------------------------------------------------\n",
        "# - Stores a running list of HumanMessage/AIMessage objects\n",
        "# - Matches the prompt's MessagesPlaceholder(variable_name=\"chat_history\")\n",
        "# - return_messages=True ensures history is injected as structured messages\n",
        "# - output_key=\"answer\" aligns with the chain’s output field\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",  # must match MessagesPlaceholder name\n",
        "    return_messages=True,       # keep role-based messages\n",
        "    output_key=\"answer\"         # capture assistant answers into memory\n",
        ")\n",
        "\n",
        "print(\"✅ ConversationBufferMemory ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh8pk9TTDAqw",
        "outputId": "5b8b903d-f7f4-4943-a420-be9d1cfce5fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ConversationBufferMemory ready.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3560500356.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7B.3 — Build the Conversational Retrieval Chain"
      ],
      "metadata": {
        "id": "XvrVPPrIDutZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ConversationalRetrievalChain with memory\n",
        "# ------------------------------------------------------------\n",
        "# This chain is like RetrievalQA but with chat history awareness.\n",
        "# - Retriever fetches top-k chunks from the vector DB\n",
        "# - Memory fills the {chat_history} slot in your prompt\n",
        "# - Prompt ensures the model uses both context + conversation\n",
        "#\n",
        "# Key params:\n",
        "# - search_kwargs={\"k\": 3} : top 3 chunks retrieved each turn\n",
        "# - combine_docs_chain_kwargs={\"prompt\": prompt} : injects your custom system/human template\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "conv_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,   # our ConversationBufferMemory\n",
        "    combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        ")\n",
        "\n",
        "print(\"✅ ConversationalRetrievalChain ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAuEX1tgDC83",
        "outputId": "6c1f626b-d937-433c-ced7-d222cee35f9a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ConversationalRetrievalChain ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7B.4 — Ask a question + a follow-up"
      ],
      "metadata": {
        "id": "BtCYdYZdERox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo: conversational Q&A with memory\n",
        "# ------------------------------------------------------------\n",
        "# First we ask a full question.\n",
        "# Then we ask a follow-up that ONLY makes sense if the model sees chat_history.\n",
        "\n",
        "# Turn 1\n",
        "q1 = \"List the main prompting techniques covered in the PDF.\"\n",
        "a1 = conv_chain.invoke({\"question\": q1})[\"answer\"]\n",
        "\n",
        "# Turn 2 (follow-up relies on history)\n",
        "q2 = \"And what about CoT?\"\n",
        "a2 = conv_chain.invoke({\"question\": q2})[\"answer\"]\n",
        "\n",
        "print(\"Q1:\", q1, \"\\nA1:\\n\", a1, \"\\n\")\n",
        "print(\"Q2:\", q2, \"\\nA2:\\n\", a2, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpEVW59hEUVe",
        "outputId": "f8dc9535-911a-49ab-dee0-0145320d96e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: List the main prompting techniques covered in the PDF. \n",
            "A1:\n",
            " The main prompting techniques covered in the PDF are:\n",
            "\n",
            "1. General prompting / zero shot\n",
            "2. One-shot & few-shot prompting\n",
            "3. System prompting\n",
            "4. Contextual prompting\n",
            "5. Role prompting\n",
            "6. Step-back prompting\n",
            "7. Chain of Thought (CoT)\n",
            "8. Self-consistency\n",
            "9. Tree of Thoughts (ToT)\n",
            "10. ReAct (reason & act)\n",
            "11. Automatic Prompt Engineering\n",
            "12. Code prompting (including prompts for writing, explaining, translating, debugging, and reviewing code) \n",
            "\n",
            "These techniques are discussed in various sections of the document, particularly from pages 13 to 48. \n",
            "\n",
            "Q2: And what about CoT? \n",
            "A2:\n",
            " Chain of Thought (CoT) prompting is a technique for improving the reasoning capabilities of large language models (LLMs) by generating intermediate reasoning steps. This approach helps the LLM produce more accurate answers by breaking down the problem into smaller, manageable steps. CoT can be particularly effective when combined with few-shot prompting for complex tasks that require reasoning before responding, as it addresses challenges associated with zero-shot prompting. \n",
            "\n",
            "Advantages of CoT include its low-effort implementation, effectiveness with off-the-shelf LLMs (without the need for fine-tuning), and enhanced interpretability, allowing users to learn from the LLM's responses and identify any reasoning errors. Additionally, CoT improves robustness across different LLM versions, leading to more consistent performance. However, it does result in longer outputs due to the inclusion of reasoning steps (pages 29-36). \n",
            "\n"
          ]
        }
      ]
    }
  ]
}