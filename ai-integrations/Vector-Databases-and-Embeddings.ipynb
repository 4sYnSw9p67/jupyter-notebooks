{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "135904f6"
      },
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "# Make sure to add your API key to Colab secrets under the name 'OPENAI_API_KEY'\n",
        "# For more info, see https://colab.research.google.com/notebooks/integrations.ipynb\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "# This client is used to interact with the OpenAI API\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63c0c207"
      },
      "source": [
        "# 1. Types of Memory\n",
        "\n",
        "1.  **Contextual (Short-Term) Memory:** Keeps conversations coherent in a single session.\n",
        "2.  **Long-Term (Persistent) Memory:** Remembers facts and preferences across sessions.\n",
        "3.  **Semantic Memory:** The model’s built-in factual and conceptual knowledge.\n",
        "4.  **Working Memory:** Handles multi-step reasoning and problem-solving within a single query.\n",
        "5.  **External / Tool-Augmented Memory (RAG):** Extends the model’s capacity by retrieving info from external sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "017713d2"
      },
      "source": [
        "## 1. Contextual (Short-Term) Memory\n",
        "\n",
        "Keeps conversations coherent in a single session by remembering recent interactions. This type of memory is crucial for maintaining flow and understanding within a limited timeframe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: First message to set context\n",
        "# This message introduces the user and sets the initial context for the conversation.\n",
        "response1 = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",  # Small, fast model for demo\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"My name is Alex.\"}\n",
        "    ]\n",
        ")\n",
        "print(response1.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5--tBgy9LP3",
        "outputId": "5c4213c5-4993-408d-bb99-0e934d281d76"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, Alex! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Second message reuses context (conversation continues)\n",
        "# By including the previous messages, the model remembers the user's name from the first interaction.\n",
        "response2 = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        # Include the previous messages to maintain context\n",
        "        {\"role\": \"assistant\", \"content\": response1.choices[0].message.content},\n",
        "        {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
        "    ]\n",
        ")\n",
        "print(response2.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtTdS4WN9XZm",
        "outputId": "ab30ad02-cd11-4e71-b977-02f5ba624516"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You mentioned your name is Alex. How can I help you today, Alex?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c01c4566"
      },
      "source": [
        "## 2. Long-Term (Persistent) Memory\n",
        "\n",
        "Remembers facts and preferences across sessions, allowing the model to maintain consistency and personalization over extended periods."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persistent file for memory\n",
        "MEMORY_FILE = \"memory.txt\"\n",
        "\n",
        "def save_memory(data):\n",
        "    with open(MEMORY_FILE, \"w\") as f:\n",
        "        f.write(data)\n",
        "\n",
        "def load_memory():\n",
        "    if os.path.exists(MEMORY_FILE):\n",
        "        with open(MEMORY_FILE, \"r\") as f:\n",
        "            return f.read().strip()\n",
        "    return None\n",
        "\n",
        "# Step 1: User gives info\n",
        "user_message = \"My name is Alex\"\n",
        "\n",
        "# Step 2: Ask GPT-4 if this should be remembered\n",
        "decision = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"system\", \"content\": \"Decide if the user's message contains important facts to store such as name, preferences and so on\"},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Model decision:\", decision.choices[0].message.content)\n",
        "\n",
        "# Step 3: If model says it's important, trigger 'store_memory'\n",
        "if \"remember\" in decision.choices[0].message.content.lower():\n",
        "    save_memory(user_message)\n",
        "    print(\"Stored in memory:\", user_message)\n",
        "\n",
        "# Step 4: Later, retrieve memory and use it in a new conversation\n",
        "stored_info = load_memory()\n",
        "if stored_info:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Previously you learned: {stored_info}. Now greet me using that info.\"}\n",
        "        ]\n",
        "    )\n",
        "    print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_WQwnZr9piA",
        "outputId": "5b713ffe-099f-4371-8591-993da4438d9b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model decision: Storing this information: User's name is Alex.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple LLM Long Memory System\n",
        "- Stores user facts in JSON file\n",
        "- Uses OpenAI function calling to save/recall info\n",
        "- Perfect for Google Colab experiments\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "MEMORY_FILE = \"memory.json\"  # Where memories are stored\n",
        "\n",
        "def save_memory(key: str, value: str):\n",
        "    \"\"\"Store a key-value pair in memory file\"\"\"\n",
        "    print(\"Memory updating...\")\n",
        "    memory = {}\n",
        "    if os.path.exists(MEMORY_FILE):\n",
        "        with open(MEMORY_FILE, \"r\") as f:\n",
        "            memory = json.load(f)\n",
        "\n",
        "    memory[key] = value\n",
        "    with open(MEMORY_FILE, \"w\") as f:\n",
        "        json.dump(memory, f, indent=2)\n",
        "    return \"Stored successfully\"\n",
        "\n",
        "def load_memory(query: str):\n",
        "    \"\"\"Search for stored memories matching the query\"\"\"\n",
        "    print(\"Reading memory...\")\n",
        "    if not os.path.exists(MEMORY_FILE):\n",
        "        return \"No memories stored\"\n",
        "\n",
        "    with open(MEMORY_FILE, \"r\") as f:\n",
        "        memory = json.load(f)\n",
        "\n",
        "    if not memory:\n",
        "        return \"No memories stored\"\n",
        "\n",
        "    # Search for relevant info\n",
        "    # query_lower = query.lower()\n",
        "    # relevant = {k: v for k, v in memory.items()\n",
        "    #            if query_lower in k.lower() or query_lower in str(v).lower()}\n",
        "\n",
        "    # if relevant:\n",
        "    #     return json.dumps(relevant)\n",
        "    # else:\n",
        "    #     return f\"No memory found for: {query}\"\n",
        "    return memory # Return the entire memory dictionary\n",
        "\n",
        "# Function tools for the AI to call\n",
        "\n",
        "# Tool definitions\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"store_fact\",\n",
        "            \"description\": \"Store user info like name, age, etc.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"key\": {\"type\": \"string\"},\n",
        "                    \"value\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"key\", \"value\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"recall_fact\",\n",
        "            \"description\": \"Get stored user info\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"query\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def chat_with_memory(user_input: str):\n",
        "    \"\"\"Main function that handles conversation with memory capabilities\"\"\"\n",
        "    client = OpenAI()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You have memory tools. When user shares personal info, store it. When they ask about themselves, recall it first.\"},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "\n",
        "    # First API call - may trigger tool usage\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Handle tool calls if any\n",
        "    if response.choices[0].message.tool_calls:\n",
        "        messages.append(response.choices[0].message)\n",
        "\n",
        "        for tool_call in response.choices[0].message.tool_calls:\n",
        "            args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "            # Execute the appropriate tool\n",
        "            if tool_call.function.name == \"store_fact\":\n",
        "                result = save_memory(args[\"key\"], args[\"value\"])\n",
        "            elif tool_call.function.name == \"recall_fact\":\n",
        "                result = load_memory(args[\"query\"])\n",
        "\n",
        "            messages.append({\n",
        "                \"role\": \"tool\",\n",
        "                \"tool_call_id\": tool_call.id,\n",
        "                \"content\": str(result) # Ensure the content is a string\n",
        "            })\n",
        "\n",
        "        # Second API call with tool results\n",
        "        final_response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=messages\n",
        "        )\n",
        "\n",
        "\n",
        "        return final_response.choices[0].message.content\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# ==================== DEMO ====================\n",
        "\n",
        "\n",
        "\n",
        "user_text = input(\"\\n💬 You: \")\n",
        "\n",
        "print(\"\\n🤖 Assistant:\")\n",
        "response = chat_with_memory(user_text)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOVQJJzo-u0d",
        "outputId": "d3e59187-6285-4310-d927-4431f71ec447"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💬 You: how old Am i\n",
            "\n",
            "🤖 Assistant:\n",
            "Reading memory...\n",
            "You are 54 years old, Jo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ee559c"
      },
      "source": [
        "## 3. Semantic Memory\n",
        "\n",
        "The model’s built-in factual and conceptual knowledge, enabling it to understand and generate human-like text based on its training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef87cced",
        "outputId": "da60ed49-add8-45bb-e357-550c2f4cb321"
      },
      "source": [
        "# Define the input message for the model\n",
        "input_message = \"Write a short scientific article about nature.\" # Using the original prompt content\n",
        "\n",
        "# Create a response request using the responses endpoint\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",  # Using gpt-4o-mini\n",
        "    input=input_message,\n",
        "    max_output_tokens=100\n",
        ")\n",
        "\n",
        "# Extract and print the model's output text\n",
        "print(response.output_text)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**The Importance of Biodiversity in Ecosystem Health**\n",
            "\n",
            "**Abstract:**  \n",
            "Biodiversity, the variety of life forms within a given ecosystem, is crucial for maintaining ecological balance and resilience. This article explores the importance of biodiversity, the threats it faces, and the impact on ecosystem health and human well-being.\n",
            "\n",
            "**Introduction:**  \n",
            "Nature is a complex web of interconnected organisms, functioning collectively to sustain life on Earth. Biodiversity includes not only the variety of species but also genetic diversity within those species and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66cb3ab6"
      },
      "source": [
        "# 2. Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8047a247"
      },
      "source": [
        "## 1. Vectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_vector(n):\n",
        "    \"\"\"Generate a vector with n dimensions filled with random values\"\"\"\n",
        "    return np.random.rand(n)\n",
        "\n",
        "# Example usage\n",
        "number = 110\n",
        "vector = generate_vector(number)\n",
        "\n",
        "print(f\"Input: {number}\")\n",
        "print(f\"Vector: {vector}\")\n",
        "print(f\"Shape: {vector.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4PPX3lgFJlI",
        "outputId": "4ab92837-c940-458f-f90c-5ffa742d2082"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 110\n",
            "Vector: [0.38671283 0.41435042 0.11562326 0.68164822 0.82984494 0.34270913\n",
            " 0.95964889 0.47047841 0.55288474 0.1822974  0.43532998 0.27548134\n",
            " 0.06987586 0.68181388 0.64756457 0.18136804 0.88466798 0.19942417\n",
            " 0.0385368  0.4312381  0.55856187 0.55743958 0.65924505 0.68006215\n",
            " 0.75321626 0.72458764 0.36890083 0.31565361 0.02460112 0.25667288\n",
            " 0.0797472  0.65190499 0.69607954 0.65230246 0.03675547 0.34324064\n",
            " 0.41411347 0.60966287 0.64602818 0.50818257 0.35868865 0.33985861\n",
            " 0.45761426 0.19375051 0.2105351  0.97093743 0.2858085  0.33831274\n",
            " 0.76760505 0.83376719 0.17297143 0.42487354 0.8292875  0.41171761\n",
            " 0.30674687 0.97826775 0.31216793 0.41253795 0.94441928 0.65674818\n",
            " 0.62890189 0.81527392 0.12294385 0.90380627 0.50398009 0.0365338\n",
            " 0.80075509 0.5236378  0.50095102 0.21214549 0.9015869  0.50406753\n",
            " 0.93513715 0.31088903 0.94677817 0.51617887 0.03564318 0.94079765\n",
            " 0.68655681 0.91905829 0.26299852 0.01076258 0.43750913 0.80323554\n",
            " 0.78465948 0.8171652  0.2857702  0.96391063 0.80828321 0.79674946\n",
            " 0.61742542 0.38260738 0.73952809 0.99944197 0.65481777 0.60782843\n",
            " 0.89856648 0.61070359 0.86574897 0.59004652 0.82319781 0.57649114\n",
            " 0.05900189 0.99473027 0.97515644 0.76826047 0.19040446 0.61838086\n",
            " 0.94964598 0.99329094]\n",
            "Shape: (110,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate Embeddings"
      ],
      "metadata": {
        "id": "kNphuftXGYFY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f394ae07",
        "outputId": "30c32d5a-277e-42c1-fc55-e1d499864b25"
      },
      "source": [
        "# Cell 1: Generate Embeddings\n",
        "# This code demonstrates how to generate embeddings for text using the OpenAI API.\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"\n",
        "    Generate embeddings for text using OpenAI API\n",
        "    \"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",  # A common and efficient embedding model\n",
        "        input=text,\n",
        "        encoding_format=\"float\"  # Specify the format of the embedding vector\n",
        "    )\n",
        "\n",
        "    # The embedding vector is in the 'data' attribute of the response\n",
        "    # It's a list of embedding objects, we take the first one's embedding\n",
        "    embedding_vector = response.data[0].embedding\n",
        "    return embedding_vector\n",
        "\n",
        "# Example usage\n",
        "text_to_embed = \"This is a sample sentence for creating an embedding.\"\n",
        "\n",
        "embedding_vector = get_embedding(text_to_embed)\n",
        "\n",
        "# Print the embedding vector and its dimension\n",
        "print(\"Text:\", text_to_embed)\n",
        "print(\"Embedding vector (first 10 elements):\", embedding_vector[:10])\n",
        "print(\"Dimension of the embedding vector:\", len(embedding_vector))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This is a sample sentence for creating an embedding.\n",
            "Embedding vector (first 10 elements): [0.03280215, 0.011288634, 0.02859873, 0.0051769116, -0.003304069, -0.03293109, 0.03349842, -0.016040046, -0.024150325, 0.019882435]\n",
            "Dimension of the embedding vector: 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Compare Embeddings"
      ],
      "metadata": {
        "id": "jraxLZkWGdkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Compare Embeddings\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Calculate cosine similarity between two vectors\n",
        "    \"\"\"\n",
        "    v1 = np.array(vec1)\n",
        "    v2 = np.array(vec2)\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "# Example texts to compare\n",
        "text1 = \"How to train a cat?\"\n",
        "text2 = \"How to drive a car?\"\n",
        "\n",
        "# Get embeddings for both texts\n",
        "embedding1 = get_embedding(text1)\n",
        "embedding2 = get_embedding(text2)\n",
        "\n",
        "# Calculate similarity\n",
        "similarity = cosine_similarity(embedding1, embedding2)\n",
        "\n",
        "print(f\"Text 1: {text1}\")\n",
        "print(f\"Text 2: {text2}\")\n",
        "print(f\"Similarity: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OShBoF8UGgx9",
        "outputId": "eff704d9-c6c9-497f-bc55-50c55e146e5b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: How to train a cat?\n",
            "Text 2: How to drive a car?\n",
            "Similarity: 0.3499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "958e57db"
      },
      "source": [
        "# 3. Vector Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3939a3df"
      },
      "source": [
        "## 1. Chroma DB\n",
        "\n",
        "Solo dev / small app / on-prem or hybrid → start with Chroma (OSS, dead-simple API; can move to Chroma Cloud later). Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Basic Setup"
      ],
      "metadata": {
        "id": "bgIqDKPCMYGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup\n",
        "!pip install chromadb\n",
        "\n",
        "import chromadb\n",
        "\n",
        "chroma = chromadb.Client()\n",
        "collection = chroma.get_or_create_collection(\"demo\")\n",
        "\n",
        "def get_embedding(text):\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=text\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "print(\"✅ Ready!\")"
      ],
      "metadata": {
        "id": "DEEltyQzMSdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Store Embeddings\n",
        "\n",
        "texts = [\n",
        "    \"How do I cook pasta?\",\n",
        "    \"The weather is sunny\",\n",
        "    \"AI is amazing\"\n",
        "]\n",
        "\n",
        "print(\"Storing embeddings...\")\n",
        "\n",
        "embeddings = [get_embedding(text) for text in texts]\n",
        "\n",
        "collection.add(\n",
        "    ids=[\"1\", \"2\", \"3\"],\n",
        "    documents=texts,\n",
        "    embeddings=embeddings\n",
        ")\n",
        "\n",
        "print(f\"✅ Stored {len(texts)} documents!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8odlvQV7NY3-",
        "outputId": "e3b92979-09bd-4490-e767-89ab79392b45"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Storing embeddings...\n",
            "✅ Stored 3 documents!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: View Stored Data\n",
        "\n",
        "# Show what's stored\n",
        "# Explicitly include embeddings, documents, and metadatas\n",
        "data = collection.get(\n",
        "    include=['embeddings', 'documents', 'metadatas']\n",
        ")\n",
        "\n",
        "ids = data[\"ids\"]\n",
        "docs = data[\"documents\"]\n",
        "embs = data.get(\"embeddings\")\n",
        "\n",
        "\n",
        "print(f\"Found {len(ids)} stored items:\\n\")\n",
        "\n",
        "# If for some reason embeddings are missing, don't try to iterate over them\n",
        "if embs is None or any(e is None for e in embs):\n",
        "    for idx, doc in zip(ids, docs):\n",
        "        print(f\"ID: {idx}\")\n",
        "        print(f\"Text: {doc}\")\n",
        "        print(\"(no embeddings returned)\")\n",
        "        print(\"-\" * 40)\n",
        "else:\n",
        "    for idx, doc, emb in zip(ids, docs, embs):\n",
        "        print(f\"ID: {idx}\")\n",
        "        print(f\"Text: {doc}\")\n",
        "        print(f\"Embedding (first 5 dims): {emb[:5]}\")\n",
        "        print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac25lPC0NxIa",
        "outputId": "b2a535a4-be2f-4099-938d-0d25cf678342"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 stored items:\n",
            "\n",
            "ID: 1\n",
            "Text: How do I cook pasta?\n",
            "Embedding (first 5 dims): [-0.06047147 -0.06067772 -0.01197261 -0.02462583  0.00011247]\n",
            "----------------------------------------\n",
            "ID: 2\n",
            "Text: The weather is sunny\n",
            "Embedding (first 5 dims): [ 0.00202606 -0.04834312  0.00734349 -0.00663184  0.02571756]\n",
            "----------------------------------------\n",
            "ID: 3\n",
            "Text: AI is amazing\n",
            "Embedding (first 5 dims): [-0.00992907 -0.00132574 -0.01422693  0.04163846  0.00857541]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. RAG"
      ],
      "metadata": {
        "id": "665K1g2btt01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Set up Chroma + OpenAI API"
      ],
      "metadata": {
        "id": "-luJDw-3umQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "\n",
        "# Create (or get) a collection to hold our chunks\n",
        "collection = chroma.get_or_create_collection(name=\"rag_demo\")\n",
        "\n",
        "print(\"✅ Setup complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77PSNIaXtwF3",
        "outputId": "2f7e0bfa-fc87-4ce7-c36e-4f92ae603326"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare and chunk a text"
      ],
      "metadata": {
        "id": "3vTOqa10vGim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A small sample text (replace with your own)\n",
        "text = \"\"\"\n",
        "Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our\n",
        "\n",
        "\n",
        "\n",
        "This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.\n",
        "\"\"\"\n",
        "\n",
        "# Simple sentence-based chunking (groups of ~2–3 sentences)\n",
        "import re\n",
        "\n",
        "def chunk_text(t: str, max_sentences=1):\n",
        "    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', t.strip()) if s.strip()]\n",
        "    chunks, buf = [], []\n",
        "    for s in sents:\n",
        "        buf.append(s)\n",
        "        if len(buf) >= max_sentences:\n",
        "            chunks.append(\" \".join(buf))\n",
        "            buf = []\n",
        "    if buf:\n",
        "        chunks.append(\" \".join(buf))\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(text, max_sentences=2)\n",
        "for i, c in enumerate(chunks):\n",
        "    print(f\"[{i}] {c}\\n\")\n",
        "\n",
        "print(f\"✅ Prepared {len(chunks)} chunks.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9G7mfezvJet",
        "outputId": "f075931c-6fbb-49fc-fb50-3df5223878db"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[1] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[2] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[3] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[4] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[5] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[6] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[7] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[8] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[9] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[10] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[11] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[12] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[13] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[14] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[15] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[16] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[17] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[18] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[19] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[20] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[21] This improves accuracy and keeps responses up-to-date with our data.\n",
            "\n",
            "✅ Prepared 22 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Turn chunks into embeddings"
      ],
      "metadata": {
        "id": "Z_DPCViUvRIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_MODEL = \"text-embedding-3-small\"  # 1536-dim, fast & inexpensive\n",
        "\n",
        "def embed_texts(text_list):\n",
        "    resp = client.embeddings.create(model=EMBED_MODEL, input=text_list)\n",
        "    return [d.embedding for d in resp.data]\n",
        "\n",
        "embeddings = embed_texts(chunks)\n",
        "\n",
        "print(f\"✅ Got {len(embeddings)} embeddings.\")\n",
        "print(f\"   Each vector has dimension: {len(embeddings[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0g0hcTbvLEH",
        "outputId": "07acdc19-c0f6-464b-8c0d-70f90e59da49"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Got 22 embeddings.\n",
            "   Each vector has dimension: 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.  Store in the vector DB (Chroma)"
      ],
      "metadata": {
        "id": "TPKMKUGBvmK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use string IDs \"0\", \"1\", ...\n",
        "ids = [str(i) for i in range(len(chunks))]\n",
        "metadatas = [{\"source\": \"demo\", \"chunk_id\": i} for i in range(len(chunks))]\n",
        "\n",
        "collection.add(\n",
        "    ids=ids,\n",
        "    documents=chunks,\n",
        "    embeddings=embeddings,\n",
        "    metadatas=metadatas\n",
        ")\n",
        "\n",
        "# Peek at what we stored (without dumping full vectors)\n",
        "data = collection.get(include=[\"documents\", \"metadatas\", \"embeddings\"])  # add \"embeddings\" if you want them\n",
        "\n",
        "print(\"✅ Stored items:\")\n",
        "for i, doc, emb, meta in zip(data[\"ids\"], data[\"documents\"],data['embeddings'], data[\"metadatas\"]):\n",
        "    print(f\"- id={i}, meta={meta}\\n  {doc}\\n {emb}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoJnaE9CvnOe",
        "outputId": "e9d370e8-3ae2-42b6-cb55-79bf1366ab53"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Stored items:\n",
            "- id=0, meta={'source': 'demo', 'chunk_id': 0}\n",
            "  Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01603832  0.02678413  0.03635518 ... -0.00518432  0.02789704\n",
            " -0.00210372]\n",
            "\n",
            "- id=1, meta={'chunk_id': 1, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.\n",
            " [-0.00189765  0.02580518  0.07134885 ... -0.00433957  0.00373362\n",
            "  0.01648745]\n",
            "\n",
            "- id=2, meta={'source': 'demo', 'chunk_id': 2}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=3, meta={'chunk_id': 3, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=4, meta={'chunk_id': 4, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=5, meta={'chunk_id': 5, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=6, meta={'source': 'demo', 'chunk_id': 6}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=7, meta={'source': 'demo', 'chunk_id': 7}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=8, meta={'source': 'demo', 'chunk_id': 8}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=9, meta={'chunk_id': 9, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=10, meta={'source': 'demo', 'chunk_id': 10}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=11, meta={'source': 'demo', 'chunk_id': 11}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=12, meta={'chunk_id': 12, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=13, meta={'source': 'demo', 'chunk_id': 13}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01393001  0.03513355  0.05262155 ... -0.00088458  0.02453834\n",
            "  0.00123332]\n",
            "\n",
            "- id=14, meta={'chunk_id': 14, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01393001  0.03513355  0.05262155 ... -0.00088458  0.02453834\n",
            "  0.00123332]\n",
            "\n",
            "- id=15, meta={'chunk_id': 15, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=16, meta={'chunk_id': 16, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=17, meta={'chunk_id': 17, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=18, meta={'chunk_id': 18, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01393001  0.03513355  0.05262155 ... -0.00088458  0.02453834\n",
            "  0.00123332]\n",
            "\n",
            "- id=19, meta={'source': 'demo', 'chunk_id': 19}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01393001  0.03513355  0.05262155 ... -0.00088458  0.02453834\n",
            "  0.00123332]\n",
            "\n",
            "- id=20, meta={'chunk_id': 20, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=21, meta={'source': 'demo', 'chunk_id': 21}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.\n",
            " [-0.00187998  0.0258261   0.07136656 ... -0.00434427  0.00373455\n",
            "  0.01652057]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display the data"
      ],
      "metadata": {
        "id": "wUy8oK0QxNVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Rich once in Colab\n",
        "!pip -q install rich\n",
        "\n",
        "data = collection.get(include=[\"documents\", \"metadatas\", \"embeddings\"])\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "import json\n",
        "\n",
        "def fmt_text(s, maxlen=120):\n",
        "    s = s or \"\"\n",
        "    return (s[:maxlen] + \"…\") if len(s) > maxlen else s\n",
        "\n",
        "def fmt_emb(e, preview=8):\n",
        "    if e is None:\n",
        "        return \"—\"\n",
        "    head = \", \".join(f\"{x:.4f}\" for x in e[:preview])\n",
        "    tail = \" …\" if len(e) > preview else \"\"\n",
        "    return f\"[{head}{tail}]  (dim={len(e)})\"\n",
        "\n",
        "table = Table(show_header=True, header_style=\"bold magenta\")\n",
        "table.add_column(\"ID\", style=\"cyan\", no_wrap=True)\n",
        "table.add_column(\"Text\", style=\"white\")\n",
        "table.add_column(\"Embedding (preview)\", style=\"green\")\n",
        "table.add_column(\"Metadata\", style=\"yellow\")\n",
        "\n",
        "for i, doc, emb, meta in zip(data[\"ids\"], data[\"documents\"], data[\"embeddings\"], data[\"metadatas\"]):\n",
        "    table.add_row(str(i), fmt_text(doc), fmt_emb(emb, 8), json.dumps(meta, ensure_ascii=False))\n",
        "\n",
        "Console().print(\"Stored items:\", style=\"bold\")\n",
        "Console().print(table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P7l__Z_-xMhf",
        "outputId": "181bda42-7f81-4d2b-dd7c-5d03f365a1c6"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mStored items:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Stored items:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35mID\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mText                             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mEmbedding (preview)               \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMetadata                         \u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m0 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mRetrieval-Augmented Generation   \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0160, 0.0268, 0.0364, 0.0264, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 0, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37m(RAG) helps language models      \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0171, 0.0024, -0.0029, 0.0235  \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37manswer questions by looking up   \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m…]  (dim=1536)                    \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37msupporting information from a …  \u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m1 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0019, 0.0258, 0.0713, 0.0449, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 1, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0035, 0.0076, 0.0031, 0.0305 …] \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.                            \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m2 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 2, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m3 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\": 3}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m4 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\": 4}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m5 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\": 5}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m6 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 6, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m7 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\": 7}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m8 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 8, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m9 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 9, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m10\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m10}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m11\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m11}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m12\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 12, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m13\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0139, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 13, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0176, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m14\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0139, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0176, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m14}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m15\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m15}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m16\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 16, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m17\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m17}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m18\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0139, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 18, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0176, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m19\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0139, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 19, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0176, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m20\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 20, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m21\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0019, 0.0258, 0.0714, 0.0449, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0035, 0.0076, 0.0031, 0.0305 …] \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m21}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.                            \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "└────┴───────────────────────────────────┴────────────────────────────────────┴───────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> ID </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Text                              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Embedding (preview)                </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Metadata                          </span>┃\n",
              "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 0  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Retrieval-Augmented Generation    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0160, 0.0268, 0.0364, 0.0264,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 0, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> (RAG) helps language models       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0171, 0.0024, -0.0029, 0.0235   </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> answer questions by looking up    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> …]  (dim=1536)                     </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> supporting information from a …   </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 1  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0019, 0.0258, 0.0713, 0.0449,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 1, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0035, 0.0076, 0.0031, 0.0305 …]  </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.                             </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 2  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 2, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 3  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\": 3} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 4  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\": 4} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 5  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\": 5} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 6  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 6, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 7  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\": 7} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 8  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 8, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 9  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 9, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 10 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 10}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 11 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 11}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 12 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 12, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 13 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0139, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 13, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0176, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 14 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0139, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0176, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 14}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 15 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 15}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 16 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 16, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 17 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 17}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 18 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0139, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 18, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0176, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 19 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0139, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 19, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0176, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 20 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 20, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 21 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0019, 0.0258, 0.0714, 0.0449,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0035, 0.0076, 0.0031, 0.0305 …]  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 21}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.                             </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "└────┴───────────────────────────────────┴────────────────────────────────────┴───────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what is stored in the Chroma collection\n",
        "data = collection.get(\n",
        "    include=['documents', 'metadatas'] # Include documents and metadatas\n",
        ")\n",
        "\n",
        "print(\"✅ Data currently stored in Chroma:\")\n",
        "if data and data['ids']:\n",
        "    for i, (doc, meta) in enumerate(zip(data['documents'], data['metadatas'])):\n",
        "        print(f\"- ID: {data['ids'][i]}, Metadata: {meta}\")\n",
        "        print(f\"  Document: {doc}\\n\")\n",
        "else:\n",
        "    print(\"No data found in the collection.\")\n",
        "\n",
        "print(f\"📊 Total items in collection: {collection.count()}\")"
      ],
      "metadata": {
        "id": "fa5Pr8HavobH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. A simple vector search"
      ],
      "metadata": {
        "id": "FAW2T3cIwtdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User query → embed → query Chroma\n",
        "query = \"What is RAG?\"\n",
        "q_emb = embed_texts([query])[0]\n",
        "\n",
        "results = collection.query(\n",
        "    query_embeddings=[q_emb],\n",
        "    n_results=3,\n",
        "    include=[\"distances\", \"documents\", \"metadatas\"] # Removed 'ids' from include\n",
        ")\n",
        "\n",
        "print(\"🔎 Top matches:\")\n",
        "for i in range(len(results[\"ids\"][0])):\n",
        "    doc_id    = results[\"ids\"][0][i]\n",
        "    distance  = results[\"distances\"][0][i]  # smaller is closer (cosine distance)\n",
        "    doc_text  = results[\"documents\"][0][i]\n",
        "    meta      = results[\"metadatas\"][0][i]\n",
        "    print(f\"\\nRank {i+1} | id={doc_id} | distance={distance:.4f} | meta={meta}\\n{doc_text}\")\n",
        "\n",
        "# From here you could pass the retrieved text to your LLM prompt to 'augment' generation."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0VDS2JFwIui",
        "outputId": "27fcae34-dc11-49af-ecc6-e08adf983245"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Top matches:\n",
            "\n",
            "Rank 1 | id=0 | distance=0.9758 | meta={'source': 'demo', 'chunk_id': 0}\n",
            "Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "Rank 2 | id=7 | distance=1.0302 | meta={'chunk_id': 7, 'source': 'demo'}\n",
            "This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "Rank 3 | id=10 | distance=1.0302 | meta={'chunk_id': 10, 'source': 'demo'}\n",
            "This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n"
          ]
        }
      ]
    }
  ]
}