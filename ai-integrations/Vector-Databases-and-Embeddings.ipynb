{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "135904f6"
      },
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "# Make sure to add your API key to Colab secrets under the name 'OPENAI_API_KEY'\n",
        "# For more info, see https://colab.research.google.com/notebooks/integrations.ipynb\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "# This client is used to interact with the OpenAI API\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63c0c207"
      },
      "source": [
        "# 1. Types of Memory\n",
        "\n",
        "1.  **Contextual (Short-Term) Memory:** Keeps conversations coherent in a single session.\n",
        "2.  **Long-Term (Persistent) Memory:** Remembers facts and preferences across sessions.\n",
        "3.  **Semantic Memory:** The model’s built-in factual and conceptual knowledge.\n",
        "4.  **Working Memory:** Handles multi-step reasoning and problem-solving within a single query.\n",
        "5.  **External / Tool-Augmented Memory (RAG):** Extends the model’s capacity by retrieving info from external sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "017713d2"
      },
      "source": [
        "## 1. Contextual (Short-Term) Memory\n",
        "\n",
        "Keeps conversations coherent in a single session by remembering recent interactions. This type of memory is crucial for maintaining flow and understanding within a limited timeframe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: First message to set context\n",
        "# This message introduces the user and sets the initial context for the conversation.\n",
        "response1 = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",  # Small, fast model for demo\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"My name is Alex.\"}\n",
        "    ]\n",
        ")\n",
        "print(response1.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5--tBgy9LP3",
        "outputId": "0db17fe8-c0e3-46aa-de64-73135adf6ae8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nice to meet you, Alex! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Second message reuses context (conversation continues)\n",
        "# By including the previous messages, the model remembers the user's name from the first interaction.\n",
        "response2 = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        # Include the previous messages to maintain context\n",
        "        {\"role\": \"assistant\", \"content\": response1.choices[0].message.content},\n",
        "        {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
        "    ]\n",
        ")\n",
        "print(response2.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtTdS4WN9XZm",
        "outputId": "d047449d-3612-4c71-a16e-e9120227c91a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You mentioned your name is Alex. How can I help you today, Alex?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c01c4566"
      },
      "source": [
        "## 2. Long-Term (Persistent) Memory\n",
        "\n",
        "Remembers facts and preferences across sessions, allowing the model to maintain consistency and personalization over extended periods."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persistent file for memory\n",
        "MEMORY_FILE = \"memory.txt\"\n",
        "\n",
        "def save_memory(data):\n",
        "    with open(MEMORY_FILE, \"w\") as f:\n",
        "        f.write(data)\n",
        "\n",
        "def load_memory():\n",
        "    if os.path.exists(MEMORY_FILE):\n",
        "        with open(MEMORY_FILE, \"r\") as f:\n",
        "            return f.read().strip()\n",
        "    return None\n",
        "\n",
        "# Step 1: User gives info\n",
        "user_message = \"My name is Alex\"\n",
        "\n",
        "# Step 2: Ask GPT-4 if this should be remembered\n",
        "decision = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"system\", \"content\": \"Decide if the user's message contains important facts to store such as name, preferences and so on. If you do decide to remember the facts begin the answer with 'Fact remembered!'\"},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Model decision:\", decision.choices[0].message.content)\n",
        "\n",
        "# Step 3: If model says it's important, trigger 'store_memory'\n",
        "if \"remembered\" in decision.choices[0].message.content.lower():\n",
        "    save_memory(user_message)\n",
        "    print(\"Stored in memory:\", user_message)\n",
        "\n",
        "# Step 4: Later, retrieve memory and use it in a new conversation\n",
        "stored_info = load_memory()\n",
        "if stored_info:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Previously you learned: {stored_info}. Now greet me using that info.\"}\n",
        "        ]\n",
        "    )\n",
        "    print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_WQwnZr9piA",
        "outputId": "69ba653a-3fb9-418b-d032-bb256b9e787b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model decision: Fact remembered! Your name is Alex.\n",
            "Stored in memory: My name is Alex\n",
            "Hello, Alex! It's great to meet you! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple LLM Long Memory System\n",
        "- Stores user facts in JSON file\n",
        "- Uses OpenAI function calling to save/recall info\n",
        "- Perfect for Google Colab experiments\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "MEMORY_FILE = \"memory.json\"  # Where memories are stored\n",
        "\n",
        "def save_memory(key: str, value: str):\n",
        "    \"\"\"Store a key-value pair in memory file\"\"\"\n",
        "    print(\"Memory updating...\")\n",
        "    memory = {}\n",
        "    if os.path.exists(MEMORY_FILE):\n",
        "        with open(MEMORY_FILE, \"r\") as f:\n",
        "            memory = json.load(f)\n",
        "\n",
        "    memory[key] = value\n",
        "    with open(MEMORY_FILE, \"w\") as f:\n",
        "        json.dump(memory, f, indent=2)\n",
        "    return \"Stored successfully\"\n",
        "\n",
        "def load_memory(query: str):\n",
        "    \"\"\"Search for stored memories matching the query\"\"\"\n",
        "    print(\"Reading memory...\")\n",
        "    if not os.path.exists(MEMORY_FILE):\n",
        "        return \"No memories stored\"\n",
        "\n",
        "    with open(MEMORY_FILE, \"r\") as f:\n",
        "        memory = json.load(f)\n",
        "\n",
        "    if not memory:\n",
        "        return \"No memories stored\"\n",
        "\n",
        "    # Search for relevant info\n",
        "    # query_lower = query.lower()\n",
        "    # relevant = {k: v for k, v in memory.items()\n",
        "    #            if query_lower in k.lower() or query_lower in str(v).lower()}\n",
        "\n",
        "    # if relevant:\n",
        "    #     return json.dumps(relevant)\n",
        "    # else:\n",
        "    #     return f\"No memory found for: {query}\"\n",
        "    return memory # Return the entire memory dictionary\n",
        "\n",
        "# Function tools for the AI to call\n",
        "\n",
        "# Tool definitions\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"store_fact\",\n",
        "            \"description\": \"Store user info like name, age, etc.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"key\": {\"type\": \"string\"},\n",
        "                    \"value\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"key\", \"value\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"recall_fact\",\n",
        "            \"description\": \"Get stored user info\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"query\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def chat_with_memory(user_input: str):\n",
        "    \"\"\"Main function that handles conversation with memory capabilities\"\"\"\n",
        "    client = OpenAI()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You have memory tools. When user shares personal info, store it. When they ask about themselves, recall it first.\"},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "\n",
        "    # First API call - may trigger tool usage\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Handle tool calls if any\n",
        "    if response.choices[0].message.tool_calls:\n",
        "        messages.append(response.choices[0].message)\n",
        "\n",
        "        for tool_call in response.choices[0].message.tool_calls:\n",
        "            args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "            # Execute the appropriate tool\n",
        "            if tool_call.function.name == \"store_fact\":\n",
        "                result = save_memory(args[\"key\"], args[\"value\"])\n",
        "            elif tool_call.function.name == \"recall_fact\":\n",
        "                result = load_memory(args[\"query\"])\n",
        "\n",
        "            messages.append({\n",
        "                \"role\": \"tool\",\n",
        "                \"tool_call_id\": tool_call.id,\n",
        "                \"content\": str(result) # Ensure the content is a string\n",
        "            })\n",
        "\n",
        "        # Second API call with tool results\n",
        "        final_response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=messages\n",
        "        )\n",
        "\n",
        "\n",
        "        return final_response.choices[0].message.content\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# ==================== DEMO ====================\n",
        "\n",
        "\n",
        "\n",
        "user_text = input(\"\\n💬 You: \")\n",
        "\n",
        "print(\"\\n🤖 Assistant:\")\n",
        "response = chat_with_memory(user_text)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOVQJJzo-u0d",
        "outputId": "5605f769-8eb7-4654-a8f1-b01498b23942"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💬 You: Hello there! I am 20 years old and my name is Alex\n",
            "\n",
            "🤖 Assistant:\n",
            "Memory updating...\n",
            "Memory updating...\n",
            "Hi Alex! It's great to meet you. How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ee559c"
      },
      "source": [
        "## 3. Semantic Memory\n",
        "\n",
        "The model’s built-in factual and conceptual knowledge, enabling it to understand and generate human-like text based on its training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef87cced",
        "outputId": "20224474-9c20-4e62-8d4c-5e1c88da6cdc"
      },
      "source": [
        "# Define the input message for the model\n",
        "input_message = \"Write a short scientific article about nature.\" # Using the original prompt content\n",
        "\n",
        "# Create a response request using the responses endpoint\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",  # Using gpt-4o-mini\n",
        "    input=input_message,\n",
        "    max_output_tokens=100\n",
        ")\n",
        "\n",
        "# Extract and print the model's output text\n",
        "print(response.output_text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# The Interconnectedness of Nature: A Delicate Balance\n",
            "\n",
            "## Abstract\n",
            "Nature is a complex web of interactions that sustains life on Earth. This article explores the fundamental elements of ecosystems, the relationship between biodiversity and ecosystem services, and the impact of human activity on natural systems.\n",
            "\n",
            "## Introduction\n",
            "Nature, often perceived as a collection of individual species and landscapes, is, in reality, an interconnected network of relationships that sustains life. Ecosystems, defined as communities of living organisms interacting with their\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66cb3ab6"
      },
      "source": [
        "# 2. Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8047a247"
      },
      "source": [
        "## 1. Vectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_vector(n):\n",
        "    \"\"\"Generate a vector with n dimensions filled with random values\"\"\"\n",
        "    return np.random.rand(n)\n",
        "\n",
        "# Example usage\n",
        "number = 110\n",
        "vector = generate_vector(number)\n",
        "\n",
        "print(f\"Input: {number}\")\n",
        "print(f\"Vector: {vector}\")\n",
        "print(f\"Shape: {vector.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4PPX3lgFJlI",
        "outputId": "251475df-26b1-484d-f634-131597354fa4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 110\n",
            "Vector: [0.70473026 0.69562759 0.31124105 0.85171687 0.98094158 0.93547458\n",
            " 0.2957988  0.25989481 0.28693497 0.08102088 0.46816844 0.46277758\n",
            " 0.7408384  0.9831221  0.84089429 0.02214819 0.67074792 0.45428174\n",
            " 0.26494666 0.88836125 0.31786706 0.78110637 0.13716815 0.76843992\n",
            " 0.97307726 0.88823262 0.65598551 0.00848529 0.08207384 0.05479042\n",
            " 0.62375421 0.62953462 0.25157051 0.76645527 0.30245514 0.18304236\n",
            " 0.61517575 0.03011538 0.78854075 0.25820766 0.48399333 0.75791422\n",
            " 0.75659371 0.48547041 0.34400719 0.14910311 0.4812495  0.2375033\n",
            " 0.95979445 0.78444793 0.94132784 0.40159678 0.22056531 0.80020971\n",
            " 0.56394193 0.42495733 0.95456606 0.51851623 0.48286589 0.15775229\n",
            " 0.70233872 0.56193559 0.09997356 0.04678185 0.48753984 0.58845103\n",
            " 0.51950102 0.14273828 0.09004669 0.92832353 0.71492236 0.62950373\n",
            " 0.22387372 0.34815242 0.54547043 0.02959915 0.49100049 0.70264211\n",
            " 0.47318651 0.18307098 0.40934184 0.6724792  0.04170277 0.80233729\n",
            " 0.52886555 0.67457919 0.37131834 0.18778419 0.96933008 0.90259795\n",
            " 0.99846064 0.71994612 0.27895035 0.96634591 0.02902153 0.75920306\n",
            " 0.49466851 0.74912749 0.69411455 0.8735623  0.62119505 0.3569503\n",
            " 0.21418823 0.04142291 0.31473113 0.42093551 0.0321318  0.37076651\n",
            " 0.7173938  0.00585078]\n",
            "Shape: (110,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate Embeddings"
      ],
      "metadata": {
        "id": "kNphuftXGYFY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f394ae07",
        "outputId": "1e80331f-eb1e-4e96-f59f-1839a396a892"
      },
      "source": [
        "# Cell 1: Generate Embeddings\n",
        "# This code demonstrates how to generate embeddings for text using the OpenAI API.\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"\n",
        "    Generate embeddings for text using OpenAI API\n",
        "    \"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",  # A common and efficient embedding model\n",
        "        input=text,\n",
        "        encoding_format=\"float\"  # Specify the format of the embedding vector\n",
        "    )\n",
        "\n",
        "    # The embedding vector is in the 'data' attribute of the response\n",
        "    # It's a list of embedding objects, we take the first one's embedding\n",
        "    embedding_vector = response.data[0].embedding\n",
        "    return embedding_vector\n",
        "\n",
        "# Example usage\n",
        "text_to_embed = \"This is a sample sentence for creating an embedding.\"\n",
        "\n",
        "embedding_vector = get_embedding(text_to_embed)\n",
        "\n",
        "# Print the embedding vector and its dimension\n",
        "print(\"Text:\", text_to_embed)\n",
        "print(\"Embedding vector (first 10 elements):\", embedding_vector[:10])\n",
        "print(\"Dimension of the embedding vector:\", len(embedding_vector))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This is a sample sentence for creating an embedding.\n",
            "Embedding vector (first 10 elements): [0.03280215, 0.011288634, 0.02859873, 0.0051769116, -0.003304069, -0.03293109, 0.03349842, -0.016040046, -0.024150325, 0.019882435]\n",
            "Dimension of the embedding vector: 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Compare Embeddings"
      ],
      "metadata": {
        "id": "jraxLZkWGdkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Compare Embeddings\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Calculate cosine similarity between two vectors\n",
        "    \"\"\"\n",
        "    v1 = np.array(vec1)\n",
        "    v2 = np.array(vec2)\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "# Example texts to compare\n",
        "text1 = \"How to train a cat?\"\n",
        "text2 = \"How to drive a car?\"\n",
        "\n",
        "# Get embeddings for both texts\n",
        "embedding1 = get_embedding(text1)\n",
        "embedding2 = get_embedding(text2)\n",
        "\n",
        "# Calculate similarity\n",
        "similarity = cosine_similarity(embedding1, embedding2)\n",
        "\n",
        "print(f\"Text 1: {text1}\")\n",
        "print(f\"Text 2: {text2}\")\n",
        "print(f\"Similarity: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OShBoF8UGgx9",
        "outputId": "559cd90b-072d-44f2-e89e-4cabfaba07ad"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: How to train a cat?\n",
            "Text 2: How to drive a car?\n",
            "Similarity: 0.3499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "958e57db"
      },
      "source": [
        "# 3. Vector Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3939a3df"
      },
      "source": [
        "## 1. Chroma DB\n",
        "\n",
        "Solo dev / small app / on-prem or hybrid → start with Chroma (OSS, dead-simple API; can move to Chroma Cloud later). Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Basic Setup"
      ],
      "metadata": {
        "id": "bgIqDKPCMYGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup\n",
        "!pip install chromadb\n",
        "\n",
        "import chromadb\n",
        "\n",
        "chroma = chromadb.Client()\n",
        "collection = chroma.get_or_create_collection(\"demo\")\n",
        "\n",
        "def get_embedding(text):\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=text\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "print(\"✅ Ready!\")"
      ],
      "metadata": {
        "id": "DEEltyQzMSdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ddc679e-5af3-416e-d50e-9bdb93a368d9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.4)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.16.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.8)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=2146430bc9b89d8e16d88c08e5060307a8dc9aee20998468e05fc29fd9df8c0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, overrides, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, onnxruntime, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.20 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n",
            "✅ Ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Store Embeddings\n",
        "\n",
        "texts = [\n",
        "    \"How do I cook pasta?\",\n",
        "    \"The weather is sunny\",\n",
        "    \"AI is amazing\"\n",
        "]\n",
        "\n",
        "print(\"Storing embeddings...\")\n",
        "\n",
        "embeddings = [get_embedding(text) for text in texts]\n",
        "\n",
        "collection.add(\n",
        "    ids=[\"1\", \"2\", \"3\"],\n",
        "    documents=texts,\n",
        "    embeddings=embeddings\n",
        ")\n",
        "\n",
        "print(f\"✅ Stored {len(texts)} documents!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8odlvQV7NY3-",
        "outputId": "d5523b73-d4ee-44b2-bbed-c56e9cff0d73"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Storing embeddings...\n",
            "✅ Stored 3 documents!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: View Stored Data\n",
        "\n",
        "# Show what's stored\n",
        "# Explicitly include embeddings, documents, and metadatas\n",
        "data = collection.get(\n",
        "    include=['embeddings', 'documents', 'metadatas']\n",
        ")\n",
        "\n",
        "ids = data[\"ids\"]\n",
        "docs = data[\"documents\"]\n",
        "embs = data.get(\"embeddings\")\n",
        "\n",
        "\n",
        "print(f\"Found {len(ids)} stored items:\\n\")\n",
        "\n",
        "# If for some reason embeddings are missing, don't try to iterate over them\n",
        "if embs is None or any(e is None for e in embs):\n",
        "    for idx, doc in zip(ids, docs):\n",
        "        print(f\"ID: {idx}\")\n",
        "        print(f\"Text: {doc}\")\n",
        "        print(\"(no embeddings returned)\")\n",
        "        print(\"-\" * 40)\n",
        "else:\n",
        "    for idx, doc, emb in zip(ids, docs, embs):\n",
        "        print(f\"ID: {idx}\")\n",
        "        print(f\"Text: {doc}\")\n",
        "        print(f\"Embedding (first 5 dims): {emb[:5]}\")\n",
        "        print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac25lPC0NxIa",
        "outputId": "06192bbc-7702-4e06-cf0e-a7a42e55bc25"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 stored items:\n",
            "\n",
            "ID: 1\n",
            "Text: How do I cook pasta?\n",
            "Embedding (first 5 dims): [-6.04647398e-02 -6.07534535e-02 -1.20228324e-02 -2.46024672e-02\n",
            "  9.77145100e-05]\n",
            "----------------------------------------\n",
            "ID: 2\n",
            "Text: The weather is sunny\n",
            "Embedding (first 5 dims): [ 0.00196909 -0.04838687  0.00736109 -0.00663111  0.02561658]\n",
            "----------------------------------------\n",
            "ID: 3\n",
            "Text: AI is amazing\n",
            "Embedding (first 5 dims): [-0.00992907 -0.00132574 -0.01422693  0.04163846  0.00857541]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. RAG"
      ],
      "metadata": {
        "id": "665K1g2btt01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Set up Chroma + OpenAI API"
      ],
      "metadata": {
        "id": "-luJDw-3umQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "\n",
        "# Create (or get) a collection to hold our chunks\n",
        "collection = chroma.get_or_create_collection(name=\"rag_demo\")\n",
        "\n",
        "print(\"✅ Setup complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77PSNIaXtwF3",
        "outputId": "7d28d09f-cf60-484f-9919-7441a19d9f96"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare and chunk a text"
      ],
      "metadata": {
        "id": "3vTOqa10vGim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A small sample text (replace with your own)\n",
        "text = \"\"\"\n",
        "Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our\n",
        "\n",
        "\n",
        "\n",
        "This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
        "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
        "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
        "to ground the answer. This improves accuracy and keeps responses up-to-date with our data.\n",
        "\"\"\"\n",
        "\n",
        "# Simple sentence-based chunking (groups of ~2–3 sentences)\n",
        "import re\n",
        "\n",
        "def chunk_text(t: str, max_sentences=1):\n",
        "    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', t.strip()) if s.strip()]\n",
        "    chunks, buf = [], []\n",
        "    for s in sents:\n",
        "        buf.append(s)\n",
        "        if len(buf) >= max_sentences:\n",
        "            chunks.append(\" \".join(buf))\n",
        "            buf = []\n",
        "    if buf:\n",
        "        chunks.append(\" \".join(buf))\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(text, max_sentences=2)\n",
        "for i, c in enumerate(chunks):\n",
        "    print(f\"[{i}] {c}\\n\")\n",
        "\n",
        "print(f\"✅ Prepared {len(chunks)} chunks.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9G7mfezvJet",
        "outputId": "a00894f7-0e04-45ca-c9a4-a35329783086"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[1] This improves accuracy and keeps responses up-to-date with our\n",
            "\n",
            "\n",
            "\n",
            "This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[2] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[3] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[4] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[5] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[6] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[7] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[8] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[9] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[10] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[11] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[12] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[13] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[14] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[15] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[16] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[17] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[18] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[19] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[20] This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "[21] This improves accuracy and keeps responses up-to-date with our data.\n",
            "\n",
            "✅ Prepared 22 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Turn chunks into embeddings"
      ],
      "metadata": {
        "id": "Z_DPCViUvRIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_MODEL = \"text-embedding-3-small\"  # 1536-dim, fast & inexpensive\n",
        "\n",
        "def embed_texts(text_list):\n",
        "    resp = client.embeddings.create(model=EMBED_MODEL, input=text_list)\n",
        "    return [d.embedding for d in resp.data]\n",
        "\n",
        "embeddings = embed_texts(chunks)\n",
        "\n",
        "print(f\"✅ Got {len(embeddings)} embeddings.\")\n",
        "print(f\"   Each vector has dimension: {len(embeddings[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0g0hcTbvLEH",
        "outputId": "72cbabb9-c0fa-45c6-b0bf-620172ec3856"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Got 22 embeddings.\n",
            "   Each vector has dimension: 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.  Store in the vector DB (Chroma)"
      ],
      "metadata": {
        "id": "TPKMKUGBvmK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use string IDs \"0\", \"1\", ...\n",
        "ids = [str(i) for i in range(len(chunks))]\n",
        "metadatas = [{\"source\": \"demo\", \"chunk_id\": i} for i in range(len(chunks))]\n",
        "\n",
        "collection.add(\n",
        "    ids=ids,\n",
        "    documents=chunks,\n",
        "    embeddings=embeddings,\n",
        "    metadatas=metadatas\n",
        ")\n",
        "\n",
        "# Peek at what we stored (without dumping full vectors)\n",
        "data = collection.get(include=[\"documents\", \"metadatas\", \"embeddings\"])  # add \"embeddings\" if you want them\n",
        "\n",
        "print(\"✅ Stored items:\")\n",
        "for i, doc, emb, meta in zip(data[\"ids\"], data[\"documents\"],data['embeddings'], data[\"metadatas\"]):\n",
        "    print(f\"- id={i}, meta={meta}\\n  {doc}\\n {emb}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoJnaE9CvnOe",
        "outputId": "39e6d65e-5393-4837-d99b-a7b417048e39"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Stored items:\n",
            "- id=0, meta={'chunk_id': 0, 'source': 'demo'}\n",
            "  Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01604582  0.02682545  0.03639361 ... -0.00516421  0.02783913\n",
            " -0.00210617]\n",
            "\n",
            "- id=1, meta={'source': 'demo', 'chunk_id': 1}\n",
            "  This improves accuracy and keeps responses up-to-date with our\n",
            "\n",
            "\n",
            "\n",
            "This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.00671253  0.03448062  0.05353642 ... -0.00039564  0.02271407\n",
            " -0.00155443]\n",
            "\n",
            "- id=2, meta={'source': 'demo', 'chunk_id': 2}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=3, meta={'chunk_id': 3, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=4, meta={'chunk_id': 4, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=5, meta={'source': 'demo', 'chunk_id': 5}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=6, meta={'chunk_id': 6, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=7, meta={'source': 'demo', 'chunk_id': 7}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=8, meta={'source': 'demo', 'chunk_id': 8}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=9, meta={'chunk_id': 9, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01393001  0.03513355  0.05262155 ... -0.00088458  0.02453834\n",
            "  0.00123332]\n",
            "\n",
            "- id=10, meta={'chunk_id': 10, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01393001  0.03513355  0.05262155 ... -0.00088458  0.02453834\n",
            "  0.00123332]\n",
            "\n",
            "- id=11, meta={'chunk_id': 11, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=12, meta={'source': 'demo', 'chunk_id': 12}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=13, meta={'chunk_id': 13, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=14, meta={'chunk_id': 14, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395491  0.03507762  0.05259018 ... -0.00089187  0.02453596\n",
            "  0.00126766]\n",
            "\n",
            "- id=15, meta={'chunk_id': 15, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01393001  0.03513355  0.05262155 ... -0.00088458  0.02453834\n",
            "  0.00123332]\n",
            "\n",
            "- id=16, meta={'chunk_id': 16, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=17, meta={'chunk_id': 17, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01393001  0.03513355  0.05262155 ... -0.00088458  0.02453834\n",
            "  0.00123332]\n",
            "\n",
            "- id=18, meta={'source': 'demo', 'chunk_id': 18}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01393001  0.03513355  0.05262155 ... -0.00088458  0.02453834\n",
            "  0.00123332]\n",
            "\n",
            "- id=19, meta={'source': 'demo', 'chunk_id': 19}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01395565  0.03507949  0.05259297 ... -0.00089274  0.02453726\n",
            "  0.00125624]\n",
            "\n",
            "- id=20, meta={'chunk_id': 20, 'source': 'demo'}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            " [-0.01393001  0.03513355  0.05262155 ... -0.00088458  0.02453834\n",
            "  0.00123332]\n",
            "\n",
            "- id=21, meta={'source': 'demo', 'chunk_id': 21}\n",
            "  This improves accuracy and keeps responses up-to-date with our data.\n",
            " [-0.00187269  0.0258257   0.07124931 ... -0.00434058  0.00375627\n",
            "  0.01652032]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display the data"
      ],
      "metadata": {
        "id": "wUy8oK0QxNVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Rich once in Colab\n",
        "!pip -q install rich\n",
        "\n",
        "data = collection.get(include=[\"documents\", \"metadatas\", \"embeddings\"])\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "import json\n",
        "\n",
        "def fmt_text(s, maxlen=120):\n",
        "    s = s or \"\"\n",
        "    return (s[:maxlen] + \"…\") if len(s) > maxlen else s\n",
        "\n",
        "def fmt_emb(e, preview=8):\n",
        "    if e is None:\n",
        "        return \"—\"\n",
        "    head = \", \".join(f\"{x:.4f}\" for x in e[:preview])\n",
        "    tail = \" …\" if len(e) > preview else \"\"\n",
        "    return f\"[{head}{tail}]  (dim={len(e)})\"\n",
        "\n",
        "table = Table(show_header=True, header_style=\"bold magenta\")\n",
        "table.add_column(\"ID\", style=\"cyan\", no_wrap=True)\n",
        "table.add_column(\"Text\", style=\"white\")\n",
        "table.add_column(\"Embedding (preview)\", style=\"green\")\n",
        "table.add_column(\"Metadata\", style=\"yellow\")\n",
        "\n",
        "for i, doc, emb, meta in zip(data[\"ids\"], data[\"documents\"], data[\"embeddings\"], data[\"metadatas\"]):\n",
        "    table.add_row(str(i), fmt_text(doc), fmt_emb(emb, 8), json.dumps(meta, ensure_ascii=False))\n",
        "\n",
        "Console().print(\"Stored items:\", style=\"bold\")\n",
        "Console().print(table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P7l__Z_-xMhf",
        "outputId": "8dfc297b-1e9c-4042-e123-6f60cfea57e7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mStored items:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Stored items:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35mID\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mText                             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mEmbedding (preview)               \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMetadata                         \u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m0 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mRetrieval-Augmented Generation   \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0160, 0.0268, 0.0364, 0.0264, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 0, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37m(RAG) helps language models      \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0172, 0.0024, -0.0029, 0.0235  \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37manswer questions by looking up   \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m…]  (dim=1536)                    \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37msupporting information from a …  \u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m1 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0067, 0.0345, 0.0535, 0.0387, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 1, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0180, -0.0081, 0.0047, 0.0405  \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                 \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m…]  (dim=1536)                    \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                 \u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                 \u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date …           \u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m2 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 2, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m3 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\": 3}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m4 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 4, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m5 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\": 5}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m6 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 6, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m7 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\": 7}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m8 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 8, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m9 \u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0139, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 9, \"source\": \"demo\"}\u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0176, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m10\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0139, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0176, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m10}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m11\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 11, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m12\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m12}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m13\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 13, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m14\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m14}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m15\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0139, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 15, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0176, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m16\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 16, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m17\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0139, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 17, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0176, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m18\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0139, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0176, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m18}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m19\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0140, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0177, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m19}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m20\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0139, 0.0351, 0.0526, 0.0331, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"chunk_id\": 20, \"source\":       \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-0.0176, 0.0031, 0.0012, 0.0394 …]\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m\"demo\"}                          \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.Retrieval-Augmented         \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mGeneration (RAG) helps language …\u001b[0m\u001b[37m \u001b[0m│\u001b[32m                                    \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m21\u001b[0m\u001b[36m \u001b[0m│\u001b[37m \u001b[0m\u001b[37mThis improves accuracy and keeps \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[-0.0019, 0.0258, 0.0712, 0.0449, \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m{\"source\": \"demo\", \"chunk_id\":   \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mresponses up-to-date with our    \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0035, 0.0076, 0.0031, 0.0305 …] \u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m21}                              \u001b[0m\u001b[33m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[37m \u001b[0m\u001b[37mdata.                            \u001b[0m\u001b[37m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m(dim=1536)                        \u001b[0m\u001b[32m \u001b[0m│\u001b[33m                                   \u001b[0m│\n",
              "└────┴───────────────────────────────────┴────────────────────────────────────┴───────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> ID </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Text                              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Embedding (preview)                </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Metadata                          </span>┃\n",
              "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 0  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Retrieval-Augmented Generation    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0160, 0.0268, 0.0364, 0.0264,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 0, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> (RAG) helps language models       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0172, 0.0024, -0.0029, 0.0235   </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> answer questions by looking up    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> …]  (dim=1536)                     </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> supporting information from a …   </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 1  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0067, 0.0345, 0.0535, 0.0387,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 1, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0180, -0.0081, 0.0047, 0.0405   </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                   </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> …]  (dim=1536)                     </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                   </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                   </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date …            </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 2  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 2, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 3  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\": 3} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 4  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 4, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 5  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\": 5} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 6  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 6, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 7  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\": 7} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 8  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 8, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 9  </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0139, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 9, \"source\": \"demo\"} </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0176, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 10 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0139, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0176, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 10}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 11 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 11, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 12 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 12}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 13 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 13, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 14 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 14}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 15 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0139, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 15, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0176, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 16 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 16, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 17 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0139, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 17, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0176, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 18 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0139, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0176, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 18}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 19 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0140, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0177, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 19}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 20 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0139, 0.0351, 0.0526, 0.0331,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"chunk_id\": 20, \"source\":        </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> -0.0176, 0.0031, 0.0012, 0.0394 …] </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> \"demo\"}                           </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.Retrieval-Augmented          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> Generation (RAG) helps language … </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 21 </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> This improves accuracy and keeps  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [-0.0019, 0.0258, 0.0712, 0.0449,  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> {\"source\": \"demo\", \"chunk_id\":    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> responses up-to-date with our     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0035, 0.0076, 0.0031, 0.0305 …]  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 21}                               </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> data.                             </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> (dim=1536)                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                                   </span>│\n",
              "└────┴───────────────────────────────────┴────────────────────────────────────┴───────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what is stored in the Chroma collection\n",
        "data = collection.get(\n",
        "    include=['documents', 'metadatas'] # Include documents and metadatas\n",
        ")\n",
        "\n",
        "print(\"✅ Data currently stored in Chroma:\")\n",
        "if data and data['ids']:\n",
        "    for i, (doc, meta) in enumerate(zip(data['documents'], data['metadatas'])):\n",
        "        print(f\"- ID: {data['ids'][i]}, Metadata: {meta}\")\n",
        "        print(f\"  Document: {doc}\\n\")\n",
        "else:\n",
        "    print(\"No data found in the collection.\")\n",
        "\n",
        "print(f\"📊 Total items in collection: {collection.count()}\")"
      ],
      "metadata": {
        "id": "fa5Pr8HavobH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f70395-b50c-4e85-8b94-ba5436f6471b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data currently stored in Chroma:\n",
            "- ID: 0, Metadata: {'chunk_id': 0, 'source': 'demo'}\n",
            "  Document: Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 1, Metadata: {'chunk_id': 1, 'source': 'demo'}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our\n",
            "\n",
            "\n",
            "\n",
            "This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 2, Metadata: {'source': 'demo', 'chunk_id': 2}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 3, Metadata: {'source': 'demo', 'chunk_id': 3}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 4, Metadata: {'chunk_id': 4, 'source': 'demo'}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 5, Metadata: {'source': 'demo', 'chunk_id': 5}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 6, Metadata: {'source': 'demo', 'chunk_id': 6}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 7, Metadata: {'source': 'demo', 'chunk_id': 7}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 8, Metadata: {'source': 'demo', 'chunk_id': 8}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 9, Metadata: {'source': 'demo', 'chunk_id': 9}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 10, Metadata: {'chunk_id': 10, 'source': 'demo'}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 11, Metadata: {'chunk_id': 11, 'source': 'demo'}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 12, Metadata: {'source': 'demo', 'chunk_id': 12}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 13, Metadata: {'source': 'demo', 'chunk_id': 13}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 14, Metadata: {'source': 'demo', 'chunk_id': 14}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 15, Metadata: {'chunk_id': 15, 'source': 'demo'}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 16, Metadata: {'chunk_id': 16, 'source': 'demo'}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 17, Metadata: {'source': 'demo', 'chunk_id': 17}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 18, Metadata: {'chunk_id': 18, 'source': 'demo'}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 19, Metadata: {'chunk_id': 19, 'source': 'demo'}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 20, Metadata: {'chunk_id': 20, 'source': 'demo'}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "- ID: 21, Metadata: {'chunk_id': 21, 'source': 'demo'}\n",
            "  Document: This improves accuracy and keeps responses up-to-date with our data.\n",
            "\n",
            "📊 Total items in collection: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. A simple vector search"
      ],
      "metadata": {
        "id": "FAW2T3cIwtdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User query → embed → query Chroma\n",
        "query = \"What is RAG?\"\n",
        "q_emb = embed_texts([query])[0]\n",
        "\n",
        "results = collection.query(\n",
        "    query_embeddings=[q_emb],\n",
        "    n_results=3,\n",
        "    include=[\"distances\", \"documents\", \"metadatas\"] # Removed 'ids' from include\n",
        ")\n",
        "\n",
        "print(\"🔎 Top matches:\")\n",
        "for i in range(len(results[\"ids\"][0])):\n",
        "    doc_id    = results[\"ids\"][0][i]\n",
        "    distance  = results[\"distances\"][0][i]  # smaller is closer (cosine distance)\n",
        "    doc_text  = results[\"documents\"][0][i]\n",
        "    meta      = results[\"metadatas\"][0][i]\n",
        "    print(f\"\\nRank {i+1} | id={doc_id} | distance={distance:.4f} | meta={meta}\\n{doc_text}\")\n",
        "\n",
        "# From here you could pass the retrieved text to your LLM prompt to 'augment' generation."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0VDS2JFwIui",
        "outputId": "7f5daa4f-2279-4e98-ff57-f7e0dc74c207"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Top matches:\n",
            "\n",
            "Rank 1 | id=0 | distance=0.9761 | meta={'source': 'demo', 'chunk_id': 0}\n",
            "Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "Rank 2 | id=1 | distance=0.9927 | meta={'source': 'demo', 'chunk_id': 1}\n",
            "This improves accuracy and keeps responses up-to-date with our\n",
            "\n",
            "\n",
            "\n",
            "This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n",
            "\n",
            "Rank 3 | id=2 | distance=1.0303 | meta={'chunk_id': 2, 'source': 'demo'}\n",
            "This improves accuracy and keeps responses up-to-date with our data.Retrieval-Augmented Generation (RAG) helps language models answer questions by looking up\n",
            "supporting information from a knowledge base. We chunk documents, embed those chunks,\n",
            "store the vectors in a database, and at query time we retrieve the most similar chunks\n",
            "to ground the answer.\n"
          ]
        }
      ]
    }
  ]
}