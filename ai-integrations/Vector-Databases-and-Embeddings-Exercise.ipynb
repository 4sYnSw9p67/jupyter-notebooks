{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Fetch the API key from Colab's secrets\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = openai.OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXss908DQTny",
        "outputId": "1244a998-593d-44ac-9537-065d096d1fc1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.104.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Upload the PDF"
      ],
      "metadata": {
        "id": "ZukK0OgbMgOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "r_FHoD5mFIcq",
        "outputId": "0dda8e9d-e027-4784-a150-eb7637c4f645"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8c285655-866c-490d-b853-33e63387d868\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8c285655-866c-490d-b853-33e63387d868\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving whitepaper_Foundational Large Language models & text generation_v2.pdf to whitepaper_Foundational Large Language models & text generation_v2.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Extract the text from the pdf"
      ],
      "metadata": {
        "id": "1KiFqJLcMztJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b94559e",
        "outputId": "60971cf7-e2de-4e64-f221-11c5a2511eba"
      },
      "source": [
        "!pip install pdfminer.six"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20250506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Assuming the uploaded PDF is the first (and only) file in the 'uploaded' dictionary\n",
        "pdf_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Extract text from the PDF\n",
        "raw_text = extract_text(pdf_filename)\n",
        "\n",
        "# Display the first 500 characters of the extracted text\n",
        "print(raw_text[:500])"
      ],
      "metadata": {
        "id": "oNUgzBtNM3gX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f68c3b9-7079-42d1-f02b-78aee877f70e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Foundational \n",
            "Large Language \n",
            "Models & \n",
            "Text Generation\n",
            "\n",
            "Authors: Mohammadamin Barektain,  \n",
            "Anant Nawalgaria, Daniel J. Mankowitz,  \n",
            "Majd Al Merey, Yaniv Leviathan, Massimo Mascaro,  \n",
            "Matan Kalman, Elena Buchatskaya,                                     \n",
            "Aliaksei Severyn, Irina Sigler, and Antonio Gulli\n",
            "\n",
            "\fAcknowledgements\n",
            "\n",
            "Content contributors\n",
            "\n",
            "Adam Sadvovsky\n",
            "\n",
            "Yonghui Wu\n",
            "\n",
            "Andrew Dai\n",
            "\n",
            "Efi Kokiopolou\n",
            "\n",
            "Chuck Sugnet\n",
            "\n",
            "Aleksey Vlasenko\n",
            "\n",
            "Erwin Huizenga\n",
            "\n",
            "Aida Nematzadeh\n",
            "\n",
            "Ira Ktena\n",
            "\n",
            "Olivia Wiles\n",
            "\n",
            "Lavi Nig\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Chunk the text"
      ],
      "metadata": {
        "id": "jKV5Vz2SNT2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Easy method"
      ],
      "metadata": {
        "id": "anhYP6jOPH7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size, overlap):\n",
        "    \"\"\"Chunks text into smaller pieces with overlap.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "        chunk_size (int): The desired size of each chunk.\n",
        "        overlap (int): The number of characters to overlap between chunks.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "# Example usage (replace with your desired chunk_size and overlap)\n",
        "chunk_size = 500 # 400 text # 100 next chunk\n",
        "overlap = 100\n",
        "text_chunks = chunk_text(raw_text, chunk_size, overlap)\n",
        "print(f\"Created {len(text_chunks)} chunks.\")\n",
        "print(\"First chunk:\", text_chunks[0][:200]) # Print first 200 characters of the first chunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uppcSmEsNX-w",
        "outputId": "95b0be5c-a2f7-407c-e321-e559eae8319b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 344 chunks.\n",
            "First chunk: Foundational \n",
            "Large Language \n",
            "Models & \n",
            "Text Generation\n",
            "\n",
            "Authors: Mohammadamin Barektain,  \n",
            "Anant Nawalgaria, Daniel J. Mankowitz,  \n",
            "Majd Al Merey, Yaniv Leviathan, Massimo Mascaro,  \n",
            "Matan Kalman, El\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Punctuation Chunking"
      ],
      "metadata": {
        "id": "WJAiwl_VPMIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def punctuation_chunking(text, chunk_size=500, overlap=100):\n",
        "    \"\"\"Chunks text into smaller pieces based on punctuation with overlap.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "        chunk_size (int): The approximate desired size of each chunk.\n",
        "        overlap (int): The number of characters to overlap between chunks.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text chunks.\n",
        "    \"\"\"\n",
        "    # Split the text by punctuation that typically ends sentences or paragraphs\n",
        "    split_points = r'(?<=[.!?;\\n])\\s*' # Split after punctuation followed by optional whitespace\n",
        "    sentences = re.split(split_points, text)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    i = 0\n",
        "    while i < len(sentences):\n",
        "        sentence = sentences[i]\n",
        "\n",
        "        if len(current_chunk) + len(sentence) < chunk_size:\n",
        "            current_chunk += sentence\n",
        "            i += 1\n",
        "        else:\n",
        "            if current_chunk: # Add the current chunk if it's not empty\n",
        "                chunks.append(current_chunk.strip())\n",
        "                # Start the next chunk with overlap\n",
        "                overlap_start = max(0, len(current_chunk) - overlap)\n",
        "                current_chunk = current_chunk[overlap_start:] + sentence\n",
        "                i += 1\n",
        "            else: # If current_chunk is empty, the sentence is larger than chunk_size\n",
        "                 # In this case, just add the sentence as a chunk (or part of it)\n",
        "                 chunks.append(sentence[:chunk_size].strip())\n",
        "                 sentences[i] = sentence[chunk_size:] # Keep the rest of the sentence for the next iteration\n",
        "                 if not sentences[i]: # If the rest is empty, move to the next sentence\n",
        "                     i += 1\n",
        "\n",
        "\n",
        "    # Add the last chunk if it's not empty\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Example usage (replace raw_text with your variable containing the extracted text)\n",
        "punctuation_chunks = punctuation_chunking(raw_text, chunk_size=500, overlap=100)\n",
        "print(f\"Created {len(punctuation_chunks)} chunks using punctuation chunking.\")\n",
        "print(\"First chunk:\", punctuation_chunks[0][:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2hrBx1dPHSj",
        "outputId": "62a81961-21c2-4853-81f9-5e6c455bba94"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 369 chunks using punctuation chunking.\n",
            "First chunk: Foundational \n",
            "Large Language \n",
            "Models & \n",
            "Text Generation\n",
            "Authors: Mohammadamin Barektain,  \n",
            "Anant Nawalgaria, Daniel J.Mankowitz,  \n",
            "Majd Al Merey, Yaniv Leviathan, Massimo Mascaro,  \n",
            "Matan Kalman, Elen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. AI Chunking"
      ],
      "metadata": {
        "id": "Dta6ukZdQJby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ai_chunking(text, client, model=\"gpt-4o-mini\", max_input_chars=5000):\n",
        "    \"\"\"Chunks text using an AI model to preserve context.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text (up to max_input_chars).\n",
        "        client: The initialized OpenAI client.\n",
        "        model (str): The OpenAI model to use for chunking.\n",
        "        max_input_chars (int): The maximum number of characters to send to the model.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text chunks generated by the AI.\n",
        "    \"\"\"\n",
        "    if len(text) > max_input_chars:\n",
        "        text = text[:max_input_chars]\n",
        "        print(f\"Warning: Input text truncated to {max_input_chars} characters for AI processing.\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that chunks text while preserving context. Return the chunks as a Python list of strings in JSON format. The chunks will be used for RAG. Kepp in mind that overlap is also needed\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Please chunk the following text into meaningful sections, ensuring context is preserved. Provide the output as a JSON object with a key 'chunks' containing a Python list of strings:\\n\\n{text}\"}\n",
        "            ],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        # Assuming the model returns a JSON object with a key like \"chunks\" containing the list\n",
        "        # You might need to inspect the model's output format and adjust accordingly.\n",
        "        ai_generated_chunks = json.loads(response.choices[0].message.content).get(\"chunks\", [])\n",
        "        return ai_generated_chunks\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during AI chunking: {e}\")\n",
        "        return []\n",
        "\n",
        "import json\n",
        "\n",
        "# Example usage (assuming 'client' is your initialized OpenAI client and 'raw_text' is your extracted text)\n",
        "# Note: Sending a large amount of text to the API will incur costs.\n",
        "ai_chunks = ai_chunking(raw_text, client, max_input_chars=5000)\n",
        "print(f\"Created {len(ai_chunks)} chunks using AI chunking.\")\n",
        "if ai_chunks:\n",
        "  print(\"First AI chunk:\", ai_chunks[0][:200])\n",
        "\n",
        "# You can then combine these chunks with the other chunks if needed:\n",
        "# all_chunks = text_chunks + punctuation_chunks + ai_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AjW0Q3wQ1Qj",
        "outputId": "da06c7ec-463e-469b-dc7b-34ec4e4b0d99"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Input text truncated to 5000 characters for AI processing.\n",
            "Created 22 chunks using AI chunking.\n",
            "First AI chunk: Foundational Large Language Models & Text Generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of chunks to print from the user\n",
        "num_chunks_to_print = int(input(\"Enter the number of chunks to print: \"))\n",
        "\n",
        "# Print the first n chunks\n",
        "print(f\"\\nFirst {num_chunks_to_print} AI chunks:\")\n",
        "for i, chunk in enumerate(ai_chunks[:num_chunks_to_print]):\n",
        "    print(f\"Chunk {i+1}:\\n{chunk}\\n---\")"
      ],
      "metadata": {
        "id": "7lw8J_kPSwnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eebd0a13-e1f8-4761-e2f3-8a1ea96d8b55"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of chunks to print: 3\n",
            "\n",
            "First 3 AI chunks:\n",
            "Chunk 1:\n",
            "Foundational Large Language Models & Text Generation\n",
            "---\n",
            "Chunk 2:\n",
            "Authors: Mohammadamin Barektain, Anant Nawalgaria, Daniel J. Mankowitz, Majd Al Merey, Yaniv Leviathan, Massimo Mascaro, Matan Kalman, Elena Buchatskaya, Aliaksei Severyn, Irina Sigler, and Antonio Gulli\n",
            "---\n",
            "Chunk 3:\n",
            "Acknowledgements\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Gerate embeddings"
      ],
      "metadata": {
        "id": "sHbSttl0TsX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import time\n",
        "\n",
        "def generate_embeddings(chunks, client, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"Generates embeddings for a list of text chunks using OpenAI.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): A list of text chunks (strings).\n",
        "        client: The initialized OpenAI client.\n",
        "        model (str): The OpenAI embedding model to use.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping each chunk to its embedding vector,\n",
        "              or None if an error occurs.\n",
        "    \"\"\"\n",
        "    chunk_embeddings = {}\n",
        "    try:\n",
        "        # OpenAI's embeddings endpoint can take a list of inputs\n",
        "        response = client.embeddings.create(\n",
        "            input=chunks,\n",
        "            model=model\n",
        "        )\n",
        "        # Assuming the response structure contains 'data' which is a list of embedding objects\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if i < len(response.data):\n",
        "                chunk_embeddings[chunk] = response.data[i].embedding\n",
        "            else:\n",
        "                print(f\"Warning: No embedding returned for chunk {i+1}.\")\n",
        "\n",
        "        return chunk_embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during embedding generation: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage (assuming 'client' is your initialized OpenAI client and 'punctuation_chunks' is your list of chunks)\n",
        "# Note: Generating embeddings will incur costs.\n",
        "embeddings = generate_embeddings(punctuation_chunks, client)\n",
        "\n",
        "if embeddings:\n",
        "   print(f\"Generated embeddings for {len(embeddings)} chunks.\")\n",
        "   # Example of accessing an embedding:\n",
        "   first_chunk = list(embeddings.keys())[0]\n",
        "   print(\"\\nFirst Chunk:\")\n",
        "   print(first_chunk)\n",
        "   print(\"\\nEmbedding for the first chunk (first 10 elements):\")\n",
        "   print(embeddings[first_chunk][:10]) # Print first 10 elements of the embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdiwsOdpT15z",
        "outputId": "309eea02-2c34-49a6-e57f-a20816da2914"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings for 369 chunks.\n",
            "\n",
            "First Chunk:\n",
            "Foundational \n",
            "Large Language \n",
            "Models & \n",
            "Text Generation\n",
            "Authors: Mohammadamin Barektain,  \n",
            "Anant Nawalgaria, Daniel J.Mankowitz,  \n",
            "Majd Al Merey, Yaniv Leviathan, Massimo Mascaro,  \n",
            "Matan Kalman, Elena Buchatskaya,                                     \n",
            "Aliaksei Severyn, Irina Sigler, and Antonio Gulli\n",
            "Acknowledgements\n",
            "Content contributors\n",
            "Adam Sadvovsky\n",
            "Yonghui Wu\n",
            "Andrew Dai\n",
            "Efi Kokiopolou\n",
            "Chuck Sugnet\n",
            "Aleksey Vlasenko\n",
            "Erwin Huizenga\n",
            "Aida Nematzadeh\n",
            "Ira Ktena\n",
            "Olivia Wiles\n",
            "Lavi Nigam\n",
            "\n",
            "Embedding for the first chunk (first 10 elements):\n",
            "[0.001216432428918779, -0.0006158560863696039, -0.010030262172222137, -0.010526003316044807, -0.0073410384356975555, 0.019666647538542747, -0.02906535007059574, 0.004892893601208925, -0.007836778648197651, -0.04264729097485542]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Store the embeddings in Chroma DB"
      ],
      "metadata": {
        "id": "siDt_dqfZg3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Set up Chroma DB"
      ],
      "metadata": {
        "id": "iEcxNp1TZunj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "nHe7UwPycDVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d00294f-d3ad-4f21-d452-6ccb66b1dc16"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=fc9e62c7a5af216211d8c87db2e135378fe8e48ac1045187006f4b293865ce3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, onnxruntime, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.20 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "# Create a Chroma DB client\n",
        "client_db = chromadb.Client()\n",
        "\n",
        "# Create a collection (or get an existing one)\n",
        "collection = client_db.get_or_create_collection(name=\"my_document_embeddings\")\n",
        "\n",
        "print(f\"Chroma DB client created and collection '{collection.name}' is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doUfx-CPZxvG",
        "outputId": "c5195019-1124-4137-8f3f-0f58fef93315"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chroma DB client created and collection 'my_document_embeddings' is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Store the embeddings and the chunks"
      ],
      "metadata": {
        "id": "oeZpAo7YaMmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store embeddings in Chroma DB\n",
        "\n",
        "# Prepare data for Chroma DB\n",
        "ids = [f\"chunk_{i}\" for i in range(len(embeddings))]\n",
        "documents = list(embeddings.keys())\n",
        "embedding_vectors = list(embeddings.values())\n",
        "\n",
        "# Add to the collection\n",
        "collection.add(\n",
        "    embeddings=embedding_vectors,\n",
        "    documents=documents,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(f\"Added {len(documents)} documents and embeddings to the collection.\")\n",
        "print(f\"Collection count: {collection.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-wez9RyaQQu",
        "outputId": "064c9dd2-6114-4cb6-fd19-21bff96e715c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 369 documents and embeddings to the collection.\n",
            "Collection count: 369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37f2f246",
        "outputId": "d197ff66-411e-480d-afc5-e072207bf854"
      },
      "source": [
        "# Retrieve the first 2 items from the collection\n",
        "retrieved_items = collection.get(\n",
        "    ids=[f\"chunk_{i}\" for i in range(8)], # Assuming IDs are sequential as created\n",
        "    include=['embeddings', 'documents']\n",
        ")\n",
        "\n",
        "# Print the structured output\n",
        "if retrieved_items and retrieved_items['ids']:\n",
        "    print(\"Example of the first 2 stored entries in Chroma DB:\")\n",
        "    for i in range(len(retrieved_items['ids'])):\n",
        "        print(f\"\\n--- Entry {i+1} ---\")\n",
        "        print(f\"ID: {retrieved_items['ids'][i]}\")\n",
        "        print(f\"Document (Chunk):\")\n",
        "        print(retrieved_items['documents'][i][:500] + \"...\") # Print first 200 chars of document\n",
        "        print(f\"Embedding (first 10 elements):\")\n",
        "        print(retrieved_items['embeddings'][i][:10])\n",
        "else:\n",
        "    print(\"Could not retrieve items from the collection.\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of the first 2 stored entries in Chroma DB:\n",
            "\n",
            "--- Entry 1 ---\n",
            "ID: chunk_0\n",
            "Document (Chunk):\n",
            "Foundational \n",
            "Large Language \n",
            "Models & \n",
            "Text Generation\n",
            "Authors: Mohammadamin Barektain,  \n",
            "Anant Nawalgaria, Daniel J.Mankowitz,  \n",
            "Majd Al Merey, Yaniv Leviathan, Massimo Mascaro,  \n",
            "Matan Kalman, Elena Buchatskaya,                                     \n",
            "Aliaksei Severyn, Irina Sigler, and Antonio Gulli\n",
            "Acknowledgements\n",
            "Content contributors\n",
            "Adam Sadvovsky\n",
            "Yonghui Wu\n",
            "Andrew Dai\n",
            "Efi Kokiopolou\n",
            "Chuck Sugnet\n",
            "Aleksey Vlasenko\n",
            "Erwin Huizenga\n",
            "Aida Nematzadeh\n",
            "Ira Ktena\n",
            "Olivia Wiles\n",
            "Lavi Nigam...\n",
            "Embedding (first 10 elements):\n",
            "[ 0.00121643 -0.00061586 -0.01003026 -0.010526   -0.00734104  0.01966665\n",
            " -0.02906535  0.00489289 -0.00783678 -0.04264729]\n",
            "\n",
            "--- Entry 2 ---\n",
            "ID: chunk_1\n",
            "Document (Chunk):\n",
            "olou\n",
            "Chuck Sugnet\n",
            "Aleksey Vlasenko\n",
            "Erwin Huizenga\n",
            "Aida Nematzadeh\n",
            "Ira Ktena\n",
            "Olivia Wiles\n",
            "Lavi Nigam\n",
            "Curators and Editors\n",
            "Antonio Gulli\n",
            "Anant Nawalgaria\n",
            "Grace Mollison \n",
            "Technical Writer\n",
            "Mark Iverson\n",
            "Designer\n",
            "Michael Lanning \n",
            "2\n",
            "Foundational Large Language Models & Text GenerationFebruary 2025\fTable of contents\n",
            "Introduction \n",
            "Why language models are important \n",
            "Large language models \n",
            "Transformer \n",
            "Input preparation and embedding \n",
            "Multi-head attention \n",
            "Understanding self-attention...\n",
            "Embedding (first 10 elements):\n",
            "[-0.00030185 -0.00064617 -0.00884217 -0.00034947 -0.01029756  0.01146462\n",
            " -0.01397036  0.01710768 -0.00991312 -0.05019718]\n",
            "\n",
            "--- Entry 3 ---\n",
            "ID: chunk_2\n",
            "Document (Chunk):\n",
            "Transformer \n",
            "Input preparation and embedding \n",
            "Multi-head attention \n",
            "Understanding self-attention \n",
            "Multi-head attention: power in diversity \n",
            "Layer normalization and residual connections \n",
            "Feedforward layer  \n",
            "Encoder and decoder \n",
            "Mixture of Experts (MoE) \n",
            "Training the transformer \n",
            "Data preparation \n",
            "Training and loss function \n",
            "The evolution of transformers \n",
            "GPT-1 \n",
            "BERT \n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "11\n",
            "12\n",
            "12\n",
            "14\n",
            "15\n",
            "15\n",
            "16\n",
            "17\n",
            "20\n",
            "21\n",
            "21\n",
            "23\n",
            "23\n",
            "25\n",
            "GPT-2 \n",
            "GPT-3/3.5/4 \n",
            "LaMDA \n",
            "Gopher \n",
            "GLaM \n",
            "Chinchilla \n",
            "PaLM \n",
            "PaLM 2 \n",
            "Gemini...\n",
            "Embedding (first 10 elements):\n",
            "[-0.01370184 -0.00302025  0.00267245 -0.00752026  0.0043901   0.00089328\n",
            "  0.01678607  0.00183413 -0.02784991 -0.02732493]\n",
            "\n",
            "--- Entry 4 ---\n",
            "ID: chunk_3\n",
            "Document (Chunk):\n",
            "16\n",
            "17\n",
            "20\n",
            "21\n",
            "21\n",
            "23\n",
            "23\n",
            "25\n",
            "GPT-2 \n",
            "GPT-3/3.5/4 \n",
            "LaMDA \n",
            "Gopher \n",
            "GLaM \n",
            "Chinchilla \n",
            "PaLM \n",
            "PaLM 2 \n",
            "Gemini \n",
            "Gemma \n",
            "LLaMA \n",
            "Mixtral \n",
            "OpenAI O1 \n",
            "DeepSeek \n",
            "Other open models \n",
            "Comparison \n",
            "Fine-tuning large language models \n",
            "Supervised fine-tuning  \n",
            "Reinforcement learning from human feedback \n",
            "Parameter Efficient Fine-Tuning \n",
            "Using large language models \n",
            "Prompt engineering  \n",
            "Sampling Techniques and Parameters \n",
            "Task-based Evaluation \n",
            "Accelerating inference \n",
            "25\n",
            "27\n",
            "28\n",
            "29\n",
            "31\n",
            "31\n",
            "33\n",
            "33\n",
            "34\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "40\n",
            "41\n",
            "43\n",
            "45\n",
            "46...\n",
            "Embedding (first 10 elements):\n",
            "[-0.00777431  0.00796139  0.01739173 -0.0289354  -0.00972828  0.01514674\n",
            " -0.01264538  0.03106952 -0.03913485 -0.03442315]\n",
            "\n",
            "--- Entry 5 ---\n",
            "ID: chunk_4\n",
            "Document (Chunk):\n",
            "ask-based Evaluation \n",
            "Accelerating inference \n",
            "25\n",
            "27\n",
            "28\n",
            "29\n",
            "31\n",
            "31\n",
            "33\n",
            "33\n",
            "34\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "40\n",
            "41\n",
            "43\n",
            "45\n",
            "46\n",
            "47\n",
            "49\n",
            "52\n",
            "52\n",
            "53\n",
            "54\n",
            "57\n",
            "Trade offs \n",
            "The Quality vs Latency/Cost Tradeoff \n",
            "The Latency vs Cost Tradeoff \n",
            "Output-approximating methods \n",
            "Quantization \n",
            "Distillation \n",
            "Output-preserving methods \n",
            "Flash Attention \n",
            "Prefix Caching \n",
            "Speculative Decoding \n",
            "Batching and Parallelization \n",
            "Applications \n",
            "Code and mathematics \n",
            "Machine translation \n",
            "Text summarization \n",
            "Question-answering \n",
            "Chatbots \n",
            "Content generation...\n",
            "Embedding (first 10 elements):\n",
            "[-0.01583481  0.01136718  0.01295886 -0.01024686 -0.0009948   0.00298013\n",
            " -0.00266931 -0.00514051 -0.03516722 -0.03156032]\n",
            "\n",
            "--- Entry 6 ---\n",
            "ID: chunk_5\n",
            "Document (Chunk):\n",
            "ematics \n",
            "Machine translation \n",
            "Text summarization \n",
            "Question-answering \n",
            "Chatbots \n",
            "Content generation \n",
            "Natural language inference \n",
            "Text classification \n",
            "Text analysis \n",
            "Multimodal applications \n",
            "Summary \n",
            "Endnotes \n",
            "58\n",
            "58\n",
            "59\n",
            "60\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "63\n",
            "65\n",
            "67\n",
            "68\n",
            "71\n",
            "72\n",
            "73\n",
            "73\n",
            "74\n",
            "75\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "80\n",
            "82\n",
            "We believe that this new crop of \n",
            "technologies has the potential to \n",
            "assist, complement, empower, \n",
            "and inspire people at any time \n",
            "across almost any field.Introduction...\n",
            "Embedding (first 10 elements):\n",
            "[-0.01691382  0.00169105  0.00089328 -0.00972808 -0.00885868  0.00769289\n",
            " -0.0351976   0.00747554 -0.03767408 -0.02405345]\n",
            "\n",
            "--- Entry 7 ---\n",
            "ID: chunk_6\n",
            "Document (Chunk):\n",
            "assist, complement, empower, \n",
            "and inspire people at any time \n",
            "across almost any field.Introduction\n",
            "The advent of Large Language Models (LLMs) represents a seismic shift in the world of \n",
            "artificial intelligence.Their ability to process, generate, and understand user intent is \n",
            "fundamentally changing the way we interact with information and technology.An LLM is an advanced artificial intelligence system that specializes in processing, \n",
            "understanding, and generating human-like text....\n",
            "Embedding (first 10 elements):\n",
            "[-0.0323674   0.00829827 -0.00403904 -0.02376637 -0.01383046  0.03148665\n",
            " -0.01616994  0.01269512 -0.02501868 -0.03440412]\n",
            "\n",
            "--- Entry 8 ---\n",
            "ID: chunk_7\n",
            "Document (Chunk):\n",
            "intelligence system that specializes in processing, \n",
            "understanding, and generating human-like text.These systems are typically implemented as \n",
            "a deep neural network and are trained on massive amounts of text data.This allows them to \n",
            "learn the intricate patterns of language, giving them the ability to perform a variety of tasks, \n",
            "like machine translation, creative text generation, question answering, text summarization, \n",
            "and many more reasoning and language oriented tasks....\n",
            "Embedding (first 10 elements):\n",
            "[-0.03757926 -0.00468112 -0.00266469 -0.00312075  0.01155132  0.00770088\n",
            " -0.00957072  0.01104314 -0.00070445 -0.01829448]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Vector Search functionality"
      ],
      "metadata": {
        "id": "dnUdGS7CbHiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Covert the query into embeddings"
      ],
      "metadata": {
        "id": "lJ3Lg8EpcS4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def generate_query_embedding(query_text, client, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"Generates an embedding for a text query using OpenAI.\n",
        "\n",
        "    Args:\n",
        "        query_text (str): The input query text.\n",
        "        client: The initialized OpenAI client.\n",
        "        model (str): The OpenAI embedding model to use.\n",
        "\n",
        "    Returns:\n",
        "        list: The embedding vector for the query text, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.embeddings.create(\n",
        "            input=[query_text], # OpenAI expects a list of inputs\n",
        "            model=model\n",
        "        )\n",
        "        # Assuming the response structure contains 'data' which is a list of embedding objects\n",
        "        if response.data and len(response.data) > 0:\n",
        "            return response.data[0].embedding\n",
        "        else:\n",
        "            print(\"Warning: No embedding returned for the query.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during query embedding generation: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage (assuming 'client' is your initialized OpenAI client)\n",
        "query = \"What is Layer normalization?\"\n",
        "query_embedding = generate_query_embedding(query, client)\n",
        "\n",
        "if query_embedding:\n",
        "   print(f\"Generated embedding for the query (first 10 elements):\")\n",
        "   print(query_embedding[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBh2glugazJ1",
        "outputId": "48628134-6e02-4da3-f90c-995b080e8f50"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embedding for the query (first 10 elements):\n",
            "[0.006310196593403816, -0.007549399510025978, 0.011169618926942348, -0.006407586392015219, 0.018819766119122505, -0.0020854880567640066, -0.0057191401720047, -0.00947033241391182, -0.015206263400614262, -0.0393858328461647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Vector search"
      ],
      "metadata": {
        "id": "B_ugbBDFca0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def vector_search(query_embedding, collection, n_results=5, distance_threshold=None):\n",
        "    \"\"\"Performs a vector search in Chroma DB based on a text query embedding with an optional distance threshold.\n",
        "\n",
        "    Args:\n",
        "        query_embedding (list): The embedding vector for the query.\n",
        "        collection: The Chroma DB collection object.\n",
        "        n_results (int): The number of similar results to retrieve.\n",
        "        distance_threshold (float, optional): The maximum distance for retrieved results.\n",
        "                                              Results with a distance greater than this will be excluded.\n",
        "                                              Lower values mean higher similarity.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists: the list of most relevant document chunks\n",
        "               and the list of their corresponding distances, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    if query_embedding is None:\n",
        "        print(\"Error: Query embedding is None.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        # Build the where clause for distance filtering if a threshold is provided\n",
        "        # ChromaDB's where clause filters on metadata, not directly on the distance from the query.\n",
        "        # To filter by distance after the query, we'll retrieve more results initially\n",
        "        # and then filter them based on the returned distances.\n",
        "        results = collection.query(\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=max(n_results, 10), # Retrieve more results initially\n",
        "            include=['documents', 'distances'], # Include documents and distances in the results\n",
        "            # The 'where' clause is for metadata filtering, not distance from the query embedding\n",
        "            # where={\"distance\": {\"$lt\": distance_threshold}} # This is incorrect for query distance\n",
        "        )\n",
        "\n",
        "        # The results are returned as a dictionary, extract the documents and distances\n",
        "        # Note: The structure is a bit nested, results['documents'][0] and results['distances'][0] are lists for the first query\n",
        "        if results and results.get('documents') and results['documents'][0]:\n",
        "            retrieved_documents = results['documents'][0]\n",
        "            retrieved_distances = results['distances'][0]\n",
        "\n",
        "            # Filter results based on the distance threshold after retrieval\n",
        "            filtered_documents = []\n",
        "            filtered_distances = []\n",
        "            for i in range(len(retrieved_documents)):\n",
        "                if distance_threshold is None or retrieved_distances[i] < distance_threshold:\n",
        "                    filtered_documents.append(retrieved_documents[i])\n",
        "                    filtered_distances.append(retrieved_distances[i])\n",
        "\n",
        "            # Return the top n_results after filtering\n",
        "            return filtered_documents[:n_results], filtered_distances[:n_results]\n",
        "        else:\n",
        "            print(\"No results found for the query.\")\n",
        "            return [], []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during vector search: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Example usage (assuming 'client' is your initialized OpenAI client, 'collection' is your Chroma DB collection and query_embedding is already generated)\n",
        "query_text = \"What is Layer normalization?\"\n",
        "query_embedding = generate_query_embedding(query_text, client) # Generate embedding separately\n",
        "\n",
        "if query_embedding:\n",
        "    search_results, distances = vector_search(query_embedding, collection, n_results=3, distance_threshold=0.4) # Example with a threshold\n",
        "\n",
        "    if search_results:\n",
        "        print(f\"\\nTop {len(search_results)} most relevant chunks for the query '{query_text}':\")\n",
        "        for i, chunk in enumerate(search_results):\n",
        "            print(f\"Result {i+1} (Distance: {distances[i]:.4f}):\\n{chunk}\\n---\")"
      ],
      "metadata": {
        "id": "nzZWY3Llchpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eea4933-b3c8-46cf-e5e9-87a95f2d4a8e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 3 most relevant chunks for the query 'What is Layer normalization?':\n",
            "Result 1 (Distance: 0.2871):\n",
            "and ‘Norm’ \n",
            "corresponds to layer normalization.Layer normalization computes the mean and variance \n",
            "of the activations to normalize the activations in a given layer.This is typically performed to \n",
            "reduce covariate shift as well as improve gradient flow to yield faster convergence during \n",
            "training as well as improved overall performance.Residual connections propagate the inputs to the output of one or more layers.This has the\n",
            "---\n",
            "Result 2 (Distance: 0.3294):\n",
            "l Large Language Models & Text GenerationFebruary 2025\fLayer normalization and residual connections\n",
            "Each layer in a transformer, consisting of a multi-head attention module and a feed-forward \n",
            "layer, employs layer normalization and residual connections.This corresponds to the Add \n",
            "and Norm layer in Figure 1, where ‘Add’ corresponds to the residual connection and ‘Norm’ \n",
            "corresponds to layer normalization.Layer normalization computes the mean and variance\n",
            "---\n",
            "Result 3 (Distance: 0.3698):\n",
            "lustrated transformer.Available at:  \n",
            "https://jalammar.github.io/illustrated-transformer/.6.Ba, J.L., Kiros, J.R., & Hinton, G.E., 2016, Layer normalization.arXiv preprint arXiv:1607.06450.7.He, K., Zhang, X., Ren, S., & Sun, J., 2016, Deep residual learning for image recognition.Proceedings of the \n",
            "IEEE Conference on Computer Vision and Pattern Recognition.8.HuggingFace., 2024, Byte Pair Encoding.Available at:  \n",
            "https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt.9.Kudo, T.\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. AI Response"
      ],
      "metadata": {
        "id": "0vTqWzlXeXM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def generate_ai_response(query_text, search_results, client, model=\"gpt-4o-mini\"):\n",
        "    \"\"\"Generates an AI response based on a query and retrieved document chunks.\n",
        "\n",
        "    Args:\n",
        "        query_text (str): The original user query.\n",
        "        search_results (list): A list of relevant document chunks retrieved from the vector database.\n",
        "        client: The initialized OpenAI client.\n",
        "        model (str): The OpenAI model to use for response generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The AI-generated response, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    if not search_results:\n",
        "        return \"Could not find relevant information to answer the query.\"\n",
        "\n",
        "    # Combine the retrieved chunks into a single context string\n",
        "    context = \"\\n\\n\".join(search_results)\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            temperature=0.2,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context. You should use only the information from the provided context. If the information is avaliable in the context the say: I dont know\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Based on the following context, answer the query:\\n\\nContext:\\n{context}\\n\\nQuery: {query_text}\"}\n",
        "            ]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during AI response generation: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage (assuming 'client' is your initialized OpenAI client, 'search_results' are from the vector search)\n",
        "# query = \"What is prompt engineering?\"\n",
        "# search_results = vector_search(query, client, collection, n_results=3) # Get search results first\n",
        "\n",
        "# if search_results:\n",
        "#     ai_response = generate_ai_response(query, search_results, client)\n",
        "#     if ai_response:\n",
        "#         print(\"\\nAI Response:\")\n",
        "#         print(ai_response)\n",
        "# else:\n",
        "#     print(\"No search results to generate a response.\")"
      ],
      "metadata": {
        "id": "PRhAZuhXdSP1"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Ask questions"
      ],
      "metadata": {
        "id": "AnI-NDf2f4PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Accept a user question\n",
        "query = input(\"Please enter your question about the document: \")\n",
        "print(f\"\\nUser Query: {query}\")\n",
        "\n",
        "# Step 2: Generate embedding for the query\n",
        "query_embedding = generate_query_embedding(query, client)\n",
        "\n",
        "if query_embedding:\n",
        "    print(f\"\\nQuery Embedding (first 10 elements): {query_embedding[:10]}...\")\n",
        "\n",
        "    # Step 3: Perform vector search in Chroma DB\n",
        "    # You can adjust n_results and distance_threshold as needed\n",
        "    search_results, distances = vector_search(query_embedding, collection, n_results=5, distance_threshold=0.4)\n",
        "\n",
        "    if search_results:\n",
        "        print(f\"\\nVector Search Results ({len(search_results)} chunks found):\") # Print the number of chunks found\n",
        "        # Print the first search result and its distance\n",
        "        print(f\"Result 1 (Distance: {distances[0]:.4f}):\\n{search_results[0][:200]}...\") # Print first 200 characters of the first result\n",
        "        print(f\"Result 1 (Distance: {distances[1]:.4f}):\\n{search_results[1][:200]}...\") # Print first 200 characters of the first result\n",
        "        # Step 4: Generate AI response based on search results\n",
        "        ai_response = generate_ai_response(query, search_results, client)\n",
        "\n",
        "        if ai_response:\n",
        "            print(\"\\nAI Response:\")\n",
        "            print(ai_response)\n",
        "        else:\n",
        "            print(\"Failed to generate AI response.\")\n",
        "    else:\n",
        "        print(\"No relevant information found in the document.\")\n",
        "else:\n",
        "    print(\"Failed to generate query embedding.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXqbIRxuf-BG",
        "outputId": "be0fc833-2590-48f3-ae1d-b07324f3c51c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your question about the document: What is LSTM?\n",
            "\n",
            "User Query: What is LSTM?\n",
            "\n",
            "Query Embedding (first 10 elements): [-0.01905040815472603, -0.004589164163917303, -0.006507386453449726, -0.005726916249841452, -0.006895887199789286, -0.010718456469476223, -0.009830454364418983, 0.021534038707613945, -0.02232491411268711, -0.013271461240947247]...\n",
            "\n",
            "Vector Search Results (5 chunks found):\n",
            "Result 1 (Distance: 0.3336):\n",
            "transformers1, recurrent neural networks (RNNSs) were the popular \n",
            "approach for modeling sequences.In particular, “long short-term memory” (LSTM) and \n",
            "“gated recurrent unit” (GRU) were common architec...\n",
            "Result 1 (Distance: 0.3833):\n",
            "assist, complement, empower, \n",
            "and inspire people at any time \n",
            "across almost any field.Introduction\n",
            "The advent of Large Language Models (LLMs) represents a seismic shift in the world of \n",
            "artificial int...\n",
            "\n",
            "AI Response:\n",
            "LSTM stands for \"long short-term memory,\" which is a common architecture used in recurrent neural networks (RNNs) for modeling sequences.\n"
          ]
        }
      ]
    }
  ]
}