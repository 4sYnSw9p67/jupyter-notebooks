{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Integrations for Developers â€” Exam"
      ],
      "metadata": {
        "id": "8PySlKkufkth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "\n",
        "- This notebook is a **template** where you must put your code.  \n",
        "- You should **fill in all empty variables** and complete the code so that when I download your notebook and click **Run all**, all cells execute correctly and provide the answers.  \n",
        "- âš ï¸ **Do NOT hardcode your API key**. Use Colab environment variables (`%env OPENAI_API_KEY=your_key_here`) and access them in your code.  \n",
        "- You may **create more cells** if needed. It is recommended that your code is well-structured and split logically into separate cells.  \n",
        "- The function **`ask_ai(query)`** must be implemented by you. All queries will call this function to check your solution.  \n",
        "- âœ… **Test cases will be created by me (the instructor).** You are **not allowed to modify, remove, or add to the test cases cell**. Your code must work correctly with the provided test cases.  \n",
        "- You are **ONLY ALLOWED** to use only the following:  \n",
        "  - **Models:** OpenAI or Anthropic  \n",
        "  - **Technologies:** LangChain or vanilla Python code  \n",
        "  - **Vector Store:** Chroma DB\n",
        "\n",
        "ğŸš¨ **Any student who does not follow the template, does not stick to the required format, or whose code does not execute properly will be disqualified.**\n"
      ],
      "metadata": {
        "id": "SL2vj_IDfuxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important\n",
        "\n",
        "Fill in **all the variables** in the cell.  \n",
        "âŒ **Do NOT put your API key directly in the code.**  \n",
        "âœ… The cell must be set up to take the API key from the Colab environment variables.\n"
      ],
      "metadata": {
        "id": "a6q8k6WrgeNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6USRQ_svffQK"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# ğŸ”§ RAG Configuration Variables\n",
        "# ================================\n",
        "\n",
        "# âš ï¸ Do NOT put your API key here directly.\n",
        "# Make sure you set your API key in Colab like this:\n",
        "# %env OPENAI_API_KEY=your_key_here\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# API Key (taken from Colab environment variables)\n",
        "API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Prompt & Model Settings\n",
        "PROMPT = \"\"                       # e.g. \"Summarize the document in 3 sentences\"\n",
        "MODEL = \"gpt-4o-mini\"             # e.g. \"gpt-4\"\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"   # e.g. \"text-embedding-ada-002\"\n",
        "\n",
        "# Chunking Parameters\n",
        "CHUNK_SIZE = 300            # e.g. 500\n",
        "CHUNK_OVERLAP = 50         # e.g. 50\n",
        "TOP_N_RESULTS = 5         # e.g. 3\n",
        "\n",
        "# Generation Parameters\n",
        "OUTPUT_LENGTH = 420          # e.g. 200\n",
        "TEMPERATURE = 0.2            # e.g. 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Organization\n",
        "\n",
        "Create more cells if needed and put your code in them.  \n",
        "It is **recommended** that your code is well-structured, split logically, and kept in separate cells for clarity.\n"
      ],
      "metadata": {
        "id": "b0u-pPNriGQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ”§ Install packages\n",
        "# ================================\n",
        "\n",
        "!pip install chromadb pypdf openai tiktoken\n",
        "\n"
      ],
      "metadata": {
        "id": "W0aZh8315g55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "39b0af1c-b9bc-4759-c4fc-f6587dc1e3c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.107.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=7db40de2570cfdb16ab5fb2683b4bdc66a2f2f110918e4df9907642aa827822e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pypdf, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.36.0\n",
            "    Uninstalling opentelemetry-api-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.36.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.57b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.57b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.57b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.36.0\n",
            "    Uninstalling opentelemetry-sdk-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.36.0\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.1.0 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 posthog-5.4.0 pybase64-1.4.2 pypdf-6.0.0 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ“‚ File Upload (PDF)\n",
        "# ================================\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Upload a PDF file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get filename\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "print(f\"âœ… Uploaded file: {pdf_path}\")\n"
      ],
      "metadata": {
        "id": "Sr7fql2C5g2x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "ac0ef592-2cfc-4eec-a5f2-c19b12043f12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5eda9979-c39d-4b5e-aa04-e13695eaa410\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5eda9979-c39d-4b5e-aa04-e13695eaa410\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Prompting_Guide_101.pdf to Prompting_Guide_101.pdf\n",
            "âœ… Uploaded file: Prompting_Guide_101.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ“– Extract text from PDF\n",
        "# ================================\n",
        "\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Read PDF\n",
        "reader = PdfReader(pdf_path)\n",
        "\n",
        "# Extract text from all pages\n",
        "extracted_text = \"\"\n",
        "for page in reader.pages:\n",
        "    extracted_text += page.extract_text() + \"\\n\"\n",
        "\n",
        "# Save extracted text to a file for verification\n",
        "text_file = \"extracted_text.txt\"\n",
        "with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(extracted_text)\n",
        "\n",
        "print(f\"âœ… Text extracted and saved to {text_file} (length: {len(extracted_text)} chars)\")\n",
        "\n",
        "# Option to download file\n",
        "# from google.colab import files\n",
        "# files.download(text_file)\n"
      ],
      "metadata": {
        "id": "gEzWeB945g0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b6ec00-833e-4fa4-95cc-3f6ad5a5f801"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Text extracted and saved to extracted_text.txt (length: 111401 chars)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# âœ‚ï¸ Sentence-aware Chunking\n",
        "# ================================\n",
        "\n",
        "import re\n",
        "import tiktoken\n",
        "\n",
        "# Load tokenizer for the embedding model\n",
        "enc = tiktoken.encoding_for_model(EMBEDDING_MODEL)\n",
        "\n",
        "def num_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "def split_into_sentences(text: str):\n",
        "    # Simple regex-based sentence splitter\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "    return [s for s in sentences if s]\n",
        "\n",
        "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
        "    sentences = split_into_sentences(text)\n",
        "    chunks, current_chunk, current_tokens = [], [], 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent_tokens = num_tokens(sent)\n",
        "\n",
        "        # If adding this sentence exceeds chunk size, save current chunk\n",
        "        if current_tokens + sent_tokens > chunk_size:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            # Start new chunk with overlap from last chunk\n",
        "            overlap_tokens = []\n",
        "            while current_chunk and num_tokens(\" \".join(overlap_tokens)) < overlap:\n",
        "                overlap_tokens.insert(0, current_chunk.pop())\n",
        "            current_chunk = overlap_tokens.copy()\n",
        "            current_tokens = num_tokens(\" \".join(current_chunk))\n",
        "\n",
        "        # Add sentence to current chunk\n",
        "        current_chunk.append(sent)\n",
        "        current_tokens += sent_tokens\n",
        "\n",
        "    # Add last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Create chunks\n",
        "chunks = chunk_text(extracted_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "\n",
        "# Save chunks to file\n",
        "chunks_file = \"chunks_preview.txt\"\n",
        "with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\n\\n\")\n",
        "\n",
        "# Print statistics\n",
        "print(f\"âœ… Total Chunks: {len(chunks)}\")\n",
        "chunk_lengths = [num_tokens(c) for c in chunks]\n",
        "print(f\"ğŸ“Š Avg tokens per chunk: {sum(chunk_lengths)//len(chunk_lengths)}\")\n",
        "print(f\"ğŸ“Š Min tokens: {min(chunk_lengths)}, Max tokens: {max(chunk_lengths)}\")\n",
        "\n",
        "# Preview first chunks\n",
        "for i, chunk in enumerate(chunks[:3]):\n",
        "    print(f\"\\nğŸ” Chunk {i+1} ({num_tokens(chunk)} tokens):\\n{chunk[:400]}...\\n\")\n",
        "\n",
        "# Allow download\n",
        "# from google.colab import files\n",
        "# files.download(chunks_file)\n"
      ],
      "metadata": {
        "id": "0TX77N_c5gxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5d7335-178c-4cd3-dcba-d2799d42ebbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Total Chunks: 105\n",
            "ğŸ“Š Avg tokens per chunk: 286\n",
            "ğŸ“Š Min tokens: 120, Max tokens: 301\n",
            "\n",
            "ğŸ” Chunk 1 (261 tokens):\n",
            "1\n",
            "October 2024 edition\n",
            "A quick-start handbook \n",
            "for effective prompts\n",
            "\n",
            "2\n",
            "Writing effective prompts \n",
            "From the very beginning, Google Workspace was built to allow you to collaborate in real time with other people. Now, you can also collaborate with AI using Gemini for Google Workspace to help boost your productivity and \n",
            "creativity without sacrificing privacy or security. The embedded generative AI-p...\n",
            "\n",
            "\n",
            "ğŸ” Chunk 2 (293 tokens):\n",
            "This guide provides you with the foundational skills to write effective prompts when using Gemini for Workspace. You can think of a prompt as a conversation starter with your AI-powered assistant. You might write several \n",
            "prompts as the conversation progresses. While the possibilities are virtually endless, you can put consistent \n",
            "best practices to work today. The four main areas to consider when ...\n",
            "\n",
            "\n",
            "ğŸ” Chunk 3 (300 tokens):\n",
            "Express complete thoughts in  \n",
            "full sentences. 2. Be specific and iterate. Tell Gemini what you need it to do (summarize, write, change the tone, create). Provide as much context as possible. 3. Be concise and avoid complexity. State your request in brief â€” but specific â€” language. Avoid jargon. 4. Make it a conversation. Fine-tune your prompts if the results donâ€™t meet your expectations or if you...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# âœ‚ï¸ Semantic / Dynamic Chunking\n",
        "# ================================\n",
        "\n",
        "import re\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.encoding_for_model(EMBEDDING_MODEL)\n",
        "\n",
        "def num_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "def semantic_chunk_text(text, chunk_size=200, overlap=50):\n",
        "    # Split on paragraphs and bullet points\n",
        "    paragraphs = re.split(r'\\n\\s*\\n|â€¢', text)\n",
        "    chunks, current_chunk, current_tokens = [], [], 0\n",
        "\n",
        "    for para in paragraphs:\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "\n",
        "        tokens = num_tokens(para)\n",
        "\n",
        "        # If paragraph alone is too big, split further by sentences\n",
        "        if tokens > chunk_size:\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
        "            for sent in sentences:\n",
        "                sent_tokens = num_tokens(sent)\n",
        "                if current_tokens + sent_tokens > chunk_size:\n",
        "                    chunks.append(\" \".join(current_chunk))\n",
        "                    # Overlap handling\n",
        "                    overlap_tokens = []\n",
        "                    while current_chunk and num_tokens(\" \".join(overlap_tokens)) < overlap:\n",
        "                        overlap_tokens.insert(0, current_chunk.pop())\n",
        "                    current_chunk = overlap_tokens.copy()\n",
        "                    current_tokens = num_tokens(\" \".join(current_chunk))\n",
        "                current_chunk.append(sent)\n",
        "                current_tokens += sent_tokens\n",
        "        else:\n",
        "            if current_tokens + tokens > chunk_size:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                overlap_tokens = []\n",
        "                while current_chunk and num_tokens(\" \".join(overlap_tokens)) < overlap:\n",
        "                    overlap_tokens.insert(0, current_chunk.pop())\n",
        "                current_chunk = overlap_tokens.copy()\n",
        "                current_tokens = num_tokens(\" \".join(current_chunk))\n",
        "            current_chunk.append(para)\n",
        "            current_tokens += tokens\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Create semantic chunks\n",
        "semantic_chunks = semantic_chunk_text(extracted_text, chunk_size=200, overlap=50)\n",
        "\n",
        "print(f\"âœ… Created {len(semantic_chunks)} semantic chunks\")\n",
        "print(f\"ğŸ“Š Avg tokens: {sum(num_tokens(c) for c in semantic_chunks)//len(semantic_chunks)}\")\n",
        "print(f\"ğŸ” First chunk:\\n{semantic_chunks[0][:400]}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmTDKlNlFclP",
        "outputId": "e294ecc4-1767-4294-b02c-efc939e17e56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Created 188 semantic chunks\n",
            "ğŸ“Š Avg tokens: 190\n",
            "ğŸ” First chunk:\n",
            "1\n",
            "October 2024 edition\n",
            "A quick-start handbook \n",
            "for effective prompts 2\n",
            "Writing effective prompts \n",
            "From the very beginning, Google Workspace was built to allow you to collaborate in real time with other people. Now, you can also collaborate with AI using Gemini for Google Workspace to help boost your productivity and \n",
            "creativity without sacrificing privacy or security. The embedded generative AI-po...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ“ Contextualization of Chunks\n",
        "# ================================\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "def contextualize_chunks(chunks):\n",
        "    contextualized = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        # Use GPT to generate a short description\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Summarize this chunk in one sentence for context retrieval.\"},\n",
        "                {\"role\": \"user\", \"content\": chunk}\n",
        "            ],\n",
        "            max_tokens=50,\n",
        "            temperature=0\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        contextualized.append({\n",
        "            \"id\": f\"chunk_{i+1}\",\n",
        "            \"text\": chunk,\n",
        "            \"contextualized_text\": f\"Context: {summary}\\n\\n{chunk}\"\n",
        "        })\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f\"âœ… Contextualized {i+1}/{len(chunks)} chunks\")\n",
        "\n",
        "    return contextualized\n",
        "\n",
        "# Apply contextualization\n",
        "contextualized_chunks = contextualize_chunks(semantic_chunks)\n",
        "\n",
        "print(f\"âœ… Total contextualized chunks: {len(contextualized_chunks)}\")\n",
        "print(f\"ğŸ” Example:\\n{contextualized_chunks[0]['contextualized_text'][:400]}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99_V7m2TFmiQ",
        "outputId": "8cc78cdc-1757-420a-d439-4dbe308bebd4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Contextualized 10/188 chunks\n",
            "âœ… Contextualized 20/188 chunks\n",
            "âœ… Contextualized 30/188 chunks\n",
            "âœ… Contextualized 40/188 chunks\n",
            "âœ… Contextualized 50/188 chunks\n",
            "âœ… Contextualized 60/188 chunks\n",
            "âœ… Contextualized 70/188 chunks\n",
            "âœ… Contextualized 80/188 chunks\n",
            "âœ… Contextualized 90/188 chunks\n",
            "âœ… Contextualized 100/188 chunks\n",
            "âœ… Contextualized 110/188 chunks\n",
            "âœ… Contextualized 120/188 chunks\n",
            "âœ… Contextualized 130/188 chunks\n",
            "âœ… Contextualized 140/188 chunks\n",
            "âœ… Contextualized 150/188 chunks\n",
            "âœ… Contextualized 160/188 chunks\n",
            "âœ… Contextualized 170/188 chunks\n",
            "âœ… Contextualized 180/188 chunks\n",
            "âœ… Total contextualized chunks: 188\n",
            "ğŸ” Example:\n",
            "Context: The October 2024 edition introduces a quick-start handbook for using Gemini in Google Workspace, highlighting its generative AI features that enhance collaboration, productivity, and creativity while maintaining privacy and security across various Google applications.\n",
            "\n",
            "1\n",
            "October 2024 edition\n",
            "A quick-start handbook \n",
            "for effective prompts 2\n",
            "Writing effective prompts \n",
            "From the very beginning...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ”‘ Create Embeddings for Chunks\n",
        "# ================================\n",
        "\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "print(\"â³ Generating embeddings...\")\n",
        "\n",
        "chunks = [c[\"contextualized_text\"] for c in contextualized_chunks]\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    response = client.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=chunk\n",
        "    )\n",
        "    vector = response.data[0].embedding\n",
        "    embeddings.append({\n",
        "        \"id\": f\"chunk_{i+1}\",\n",
        "        \"text\": chunk,\n",
        "        \"embedding\": vector\n",
        "    })\n",
        "\n",
        "    if (i+1) % 10 == 0 or i == len(chunks)-1:\n",
        "        print(f\"âœ… Processed {i+1}/{len(chunks)} chunks\")\n",
        "\n",
        "# Save to JSONL file\n",
        "embeddings_file = \"chunk_embeddings.jsonl\"\n",
        "with open(embeddings_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for e in embeddings:\n",
        "        f.write(json.dumps(e) + \"\\n\")\n",
        "\n",
        "print(f\"\\nâœ… Saved embeddings to {embeddings_file} (total {len(embeddings)})\")\n",
        "\n",
        "# Allow download\n",
        "# from google.colab import files\n",
        "# files.download(embeddings_file)\n"
      ],
      "metadata": {
        "id": "t8O03AMV5guB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f655e3a7-d33e-407a-9eca-aaa553d02d3a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Generating embeddings...\n",
            "âœ… Processed 10/188 chunks\n",
            "âœ… Processed 20/188 chunks\n",
            "âœ… Processed 30/188 chunks\n",
            "âœ… Processed 40/188 chunks\n",
            "âœ… Processed 50/188 chunks\n",
            "âœ… Processed 60/188 chunks\n",
            "âœ… Processed 70/188 chunks\n",
            "âœ… Processed 80/188 chunks\n",
            "âœ… Processed 90/188 chunks\n",
            "âœ… Processed 100/188 chunks\n",
            "âœ… Processed 110/188 chunks\n",
            "âœ… Processed 120/188 chunks\n",
            "âœ… Processed 130/188 chunks\n",
            "âœ… Processed 140/188 chunks\n",
            "âœ… Processed 150/188 chunks\n",
            "âœ… Processed 160/188 chunks\n",
            "âœ… Processed 170/188 chunks\n",
            "âœ… Processed 180/188 chunks\n",
            "âœ… Processed 188/188 chunks\n",
            "\n",
            "âœ… Saved embeddings to chunk_embeddings.jsonl (total 188)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsSOIeUcJKoi",
        "outputId": "4da9f84c-41bd-4d26-e2da-bb23d4ebb558"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rank_bm25) (2.0.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ” Hybrid Search (BM25 + Chroma)\n",
        "# ================================\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "# Create Chroma client (in-memory for now, can persist later)\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "# Build BM25 index\n",
        "bm25 = BM25Okapi([c[\"text\"].split() for c in contextualized_chunks])\n",
        "\n",
        "# Rebuild Chroma collection with contextualized text\n",
        "try:\n",
        "    chroma_client.delete_collection(\"prompting_guide_hybrid\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "collection_hybrid = chroma_client.create_collection(\n",
        "    name=\"prompting_guide_hybrid\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "collection_hybrid.add(\n",
        "    ids=[c[\"id\"] for c in contextualized_chunks],\n",
        "    documents=[c[\"contextualized_text\"] for c in contextualized_chunks],\n",
        "    embeddings=[client.embeddings.create(model=EMBEDDING_MODEL, input=c[\"contextualized_text\"]).data[0].embedding for c in contextualized_chunks]\n",
        ")\n",
        "\n",
        "def hybrid_search(query, n_results=TOP_N_RESULTS, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Hybrid search using BM25 + Chroma embeddings.\n",
        "    alpha = weight for embeddings (0.5 = equal balance)\n",
        "    \"\"\"\n",
        "    # Embedding search\n",
        "    q_emb = client.embeddings.create(model=EMBEDDING_MODEL, input=query).data[0].embedding\n",
        "    chroma_results = collection_hybrid.query(query_embeddings=[q_emb], n_results=n_results*2)\n",
        "\n",
        "    # BM25 search\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "\n",
        "    # Normalize BM25 scores\n",
        "    max_score = max(bm25_scores)\n",
        "    bm25_norm = [s/max_score for s in bm25_scores]\n",
        "\n",
        "    # Combine scores\n",
        "    combined = {}\n",
        "    for i, cid in enumerate(chroma_results[\"ids\"][0]):\n",
        "        combined[cid] = {\"text\": chroma_results[\"documents\"][0][i], \"score\": (1-alpha)*(1-chroma_results[\"distances\"][0][i])}\n",
        "\n",
        "    for i, c in enumerate(contextualized_chunks):\n",
        "        if c[\"id\"] not in combined:\n",
        "            combined[c[\"id\"]] = {\"text\": c[\"contextualized_text\"], \"score\": 0}\n",
        "        combined[c[\"id\"]][\"score\"] += alpha * bm25_norm[i]\n",
        "\n",
        "    # Sort by combined score\n",
        "    ranked = sorted(combined.items(), key=lambda x: x[1][\"score\"], reverse=True)[:n_results]\n",
        "\n",
        "    return [{\"id\": rid, \"text\": rdata[\"text\"], \"score\": rdata[\"score\"]} for rid, rdata in ranked]\n",
        "\n",
        "# ğŸ” Test hybrid search\n",
        "test_query = \"How many words should effective prompts average?\"\n",
        "results = hybrid_search(test_query, n_results=3)\n",
        "\n",
        "print(f\"ğŸ” Query: {test_query}\\n\")\n",
        "for r in results:\n",
        "    print(f\"ğŸ“Œ {r['id']} (score={r['score']:.4f})\")\n",
        "    print(r[\"text\"][:400] + \"...\\n\")\n"
      ],
      "metadata": {
        "id": "hNDt5cn95gqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3df67df-f88b-4f91-ff4e-b7ecf5186c6b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Query: How many words should effective prompts average?\n",
            "\n",
            "ğŸ“Œ chunk_7 (score=0.8305)\n",
            "Context: Effective prompts for generative AI typically average around 21 words with relevant context, but users often submit shorter prompts, leading to unpredictable responses that require careful review before use.\n",
            "\n",
            "Based on what weâ€™ve learned from our users \n",
            "so far, the most fruitful prompts average around 21 words with relevant \n",
            "context, yet the prompts people try are usually less than nine wo...\n",
            "\n",
            "ğŸ“Œ chunk_6 (score=0.6214)\n",
            "Context: To enhance Gemini's output, personalize it with your Google Drive files and use the \"Make this a power prompt\" feature in Gemini Advanced to refine your prompts, aiming for around 21 words for optimal results.\n",
            "\n",
            "Personalize Geminiâ€™s output with information from your own files in Google Drive. 6. Make Gemini your prompt editor. When using Gemini Advanced, start your prompts with: â€œMake this...\n",
            "\n",
            "ğŸ“Œ chunk_8 (score=0.5313)\n",
            "Context: The document includes a table of contents outlining various topics related to effective prompt writing, administrative support, communications, customer service, and more, aimed at different roles such as executives, managers, and entrepreneurs.\n",
            "\n",
            "4\n",
            "Table of contents\n",
            "Writing effective prompts .................................................... Page 2 \n",
            "Introduction ...........................\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f0405bf"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ” Hybrid Contextual Search (with score breakdown)\n",
        "# ================================\n",
        "\n",
        "def search_chunks(query, n_results=TOP_N_RESULTS, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Hybrid search using BM25 + Chroma embeddings + contextualized chunks.\n",
        "    Returns both embedding and BM25 contributions in the score breakdown.\n",
        "    alpha = weight for BM25 (0.5 = balanced between BM25 and embeddings).\n",
        "    \"\"\"\n",
        "    # Step 1: Embed the query\n",
        "    q_emb = client.embeddings.create(model=EMBEDDING_MODEL, input=query).data[0].embedding\n",
        "\n",
        "    # Step 2: Chroma search (semantic similarity)\n",
        "    chroma_results = collection_hybrid.query(\n",
        "        query_embeddings=[q_emb],\n",
        "        n_results=n_results * 2  # get more candidates for reranking\n",
        "    )\n",
        "\n",
        "    # Step 3: BM25 search (keyword exactness)\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "    max_score = max(bm25_scores)\n",
        "    bm25_norm = [s / max_score for s in bm25_scores]\n",
        "\n",
        "    # Step 4: Combine scores with breakdown\n",
        "    combined = {}\n",
        "\n",
        "    # Add Chroma results (embedding similarity)\n",
        "    for i, cid in enumerate(chroma_results[\"ids\"][0]):\n",
        "        emb_score = 1 - chroma_results[\"distances\"][0][i]  # distance â†’ similarity\n",
        "        combined[cid] = {\n",
        "            \"text\": chroma_results[\"documents\"][0][i],\n",
        "            \"embedding_score\": emb_score,\n",
        "            \"bm25_score\": 0.0,\n",
        "            \"score\": (1 - alpha) * emb_score\n",
        "        }\n",
        "\n",
        "    # Add BM25 scores\n",
        "    for i, c in enumerate(contextualized_chunks):\n",
        "        if c[\"id\"] not in combined:\n",
        "            combined[c[\"id\"]] = {\n",
        "                \"text\": c[\"contextualized_text\"],\n",
        "                \"embedding_score\": 0.0,\n",
        "                \"bm25_score\": 0.0,\n",
        "                \"score\": 0.0\n",
        "            }\n",
        "        combined[c[\"id\"]][\"bm25_score\"] = bm25_norm[i]\n",
        "        combined[c[\"id\"]][\"score\"] += alpha * bm25_norm[i]\n",
        "\n",
        "    # Step 5: Sort & return top results\n",
        "    ranked = sorted(combined.items(), key=lambda x: x[1][\"score\"], reverse=True)[:n_results]\n",
        "\n",
        "    return [\n",
        "        {\n",
        "            \"id\": rid,\n",
        "            \"text\": rdata[\"text\"],\n",
        "            \"embedding_score\": rdata[\"embedding_score\"],\n",
        "            \"bm25_score\": rdata[\"bm25_score\"],\n",
        "            \"score\": rdata[\"score\"]\n",
        "        }\n",
        "        for rid, rdata in ranked\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "ianJyYu95gkX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ” Quick test (with score breakdown)\n",
        "test_query = \"How many words should effective prompts average?\"\n",
        "results = search_chunks(test_query, n_results=3)\n",
        "\n",
        "print(f\"ğŸ” Query: {test_query}\\n\")\n",
        "for r in results:\n",
        "    print(f\"ğŸ“Œ {r['id']} | emb={r['embedding_score']:.4f} | bm25={r['bm25_score']:.4f} | combined={r['score']:.4f}\")\n",
        "    print(r[\"text\"][:400].replace(\"\\n\", \" \") + \"...\\n\")\n"
      ],
      "metadata": {
        "id": "-DmK6emx5gcn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8b33c8-acbd-4e05-e26d-df220ea6602d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Query: How many words should effective prompts average?\n",
            "\n",
            "ğŸ“Œ chunk_7 | emb=0.6609 | bm25=1.0000 | combined=0.8305\n",
            "Context: Effective prompts for generative AI typically average around 21 words with relevant context, but users often submit shorter prompts, leading to unpredictable responses that require careful review before use.  Based on what weâ€™ve learned from our users  so far, the most fruitful prompts average around 21 words with relevant  context, yet the prompts people try are usually less than nine wo...\n",
            "\n",
            "ğŸ“Œ chunk_6 | emb=0.5649 | bm25=0.6777 | combined=0.6213\n",
            "Context: To enhance Gemini's output, personalize it with your Google Drive files and use the \"Make this a power prompt\" feature in Gemini Advanced to refine your prompts, aiming for around 21 words for optimal results.  Personalize Geminiâ€™s output with information from your own files in Google Drive. 6. Make Gemini your prompt editor. When using Gemini Advanced, start your prompts with: â€œMake this...\n",
            "\n",
            "ğŸ“Œ chunk_8 | emb=0.5457 | bm25=0.5169 | combined=0.5313\n",
            "Context: The document includes a table of contents outlining various topics related to effective prompt writing, administrative support, communications, customer service, and more, aimed at different roles such as executives, managers, and entrepreneurs.  4 Table of contents Writing effective prompts .................................................... Page 2  Introduction ...........................\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ§ª Debug Hybrid Search (Score Breakdown)\n",
        "# ================================\n",
        "\n",
        "debug_query = \"How many words should effective prompts average?\"\n",
        "\n",
        "results = search_chunks(debug_query, n_results=10, alpha=0.5)\n",
        "\n",
        "print(f\"ğŸ” Query: {debug_query}\\n\")\n",
        "for r in results:\n",
        "    txt = r[\"text\"][:300].replace(\"\\n\", \" \")\n",
        "    print(f\"ğŸ“Œ {r['id']} | emb={r['embedding_score']:.4f} | bm25={r['bm25_score']:.4f} | combined={r['score']:.4f}\")\n",
        "    print(f\"   {txt}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOCkSxrD0h4T",
        "outputId": "86e43761-b957-4fc1-b9cc-578f6eb80b51"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Query: How many words should effective prompts average?\n",
            "\n",
            "ğŸ“Œ chunk_7 | emb=0.6610 | bm25=1.0000 | combined=0.8305\n",
            "   Context: Effective prompts for generative AI typically average around 21 words with relevant context, but users often submit shorter prompts, leading to unpredictable responses that require careful review before use.  Based on what weâ€™ve learned from our users  so far, the most fruitful prompts aver\n",
            "\n",
            "ğŸ“Œ chunk_6 | emb=0.5650 | bm25=0.6777 | combined=0.6214\n",
            "   Context: To enhance Gemini's output, personalize it with your Google Drive files and use the \"Make this a power prompt\" feature in Gemini Advanced to refine your prompts, aiming for around 21 words for optimal results.  Personalize Geminiâ€™s output with information from your own files in Google Drive\n",
            "\n",
            "ğŸ“Œ chunk_173 | emb=0.4816 | bm25=0.7032 | combined=0.5924\n",
            "   Context: The guide provides tips for writing effective prompts with Gemini for Workspace to enhance productivity and includes examples of prompt iteration for brainstorming business strategies.  Learning to write effective prompts with Gemini for Workspace   will help improve your productivity and s\n",
            "\n",
            "ğŸ“Œ chunk_8 | emb=0.5458 | bm25=0.5169 | combined=0.5313\n",
            "   Context: The document includes a table of contents outlining various topics related to effective prompt writing, administrative support, communications, customer service, and more, aimed at different roles such as executives, managers, and entrepreneurs.  4 Table of contents Writing effective prompt\n",
            "\n",
            "ğŸ“Œ chunk_13 | emb=0.4844 | bm25=0.4952 | combined=0.4898\n",
            "   Context: The guide offers various prompt styles for administrative support professionals to customize and integrate into their daily tasks for improved organization and efficiency.  You will notice a variety of prompt styles. Some prompts have brackets, which  indicate where you would fill in specif\n",
            "\n",
            "ğŸ“Œ chunk_3 | emb=0.5532 | bm25=0.3837 | combined=0.4684\n",
            "   Context: Effective prompts for AI assistants should consider four main areas: persona, task, context, and format, with an emphasis on including a verb or command to guide the interaction.  You can think of a prompt as a conversation starter with your AI-powered assistant. You might write several  pr\n",
            "\n",
            "ğŸ“Œ chunk_23 | emb=0.5035 | bm25=0.4294 | combined=0.4665\n",
            "   Context: This section offers guidance on integrating prompt-writing techniques into daily tasks, particularly for creating compelling narratives and effective communication with stakeholders, exemplified by a scenario involving crafting a press release for a company acquisition.  You have to stay up\n",
            "\n",
            "ğŸ“Œ chunk_14 | emb=0.4249 | bm25=0.4374 | combined=0.4312\n",
            "   Context: This section outlines the role of an administrative support professional in maintaining team organization and efficiency, while providing guidance on using prompts to enhance collaboration with Gemini for Google Workspace, including examples for planning agendas.  7 Administrative  support \n",
            "\n",
            "ğŸ“Œ chunk_47 | emb=0.0000 | bm25=0.7379 | combined=0.3690\n",
            "   Context: The text discusses creating a customer survey to assess the effectiveness of phone calls with agents and highlights the importance of efficient decision-making for executives.  Next, you want to create a short survey that you can send after each   follow-up customer call. You open a new Goo\n",
            "\n",
            "ğŸ“Œ chunk_22 | emb=0.0000 | bm25=0.7126 | combined=0.3563\n",
            "   Context: The text describes how to use Gemini in Google Sheets to create a budget tracker for business travel, and it also outlines the responsibilities of a communications professional, emphasizing the importance of effective communication and staying updated with trends.  You open a new Google  Sh\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xUEx2jm00hlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X2CRYDa60hSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Cases (Final Cell)\n",
        "\n",
        "The final cell must contain your **test cases**.  \n",
        "When executed, the AI should provide correct answers to the given questions **based on the PDF file**.\n"
      ],
      "metadata": {
        "id": "b2S9Kbftik_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AI Query Function\n",
        "\n",
        "In this cell, you must implement the function **ask_ai(query)**.  \n",
        "This function will be the final execution point of your pipeline (RAG / LLM).  \n"
      ],
      "metadata": {
        "id": "r7v46jYqjg3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# â“ AI Query Function (with Debug Mode + Score Breakdown)\n",
        "# ================================\n",
        "\n",
        "DEBUG = True  # ğŸ” Toggle evidence printing\n",
        "\n",
        "def ask_ai(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Executes the final RAG / LLM pipeline with optional debug mode.\n",
        "    Input:\n",
        "        query (str): The question you want to ask the AI.\n",
        "    Output:\n",
        "        str: The AI's answer based on the PDF file.\n",
        "    \"\"\"\n",
        "    # Step 1: Retrieve top-N chunks\n",
        "    retrieved = search_chunks(query, n_results=TOP_N_RESULTS)\n",
        "    context = \"\\n\\n\".join([r[\"text\"] for r in retrieved])\n",
        "\n",
        "    # Debug mode: show retrieved evidence with score breakdown\n",
        "    if DEBUG:\n",
        "        print(f\"\\nğŸ” DEBUG: Retrieved {len(retrieved)} chunks for query â†’ {query}\\n\")\n",
        "        for r in retrieved:\n",
        "            print(f\"ğŸ“Œ {r['id']} | emb={r['embedding_score']:.4f} | bm25={r['bm25_score']:.4f} | combined={r['score']:.4f}\")\n",
        "            print(r[\"text\"][:300].replace(\"\\n\", \" \") + \"...\\n\")\n",
        "\n",
        "    # Step 2: Build QA prompt\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant answering questions from a company handbook.\\n\"\n",
        "        \"Only use the provided context to answer.\\n\"\n",
        "        \"If the answer is not found in the context, say: 'Not in the guide.'\\n\"\n",
        "        \"Keep answers concise and factual.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    # Step 3: Call GPT model\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        max_tokens=OUTPUT_LENGTH,\n",
        "        temperature=TEMPERATURE\n",
        "    )\n",
        "\n",
        "    # Step 4: Return the model's answer\n",
        "    return response.choices[0].message.content.strip()\n"
      ],
      "metadata": {
        "id": "GhBbP3s8imoU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Queries\n",
        "\n",
        "Use this cell to test your function with different queries.  \n",
        "The answers must be generated correctly based on the PDF file.  \n"
      ],
      "metadata": {
        "id": "3s4yDNiGjmV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ” Example Queries for Testing\n",
        "# ================================\n",
        "\n",
        "queries = [\n",
        "    \"How many words should effective prompts average?\",\n",
        "    \"List the four main areas for effective prompts.\",\n",
        "    \"What does 'persona' mean in prompt writing?\",\n",
        "    \"Name three business roles covered in this guide.\",\n",
        "    \"What is Gemini Advanced?\"\n",
        "]\n",
        "\n",
        "# Call the AI with each query\n",
        "for q in queries:\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {ask_ai(q)}\\n\")\n"
      ],
      "metadata": {
        "id": "dw5VE1YSjmHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b93a0aa-0715-4c74-fda1-346a25cafb4f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: How many words should effective prompts average?\n",
            "\n",
            "ğŸ” DEBUG: Retrieved 5 chunks for query â†’ How many words should effective prompts average?\n",
            "\n",
            "ğŸ“Œ chunk_7 | emb=0.6610 | bm25=1.0000 | combined=0.8305\n",
            "Context: Effective prompts for generative AI typically average around 21 words with relevant context, but users often submit shorter prompts, leading to unpredictable responses that require careful review before use.  Based on what weâ€™ve learned from our users  so far, the most fruitful prompts aver...\n",
            "\n",
            "ğŸ“Œ chunk_6 | emb=0.5650 | bm25=0.6777 | combined=0.6214\n",
            "Context: To enhance Gemini's output, personalize it with your Google Drive files and use the \"Make this a power prompt\" feature in Gemini Advanced to refine your prompts, aiming for around 21 words for optimal results.  Personalize Geminiâ€™s output with information from your own files in Google Drive...\n",
            "\n",
            "ğŸ“Œ chunk_8 | emb=0.5458 | bm25=0.5169 | combined=0.5313\n",
            "Context: The document includes a table of contents outlining various topics related to effective prompt writing, administrative support, communications, customer service, and more, aimed at different roles such as executives, managers, and entrepreneurs.  4 Table of contents Writing effective prompt...\n",
            "\n",
            "ğŸ“Œ chunk_13 | emb=0.4844 | bm25=0.4952 | combined=0.4898\n",
            "Context: The guide offers various prompt styles for administrative support professionals to customize and integrate into their daily tasks for improved organization and efficiency.  You will notice a variety of prompt styles. Some prompts have brackets, which  indicate where you would fill in specif...\n",
            "\n",
            "ğŸ“Œ chunk_3 | emb=0.5532 | bm25=0.3837 | combined=0.4684\n",
            "Context: Effective prompts for AI assistants should consider four main areas: persona, task, context, and format, with an emphasis on including a verb or command to guide the interaction.  You can think of a prompt as a conversation starter with your AI-powered assistant. You might write several  pr...\n",
            "\n",
            "A: Effective prompts should average around 21 words.\n",
            "\n",
            "Q: List the four main areas for effective prompts.\n",
            "\n",
            "ğŸ” DEBUG: Retrieved 5 chunks for query â†’ List the four main areas for effective prompts.\n",
            "\n",
            "ğŸ“Œ chunk_3 | emb=0.6555 | bm25=1.0000 | combined=0.8278\n",
            "Context: Effective prompts for AI assistants should consider four main areas: persona, task, context, and format, with an emphasis on including a verb or command to guide the interaction.  You can think of a prompt as a conversation starter with your AI-powered assistant. You might write several  pr...\n",
            "\n",
            "ğŸ“Œ chunk_4 | emb=0.5459 | bm25=0.6071 | combined=0.5765\n",
            "Context: The text provides guidance on creating effective prompts for Gemini in Gmail and Google Docs, emphasizing the importance of using natural language, being specific, and including a command or verb in the task.  Here is an example of a prompt using all four areas that could work well in Gmail...\n",
            "\n",
            "ğŸ“Œ chunk_2 | emb=0.0000 | bm25=0.9656 | combined=0.4828\n",
            "Context: Gemini integrates seamlessly with Workspace apps like Drive, Docs, and Gmail, allowing users to enhance workflows through effective prompt writing, focusing on persona, task, context, and format.  Gemini is accessible right where you are doing your work â€” with access to your personal  knowl...\n",
            "\n",
            "ğŸ“Œ chunk_7 | emb=0.5515 | bm25=0.3457 | combined=0.4486\n",
            "Context: Effective prompts for generative AI typically average around 21 words with relevant context, but users often submit shorter prompts, leading to unpredictable responses that require careful review before use.  Based on what weâ€™ve learned from our users  so far, the most fruitful prompts aver...\n",
            "\n",
            "ğŸ“Œ chunk_8 | emb=0.6205 | bm25=0.1912 | combined=0.4059\n",
            "Context: The document includes a table of contents outlining various topics related to effective prompt writing, administrative support, communications, customer service, and more, aimed at different roles such as executives, managers, and entrepreneurs.  4 Table of contents Writing effective prompt...\n",
            "\n",
            "A: The four main areas for effective prompts are: Persona, Task, Context, and Format.\n",
            "\n",
            "Q: What does 'persona' mean in prompt writing?\n",
            "\n",
            "ğŸ” DEBUG: Retrieved 5 chunks for query â†’ What does 'persona' mean in prompt writing?\n",
            "\n",
            "ğŸ“Œ chunk_139 | emb=0.0000 | bm25=1.0000 | combined=0.5000\n",
            "Context: You are using Gemini Advanced to analyze a recorded interview of an executive to understand their priorities and how your company can assist their organization in achieving its goals, ultimately drafting an email to propose a partnership.  Next,  you want to better understand the executive ...\n",
            "\n",
            "ğŸ“Œ chunk_12 | emb=0.0000 | bm25=0.8168 | combined=0.4084\n",
            "Context: The text emphasizes that user data in Gmail and Workspace remains private and is not used for advertising or AI training, while also introducing a guide for effective prompting with Gemini for Workspace, including examples and scenarios for various use cases.  This was the case back when we...\n",
            "\n",
            "ğŸ“Œ chunk_3 | emb=0.4295 | bm25=0.3540 | combined=0.3918\n",
            "Context: Effective prompts for AI assistants should consider four main areas: persona, task, context, and format, with an emphasis on including a verb or command to guide the interaction.  You can think of a prompt as a conversation starter with your AI-powered assistant. You might write several  pr...\n",
            "\n",
            "ğŸ“Œ chunk_23 | emb=0.4251 | bm25=0.3327 | combined=0.3789\n",
            "Context: This section offers guidance on integrating prompt-writing techniques into daily tasks, particularly for creating compelling narratives and effective communication with stakeholders, exemplified by a scenario involving crafting a press release for a company acquisition.  You have to stay up...\n",
            "\n",
            "ğŸ“Œ chunk_11 | emb=0.0000 | bm25=0.7246 | combined=0.3623\n",
            "Context: Gemini for Workspace enhances productivity and creativity by enabling users to generate personalized emails, create presentations, and summarize information while ensuring data privacy and control.  You can generate personalized emails in seconds  referencing your own Docs to pull in releva...\n",
            "\n",
            "A: In prompt writing, 'persona' refers to the role or identity that the AI assistant is expected to adopt when responding to a prompt. It helps to contextualize the task by defining who the assistant is supposed to be, such as a program manager, public relations officer, or any other relevant role.\n",
            "\n",
            "Q: Name three business roles covered in this guide.\n",
            "\n",
            "ğŸ” DEBUG: Retrieved 5 chunks for query â†’ Name three business roles covered in this guide.\n",
            "\n",
            "ğŸ“Œ chunk_161 | emb=0.3626 | bm25=1.0000 | combined=0.6813\n",
            "Context: This section introduces small business owners and entrepreneurs to AI prompts that can enhance productivity, streamline communication, and improve marketing strategies using Gemini for Google Workspace.  (Gemini in Sheets) 58 Small business  owners and  entrepreneurs As the owner of a busin...\n",
            "\n",
            "ğŸ“Œ chunk_102 | emb=0.4031 | bm25=0.5061 | combined=0.4546\n",
            "Context: The text discusses using Gemini Advanced to brainstorm business names and taglines, as well as to develop brand strategy and architecture for a company.  Can you provide three more  in that same style? (Gemini Advanced)  Now that you have a sense of what the logo could look like, you want t...\n",
            "\n",
            "ğŸ“Œ chunk_65 | emb=0.3906 | bm25=0.4827 | combined=0.4366\n",
            "Context: The user seeks to understand emerging technology trends by summarizing the top five technologies with significant potential impacts on their industry, including benefits, challenges, and implications for their company, while also looking for proactive steps to stay ahead in specific areas, ...\n",
            "\n",
            "ğŸ“Œ chunk_174 | emb=0.0000 | bm25=0.8362 | combined=0.4181\n",
            "Context: A startup founder uploads a photo of whiteboard notes from a brainstorming session to Gemini Advanced and seeks suggestions for follow-up discussion items related to the session's topic.  Prompt iteration example Use case: Brainstorm business and strategy  You just had a productive planning...\n",
            "\n",
            "ğŸ“Œ chunk_160 | emb=0.0000 | bm25=0.8083 | combined=0.4041\n",
            "Context: The text describes how a business owner uses Gemini in Gmail to create a personalized email for a customerâ€™s one-month anniversary and in Google Sheets to generate a list of gifts under $200 for new clients, highlighting the importance of efficiency in managing multiple responsibilities.  Y...\n",
            "\n",
            "A: 1. Brand Manager\n",
            "2. Chief Information Officer\n",
            "3. Startup Founder\n",
            "\n",
            "Q: What is Gemini Advanced?\n",
            "\n",
            "ğŸ” DEBUG: Retrieved 5 chunks for query â†’ What is Gemini Advanced?\n",
            "\n",
            "ğŸ“Œ chunk_137 | emb=0.6096 | bm25=0.9177 | combined=0.7637\n",
            "Context: The user utilizes Gemini in Gmail to draft and refine an email before sending it, and then proceeds to conduct research on a new customerâ€™s market strategy using Gemini Advanced.  Format  Gemini in Gmail: [Drafts email] 51 This provides a helpful starting point, but you want to try getting ...\n",
            "\n",
            "ğŸ“Œ chunk_105 | emb=0.7215 | bm25=0.4789 | combined=0.6002\n",
            "Context: Gemini Advanced is a tool used for conducting market research to identify emerging trends in an industry and for generating variations of ad copy for A/B testing based on new messaging.  (Gemini Advanced) NEW NEW Gemini Advanced 40 Use case: Conduct market research and identify trends  The ...\n",
            "\n",
            "ğŸ“Œ chunk_180 | emb=0.6025 | bm25=0.4285 | combined=0.5155\n",
            "Context: Gemini assists in drafting an email to negotiate bulk pricing with a preferred vendor after a decision is made, and also helps analyze previous years' budget data stored in a Sheet for planning purposes.  (Gemini in Docs) Gemini creates a table comparing the two different proposals. You mak...\n",
            "\n",
            "ğŸ“Œ chunk_107 | emb=0.6099 | bm25=0.4201 | combined=0.5150\n",
            "Context: The text describes the use of Gemini in various Google applications to enhance project tracking, content amplification, and meeting efficiency, including creating a project tracker in Sheets and summarizing discussions in Meet.  [Share two or three  details from their bio, such as their pre...\n",
            "\n",
            "ğŸ“Œ chunk_181 | emb=0.6036 | bm25=0.4002 | combined=0.5019\n",
            "Context: In planning and tracking budgets, you utilize Gemini Advanced to analyze past expense trends from a spreadsheet, which aids in preparing your budget proposal, while also leveraging Gemini for developing a product launch plan by simulating various launch scenarios with your marketing team.  ...\n",
            "\n",
            "A: Gemini Advanced is a tool used for conducting market research to identify emerging trends in an industry and for generating variations of ad copy for A/B testing based on new messaging.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}