{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c7d4391"
      },
      "source": [
        "# Section 1: OpenAI API Set Up\n",
        "\n",
        "To use the OpenAI API, you'll need an API key. If you don't already have one, create a key [here](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "In Colab, add the key to the secrets manager under the \"üîë\" in the left panel. Give it the name `OPENAI_API_KEY`. Then pass the key to the SDK:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39f815ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2921f1-f64d-4e0a-db52-a2493e823d5c"
      },
      "source": [
        "# Install the OpenAI Python library\n",
        "%pip install openai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.101.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6fcf56d"
      },
      "source": [
        "# Import the OpenAI library\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96e2fc1f"
      },
      "source": [
        "Before you can make any API calls, you need to initialize the OpenAI client."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "051d48f3"
      },
      "source": [
        "# Initialize the OpenAI client\n",
        "client = openai.OpenAI()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96c02c2c"
      },
      "source": [
        "Now you can make your first API call using the `gpt-4o-mini` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "317b087b",
        "outputId": "51fe9d5e-daaf-44ad-b226-500b9e6540a2"
      },
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    max_output_tokens=50,\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"–î–∞–π –º–∏ –∏—Å—Ç–æ—Ä–∏—è –∑–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è –Ω–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∏\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# The Responses API puts text outputs in `response.output_text`\n",
        "print(response.output_text)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–†–∞–∑–±–∏—Ä–∞ —Å–µ! –ï—Ç–æ –µ–¥–Ω–∞ –∫—Ä–∞—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏—è –∑–∞ –ë—ä–ª–≥–∞—Ä–∏—è:\n",
            "\n",
            "### –ò—Å—Ç–æ—Ä–∏—è –Ω–∞ –ë—ä–ª–≥–∞—Ä–∏—è\n",
            "\n",
            "–ë—ä–ª–≥–∞—Ä–∏—è –µ –µ–¥–Ω–∞ –æ—Ç –Ω–∞–π-–¥—Ä–µ–≤–Ω–∏—Ç–µ –Ω–∞—Ü–∏–∏ –≤ –ï–≤—Ä–æ–ø–∞, —Å –∫–æ—Ä–µ–Ω–∏, –¥–∞—Ç–∏—Ä–∞—â–∏ –æ—Ç V –≤–µ–∫. –ü—ä—Ä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66cc8d41"
      },
      "source": [
        "## 1. Max Output Tokens\n",
        "\n",
        "In this section, we will demonstrate the use of the `max_tokens` parameter to control the maximum length of the model's output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "269dc575",
        "outputId": "55b9ca15-81ee-42de-ae6e-59917bf30249"
      },
      "source": [
        "response_with_max_tokens = client.responses.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  input=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # Defines the role of the AI model\n",
        "    {\"role\": \"user\", \"content\": \"Give me a story about the Bulgarian history in bulgarian.\"} # Defines the role of the user\n",
        "  ],\n",
        "  max_output_tokens=50 # Limit the output to a maximum of 50 tokens\n",
        ")\n",
        "\n",
        "print(response_with_max_tokens.output_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–†–∞–∑–∫–∞–∑ –∑–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è\n",
            "\n",
            "–ë—ä–ª–≥–∞—Ä–∏—è –µ –µ–¥–Ω–∞ –æ—Ç –Ω–∞–π-–¥—Ä–µ–≤–Ω–∏—Ç–µ –¥—ä—Ä–∂–∞–≤–∏ –≤ –ï–≤—Ä–æ–ø–∞, —Å –∏—Å—Ç–æ—Ä–∏—è, –∫–æ—è—Ç–æ –¥–∞—Ç–∏—Ä–∞ –æ—Ç VI –≤–µ–∫ –ø—Ä.–Ω.–µ., –∫–æ–≥–∞—Ç–æ —Ç—Ä–∞–∫–∏—Ç–µ –Ω–∞—Å–µ–ª—è–≤–∞–ª–∏ —Ç–µ–∑–∏ –∑–µ–º–∏. –ü—Ä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91daf2ab"
      },
      "source": [
        "## 2. Temperature Parameter\n",
        "\n",
        "The `temperature` parameter controls the randomness of the model's output. Higher values like 0.8 will make the output more random and creative, while lower values like 0.2 will make it more focused and deterministic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d07381d",
        "outputId": "0e38712e-b1a2-47b7-a3f9-69fe60bf71fd"
      },
      "source": [
        "# High temperature example\n",
        "response_high_temp = client.responses.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  input=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # Defines the role of the AI model\n",
        "    {\"role\": \"user\", \"content\": \"Give me a story about the Bulgarian history in bulgarian.\"} # Defines the role of the user\n",
        "  ],\n",
        "  max_output_tokens=100,\n",
        "  temperature=0.8 # Set temperature to a higher value for more randomness\n",
        ")\n",
        "\n",
        "print(\"Response with high temperature:\")\n",
        "print(response_high_temp.output_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with high temperature:\n",
            "–†–∞–∑–∫–∞–∑ –∑–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è\n",
            "\n",
            "–ë—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è –µ –¥—ä–ª–≥–∞ –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞, –∏–∑–ø—ä–ª–Ω–µ–Ω–∞ —Å –≤–∞–∂–Ω–∏ —Å—ä–±–∏—Ç–∏—è –∏ –∫—É–ª—Ç—É—Ä–Ω–∏ –ø–æ—Å—Ç–∏–∂–µ–Ω–∏—è. –ù–∞—á–∞–ª–æ—Ç–æ –Ω–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –¥—ä—Ä–∂–∞–≤–∞ —Å–µ —Å–≤—ä—Ä–∑–≤–∞ —Å –æ—Å–Ω–æ–≤–∞–≤–∞–Ω–µ—Ç–æ –Ω–∞ –ü—ä—Ä–≤–∞—Ç–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞ –¥—ä—Ä–∂–∞–≤–∞ –ø—Ä–µ–∑ 681 –≥–æ–¥–∏–Ω–∞ –æ—Ç —Ö–∞–Ω –ê—Å–ø–∞—Ä—É—Ö. –¢–æ–π –≤–æ–¥–∏ —Å–≤–æ—è –Ω–∞—Ä–æ–¥ –ø—Ä–µ–∑ —Ä–µ–∫–∞ –î—É–Ω–∞–≤ –∏ —Å—ä–∑–¥–∞–≤–∞ –Ω–æ–≤–∞ —Ç–µ—Ä–∏—Ç–æ—Ä–∏—è, –∫–æ—è—Ç–æ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "632c3afc",
        "outputId": "e7c9ba4c-d711-41c7-8d51-8d41f3bbdc3f"
      },
      "source": [
        "# Low temperature example\n",
        "response_low_temp = client.responses.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  input=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # Defines the role of the AI model\n",
        "    {\"role\": \"user\", \"content\": \"Give me a story about the Bulgarian history in bulgarian.\"} # Defines the role of the user\n",
        "  ],\n",
        "  max_output_tokens=100,\n",
        "  temperature=0.2 # Set temperature to a lower value for less randomness\n",
        ")\n",
        "\n",
        "print(\"\\nResponse with low temperature:\")\n",
        "print(response_low_temp.output_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response with low temperature:\n",
            "–†–∞–∑–∫–∞–∑ –∑–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è\n",
            "\n",
            "–ë—ä–ª–≥–∞—Ä–∏—è –∏–º–∞ –¥—ä–ª–≥–∞ –∏ –≤—ä–ª–Ω—É–≤–∞—â–∞ –∏—Å—Ç–æ—Ä–∏—è, –∫–æ—è—Ç–æ –∑–∞–ø–æ—á–≤–∞ –æ—â–µ –≤ –¥—Ä–µ–≤–Ω–æ—Å—Ç—Ç–∞. –ü—ä—Ä–≤–∏—Ç–µ –∏–∑–≤–µ—Å—Ç–Ω–∏ –∂–∏—Ç–µ–ª–∏ –Ω–∞ —Ç–µ—Ä–∏—Ç–æ—Ä–∏—è—Ç–∞ –Ω–∞ –¥–Ω–µ—à–Ω–∞ –ë—ä–ª–≥–∞—Ä–∏—è —Å–∞ —Ç—Ä–∞–∫–∏—Ç–µ, –∫–æ–∏—Ç–æ –æ—Å—Ç–∞–≤—è—Ç —Å–ª–µ–¥ —Å–µ–±–µ —Å–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫—É–ª—Ç—É—Ä–Ω–∏ –∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏ —Å–ª–µ–¥–∏. –¢–µ —Å–∞ –∏–∑–≤–µ—Å—Ç–Ω–∏ —Å—ä—Å —Å–≤–æ–∏—Ç–µ —Ä–∏—Ç—É–∞–ª–∏, –∏–∑–∫—É—Å—Ç–≤–æ –∏ –∑–ª–∞—Ç–Ω–∏ —Å—ä–∫—Ä–æ–≤–∏—â–∞.\n",
            "\n",
            "–ü—Ä–µ–∑ VII –≤–µ–∫, –Ω–∞ —Ç–µ—Ä–∏—Ç\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "684b17ed"
      },
      "source": [
        "## 3. Top-p (Nucleus Sampling)\n",
        "\n",
        "The `top_p` parameter, also known as nucleus sampling, controls the diversity of the model's output by considering the smallest set of words whose cumulative probability exceeds the `top_p` threshold. Lower values will result in more focused and deterministic output, similar to lower temperature, while higher values will lead to more diverse and creative output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eca9972",
        "outputId": "bc4ecd3b-85d5-44cb-edc7-744b08588ae5"
      },
      "source": [
        "# High top_p example\n",
        "response_high_top_p = client.responses.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  input=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # Defines the role of the AI model\n",
        "    {\"role\": \"user\", \"content\": \"Give me a story about the Bulgarian history in bulgarian.\"} # Defines the role of the user\n",
        "  ],\n",
        "  max_output_tokens=100,\n",
        "  top_p=0.9 # Set top_p to a higher value for more diverse output\n",
        ")\n",
        "\n",
        "print(\"Response with high top_p:\")\n",
        "print(response_high_top_p.output_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with high top_p:\n",
            "–†–∞–∑–±–∏—Ä–∞ —Å–µ! –ï—Ç–æ –µ–¥–Ω–∞ –∫—Ä–∞—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏—è –∑–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è:\n",
            "\n",
            "---\n",
            "\n",
            "**–ò—Å—Ç–æ—Ä–∏—è –Ω–∞ –ë—ä–ª–≥–∞—Ä–∏—è**\n",
            "\n",
            "–ë—ä–ª–≥–∞—Ä–∏—è –µ –¥—ä—Ä–∂–∞–≤–∞ —Å –¥—ä–ª–≥–∞ –∏ –±–æ–≥–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è, –∫–æ—è—Ç–æ –¥–∞—Ç–∏—Ä–∞ –æ—Ç V –≤–µ–∫, –∫–æ–≥–∞—Ç–æ —Å–µ —Ñ–æ—Ä–º–∏—Ä–∞ –ü—ä—Ä–≤–∞—Ç–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞ –¥—ä—Ä–∂–∞–≤–∞. –ü—Ä–µ–∑ 681 –≥–æ–¥–∏–Ω–∞ —Ö–∞–Ω –ê—Å–ø–∞—Ä—É—Ö —Å—ä–∑–¥–∞–≤–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞ –¥—ä—Ä–∂–∞–≤–∞ –Ω–∞ —Ç–µ—Ä–∏—Ç–æ—Ä–∏—è—Ç–∞ –Ω–∞ –¥–Ω–µ—à–Ω–∞ –°–µ–≤–µ—Ä–æ–∏–∑—Ç–æ—á–Ω–∞ –ë—ä–ª–≥–∞—Ä–∏—è, —Å–ª–µ–¥ —É—Å–ø–µ—à–Ω–∞ –±–∏—Ç\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "543001d6",
        "outputId": "027f7050-7b62-4e11-a5f8-dfaa5f7c34da"
      },
      "source": [
        "# Low top_p example\n",
        "response_low_top_p = client.responses.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  input=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # Defines the role of the AI model\n",
        "    {\"role\": \"user\", \"content\": \"Give me a story about the Bulgarian history in bulgarian.\"} # Defines the role of the user\n",
        "  ],\n",
        "  max_output_tokens=100,\n",
        "  top_p=0.1 # Set top_p to a lower value for more focused output\n",
        ")\n",
        "\n",
        "print(\"\\nResponse with low top_p:\")\n",
        "print(response_low_top_p.output_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response with low top_p:\n",
            "–†–∞–∑–±–∏—Ä–∞ —Å–µ! –ï—Ç–æ –µ–¥–Ω–∞ –∫—Ä–∞—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏—è –∑–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è:\n",
            "\n",
            "---\n",
            "\n",
            "**–ò—Å—Ç–æ—Ä–∏—è –Ω–∞ –ë—ä–ª–≥–∞—Ä–∏—è**\n",
            "\n",
            "–ë—ä–ª–≥–∞—Ä–∏—è –∏–º–∞ –¥—ä–ª–≥–∞ –∏ –±–æ–≥–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è, –∫–æ—è—Ç–æ –∑–∞–ø–æ—á–≤–∞ –æ—â–µ –≤ –¥—Ä–µ–≤–Ω–æ—Å—Ç—Ç–∞. –ü—ä—Ä–≤–∏—Ç–µ –∏–∑–≤–µ—Å—Ç–Ω–∏ –∂–∏—Ç–µ–ª–∏ –Ω–∞ —Ç–µ—Ä–∏—Ç–æ—Ä–∏—è—Ç–∞ –Ω–∞ –¥–Ω–µ—à–Ω–∞ –ë—ä–ª–≥–∞—Ä–∏—è —Å–∞ —Ç—Ä–∞–∫–∏—Ç–µ, –∫–æ–∏—Ç–æ –æ—Å—Ç–∞–≤—è—Ç –∑–Ω–∞—á–∏—Ç–µ–ª–µ–Ω –∫—É–ª—Ç—É—Ä–µ–Ω —Å–ª–µ–¥–∞. –ü—Ä–µ–∑ 681 –≥–æ–¥–∏–Ω–∞, —Ö–∞–Ω –ê—Å–ø–∞—Ä—É—Ö –æ—Å–Ω–æ–≤–∞–≤–∞ –ü—ä—Ä–≤–∞—Ç–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞ –¥—ä—Ä–∂–∞–≤–∞,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93b13a5f"
      },
      "source": [
        "## 4. Prompt Caching\n",
        "\n",
        "Prompt caching is a technique where the language model remembers and reuses parts of a prompt from previous requests to reduce latency and cost. In the example below, we use a large static text block as part of the system message. When subsequent requests include this same static prefix, the OpenAI API can utilize caching, as indicated by the cached_tokens value in the usage details. This demonstrates how repeated elements in prompts can benefit from caching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "c5a545c5",
        "outputId": "c8ce8ca3-fdf6-4702-9b93-b3d69a8a49d4"
      },
      "source": [
        "# üì¶ Big static block to exceed 1024 tokens\n",
        "static_text = (\"This is static text for caching.\\n\" * 300)  # Adjust repeat count if needed\n",
        "\n",
        "# Static prefix = same every request ‚Üí cacheable\n",
        "STATIC_PREFIX = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"system\", \"content\": static_text}\n",
        "]\n",
        "\n",
        "def run_turn(messages):\n",
        "    res = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini-2024-07-18\",  # caching works on GPT-4o and GPT-4o-mini\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    reply = res.choices[0].message.content\n",
        "    cached = res.usage.prompt_tokens_details.cached_tokens\n",
        "    print(\"\\nAssistant:\", reply)\n",
        "    print(f\"Prompt tokens: {res.usage.prompt_tokens}\")\n",
        "    print(f\"Cached tokens: {cached}\")\n",
        "    print(f\"Completion tokens: {res.usage.completion_tokens}\")\n",
        "    return reply\n",
        "\n",
        "# üîπ TURN 1 ‚Äî First request (creates the cache)\n",
        "messages = [\n",
        "    *STATIC_PREFIX,\n",
        "    {\"role\": \"user\", \"content\": \"Give me three fun facts about Bulgaria in bulgarian.\"}\n",
        "]\n",
        "a1 = run_turn(messages)\n",
        "\n",
        "# üîπ TURN 2 ‚Äî Same static prefix, new question (prefix is cached)\n",
        "messages = [\n",
        "    *STATIC_PREFIX,\n",
        "    {\"role\": \"user\", \"content\": \"Give me three fun facts about Bulgaria in bulgarian.\"},\n",
        "    {\"role\": \"assistant\", \"content\": a1},\n",
        "    {\"role\": \"user\", \"content\": \"Give me three fun facts about Macedonia in bulgarian.\"}\n",
        "]\n",
        "run_turn(messages)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assistant: –†–∞–∑–±–∏—Ä–∞ —Å–µ! –ï—Ç–æ —Ç—Ä–∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∏ —Ñ–∞–∫—Ç–∞ –∑–∞ –ë—ä–ª–≥–∞—Ä–∏—è –Ω–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∏:\n",
            "\n",
            "1. **–°—Ç–∞—Ä–∏—è—Ç –≥—Ä–∞–¥ –ü–ª–æ–≤–¥–∏–≤** –µ –µ–¥–∏–Ω –æ—Ç –Ω–∞–π-—Å—Ç–∞—Ä–∏—Ç–µ –Ω–µ–ø—Ä–µ–∫—ä—Å–Ω–∞—Ç–æ –Ω–∞—Å–µ–ª—è–≤–∞–Ω–∏ –≥—Ä–∞–¥–æ–≤–µ –≤ —Å–≤–µ—Ç–∞, —Å –∏—Å—Ç–æ—Ä–∏—è, –∫–æ—è—Ç–æ –¥–∞—Ç–∏—Ä–∞ –ø–æ–≤–µ—á–µ –æ—Ç 6 000 –≥–æ–¥–∏–Ω–∏.\n",
            "\n",
            "2. **–ë—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ —Ä–æ–∑–∞** –µ –∏–∑–≤–µ—Å—Ç–Ω–∞ –≤ —Ü–µ–ª–∏—è —Å–≤—è—Ç. –°—Ç—Ä–∞–Ω–∞—Ç–∞ –µ –µ–¥–∏–Ω –æ—Ç –Ω–∞–π-–≥–æ–ª–µ–º–∏—Ç–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–∏ –Ω–∞ —Ä–æ–∑–æ–≤–æ –º–∞—Å–ª–æ\n",
            "Prompt tokens: 2133\n",
            "Cached tokens: 0\n",
            "Completion tokens: 100\n",
            "\n",
            "Assistant: –†–∞–∑–±–∏—Ä–∞ —Å–µ! –ï—Ç–æ —Ç—Ä–∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∏ —Ñ–∞–∫—Ç–∞ –∑–∞ –ú–∞–∫–µ–¥–æ–Ω–∏—è –Ω–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∏:\n",
            "\n",
            "1. **–û—Ö—Ä–∏–¥—Å–∫–æ—Ç–æ –µ–∑–µ—Ä–æ** –≤ –ú–∞–∫–µ–¥–æ–Ω–∏—è –µ –µ–¥–Ω–æ –æ—Ç –Ω–∞–π-–¥—ä–ª–±–æ–∫–∏—Ç–µ –∏ –Ω–∞–π-—Å—Ç–∞—Ä–∏—Ç–µ –µ–∑–µ—Ä–∞ –≤ –ï–≤—Ä–æ–ø–∞, —Å –¥—ä–ª–±–æ—á–∏–Ω–∞ –æ—Ç –æ–∫–æ–ª–æ 288 –º–µ—Ç—Ä–∞ –∏ –∏—Å—Ç–æ—Ä–∏—è, –∫–æ—è—Ç–æ –¥–∞—Ç–∏—Ä–∞ –Ω–∞–¥ 4 –º–∏–ª–∏–æ–Ω–∞ –≥–æ–¥–∏–Ω–∏.\n",
            "\n",
            "2. **–°–∫–æ–ø–∏–µ**, —Å—Ç–æ–ª–∏—Ü–∞—Ç–∞ –Ω–∞ –ú–∞–∫–µ–¥–æ–Ω–∏—è, –µ –∏–∑–≤–µ—Å—Ç–Ω–∞ —Å—ä—Å —Å–≤–æ–∏—Ç–µ –º–Ω–æ–≥\n",
            "Prompt tokens: 2253\n",
            "Cached tokens: 2176\n",
            "Completion tokens: 100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'–†–∞–∑–±–∏—Ä–∞ —Å–µ! –ï—Ç–æ —Ç—Ä–∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∏ —Ñ–∞–∫—Ç–∞ –∑–∞ –ú–∞–∫–µ–¥–æ–Ω–∏—è –Ω–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∏:\\n\\n1. **–û—Ö—Ä–∏–¥—Å–∫–æ—Ç–æ –µ–∑–µ—Ä–æ** –≤ –ú–∞–∫–µ–¥–æ–Ω–∏—è –µ –µ–¥–Ω–æ –æ—Ç –Ω–∞–π-–¥—ä–ª–±–æ–∫–∏—Ç–µ –∏ –Ω–∞–π-—Å—Ç–∞—Ä–∏—Ç–µ –µ–∑–µ—Ä–∞ –≤ –ï–≤—Ä–æ–ø–∞, —Å –¥—ä–ª–±–æ—á–∏–Ω–∞ –æ—Ç –æ–∫–æ–ª–æ 288 –º–µ—Ç—Ä–∞ –∏ –∏—Å—Ç–æ—Ä–∏—è, –∫–æ—è—Ç–æ –¥–∞—Ç–∏—Ä–∞ –Ω–∞–¥ 4 –º–∏–ª–∏–æ–Ω–∞ –≥–æ–¥–∏–Ω–∏.\\n\\n2. **–°–∫–æ–ø–∏–µ**, —Å—Ç–æ–ª–∏—Ü–∞—Ç–∞ –Ω–∞ –ú–∞–∫–µ–¥–æ–Ω–∏—è, –µ –∏–∑–≤–µ—Å—Ç–Ω–∞ —Å—ä—Å —Å–≤–æ–∏—Ç–µ –º–Ω–æ–≥'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21956c33"
      },
      "source": [
        "## 5. OpenAI Batch Processing\n",
        "\n",
        "This section demonstrates how to use OpenAI's batch processing feature to run multiple API requests asynchronously. This can be useful for processing large numbers of requests more efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aee144e"
      },
      "source": [
        "### 1. Prepare the batch input file\n",
        "\n",
        "First, we create a JSONL file containing the details for each individual API request we want to include in the batch. Each line in the file is a JSON object representing a single request."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39f847b9",
        "outputId": "c07065c4-29b9-4071-f8eb-7e6b54bf1240"
      },
      "source": [
        "import json\n",
        "import pathlib\n",
        "\n",
        "batch_lines = [\n",
        "    {\n",
        "        \"custom_id\": \"req-1\",\n",
        "        \"method\": \"POST\",\n",
        "        \"url\": \"/v1/chat/completions\",\n",
        "        \"body\": {\n",
        "            \"model\": \"gpt-4o-mini\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Give me three facts about Paris.\"}\n",
        "            ]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"custom_id\": \"req-2\",\n",
        "        \"method\": \"POST\",\n",
        "        \"url\": \"/v1/chat/completions\",\n",
        "        \"body\": {\n",
        "            \"model\": \"gpt-4o-mini\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Give me three facts about Rome.\"}\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "pathlib.Path(\"batch.jsonl\").write_text(\n",
        "    \"\\n\".join(json.dumps(line) for line in batch_lines),\n",
        "    encoding=\"utf-8\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "494"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a58a7b6b"
      },
      "source": [
        "### 2. Upload the batch input file\n",
        "\n",
        "Next, we upload the prepared JSONL file to OpenAI. This file will be used as the input for our batch job."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d65c6169",
        "outputId": "9dd18266-ed4a-4327-f34b-7d94b8c691ee"
      },
      "source": [
        "batch_file = client.files.create(file=open(\"batch.jsonl\", \"rb\"), purpose=\"batch\")\n",
        "print(\"Uploaded file:\", batch_file.id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file: file-GzeSTcTwou1myYPd1M7ErW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259a4c54"
      },
      "source": [
        "### 3. Create the batch job\n",
        "\n",
        "Now, we create the batch job itself, specifying the uploaded input file ID, the endpoint to use for the requests (in this case, chat completions), and the completion window (how long OpenAI has to process the batch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eaa4b3c",
        "outputId": "e7a821ad-9f05-4ed7-e067-260e77da092c"
      },
      "source": [
        "batch = client.batches.create(\n",
        "    input_file_id=batch_file.id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\"\n",
        ")\n",
        "print(\"Batch id:\", batch.id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch id: batch_68a210c2b83881909076ec41007ebb33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "050232e2"
      },
      "source": [
        "### 4. Poll for completion\n",
        "\n",
        "Batch jobs are processed asynchronously. We need to poll the batch status periodically until it is completed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "981e3a55",
        "outputId": "6fd43e76-613b-4f9d-839a-27d89d77b8fd"
      },
      "source": [
        "import time\n",
        "\n",
        "while True:\n",
        "    b = client.batches.retrieve(batch.id)\n",
        "    print(\"Status:\", b.status)\n",
        "    if b.status == \"completed\":\n",
        "        break\n",
        "    time.sleep(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edb7519b"
      },
      "source": [
        "### 5. Download and print results\n",
        "\n",
        "Once the batch job is completed, we can download the output file, which contains the results for each request in the batch. We then parse the JSONL output and print the responses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0f62375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ff359d-2a36-4e73-9d42-d45cbe65ef12"
      },
      "source": [
        "out_id = b.output_file_id\n",
        "out_data = client.files.content(out_id).content.decode(\"utf-8\").splitlines()\n",
        "\n",
        "for line in out_data:\n",
        "    rec = json.loads(line)\n",
        "    cid = rec[\"custom_id\"]\n",
        "    if \"response\" in rec:\n",
        "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "        print(f\"\\n{cid} ‚Üí {content}\")\n",
        "    else:\n",
        "        print(f\"\\n{cid} ‚Üí ERROR: {rec['error']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "req-1 ‚Üí Sure! Here are three facts about Paris:\n",
            "\n",
            "1. **Cultural Capital**: Paris is often referred to as the \"Cultural Capital of the World\" due to its rich history in art, fashion, literature, and philosophy. It is home to renowned museums such as the Louvre, which houses thousands of works of art including the Mona Lisa.\n",
            "\n",
            "2. **Landmarks**: The Eiffel Tower, constructed for the 1889 World's Fair, is one of the most recognizable structures in the world and a symbol of Paris. Standing at 1,083 feet (330 meters) tall, it attracts millions of visitors each year.\n",
            "\n",
            "3. **Historical Significance**: Paris has played a central role in many significant historical events, including the French Revolution. The city is known for its beautiful architecture and historic neighborhoods, such as Montmartre, which have been influential in shaping European history.\n",
            "\n",
            "req-2 ‚Üí Sure! Here are three interesting facts about Rome:\n",
            "\n",
            "1. **Ancient Civilization**: Rome is one of the oldest continuously inhabited cities in Europe, with a history that spans over 2,500 years. It was founded in 753 BC and grew from a small settlement to the center of one of the largest empires in history, the Roman Empire, which at its height controlled vast territories across Europe, North Africa, and the Middle East.\n",
            "\n",
            "2. **The Colosseum**: One of Rome's most iconic landmarks is the Colosseum, an ancient amphitheater that could hold up to 80,000 spectators. It was used for gladiatorial contests, public spectacles, and other events. The Colosseum is a UNESCO World Heritage Site and symbolizes the architectural grandeur of ancient Rome.\n",
            "\n",
            "3. **Vatican City**: Rome is home to Vatican City, the smallest independent state in the world, both in area and population. It serves as the spiritual and administrative center of the Roman Catholic Church and is the residence of the Pope. St. Peter's Basilica and the Sistine Chapel, with Michelangelo's famous ceiling, are located within Vatican City, attracting millions of visitors each year.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "480d97fb"
      },
      "source": [
        "## 6. Tool Calling\n",
        "\n",
        "This section demonstrates how to use the Tool Calling feature with OpenAI Assistants, allowing the assistant to call external functions (like a custom function or a built-in web search) to retrieve information or perform actions based on the user's request. The example below shows how to define a custom function for adding numbers and how the model can utilize both this custom function and a web search tool."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Custom function\n",
        "def add_numbers(a, b):\n",
        "    print(\"The Function is called\")\n",
        "    return {\"sum\": a + b}\n",
        "\n",
        "# Define tools - custom function + built-in web search\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"name\": \"add_numbers\",\n",
        "        \"description\": \"Add two numbers and return the sum.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"a\": {\"type\": \"number\"},\n",
        "                \"b\": {\"type\": \"number\"}\n",
        "            },\n",
        "            \"required\": [\"a\", \"b\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"web_search_preview\"\n",
        "    }\n",
        "]\n",
        "\n",
        "prompt = input(\"Your prompt: \")\n",
        "\n",
        "# Create input list\n",
        "input_list = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "# First call using Responses API\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    tools=tools,\n",
        "    input=input_list,\n",
        ")\n",
        "\n",
        "# Add response output to input list\n",
        "input_list += response.output\n",
        "\n",
        "# Check for function calls in output\n",
        "function_calls = [item for item in response.output if item.type == \"function_call\"]\n",
        "\n",
        "if function_calls:\n",
        "    # Process each function call\n",
        "    for call in function_calls:\n",
        "        if call.name == \"add_numbers\":\n",
        "            # Execute custom function\n",
        "            args = json.loads(call.arguments)\n",
        "            result = add_numbers(args[\"a\"], args[\"b\"])\n",
        "\n",
        "            # Add function result to input list\n",
        "            input_list.append({\n",
        "                \"type\": \"function_call_output\",\n",
        "                \"call_id\": call.call_id,\n",
        "                \"output\": json.dumps(result),\n",
        "            })\n",
        "        # Note: web_search_preview is handled automatically by OpenAI\n",
        "\n",
        "    # Get final response\n",
        "    final_response = client.responses.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        input=input_list,\n",
        "        tools=tools,\n",
        "    )\n",
        "\n",
        "    print(final_response.output_text)\n",
        "else:\n",
        "    print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJdT2DHRTCXD",
        "outputId": "fe42521e-65bf-455b-96e1-b8fee29b0604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your prompt: –ö–∞–∫–≤–æ –µ –≤—Ä–µ–º–µ—Ç–æ –≤ –°–æ—Ñ–∏—è?\n",
            "–í –º–æ–º–µ–Ω—Ç–∞ –≤ –°–æ—Ñ–∏—è –µ —Å–ª—ä–Ω—á–µ–≤–æ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –æ–∫–æ–ª–æ 31¬∞C (88¬∞F).\n",
            "\n",
            "## –í—Ä–µ–º–µ—Ç–æ –≤ –°–æ—Ñ–∏—è 22, –í—Ä–∞—Ü–∞, 3320 –ö–æ–∑–ª–æ–¥—É–π, –ë—ä–ª–≥–∞—Ä–∏—è:\n",
            "–í –º–æ–º–µ–Ω—Ç–∞: –°–ª—ä–Ω—á–µ–≤–æ, 88¬∞F (31¬∞C)\n",
            "\n",
            "–î–Ω–µ–≤–Ω–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞:\n",
            "* –Ω–µ–¥–µ–ª—è, –∞–≤–≥—É—Å—Ç 17: –ú–∏–Ω–∏–º–∞–ª–Ω–∞: 67¬∞F (20¬∞C), –ú–∞–∫—Å–∏–º–∞–ª–Ω–∞: 92¬∞F (34¬∞C), –ü—Ä–æ–≥–Ω–æ–∑–∞: –ü—Ä–µ–¥–∏–º–Ω–æ —Å–ª—ä–Ω—á–µ–≤–æ\n",
            "* –ø–æ–Ω–µ–¥–µ–ª–Ω–∏–∫, –∞–≤–≥—É—Å—Ç 18: –ú–∏–Ω–∏–º–∞–ª–Ω–∞: 63¬∞F (17¬∞C), –ú–∞–∫—Å–∏–º–∞–ª–Ω–∞: 86¬∞F (30¬∞C), –ü—Ä–æ–≥–Ω–æ–∑–∞: –ì—Ä—ä–º–æ—Ç–µ–≤–∏—á–Ω–∞ –±—É—Ä—è –Ω–∞ –º–µ—Å—Ç–∞ –≤ —Ä–∞–π–æ–Ω–∞\n",
            "* –≤—Ç–æ—Ä–Ω–∏–∫, –∞–≤–≥—É—Å—Ç 19: –ú–∏–Ω–∏–º–∞–ª–Ω–∞: 53¬∞F (12¬∞C), –ú–∞–∫—Å–∏–º–∞–ª–Ω–∞: 79¬∞F (26¬∞C), –ü—Ä–æ–≥–Ω–æ–∑–∞: –ì—Ä—ä–º–æ—Ç–µ–≤–∏—á–Ω–∞ –±—É—Ä—è –Ω–∞ –º–µ—Å—Ç–∞ –≤ —Ä–∞–π–æ–Ω–∞\n",
            "* —Å—Ä—è–¥–∞, –∞–≤–≥—É—Å—Ç 20: –ú–∏–Ω–∏–º–∞–ª–Ω–∞: 56¬∞F (13¬∞C), –ú–∞–∫—Å–∏–º–∞–ª–Ω–∞: 88¬∞F (31¬∞C), –ü—Ä–æ–≥–Ω–æ–∑–∞: –ü–æ-—Ç–æ–ø–ª–æ\n",
            "* —á–µ—Ç–≤—ä—Ä—Ç—ä–∫, –∞–≤–≥—É—Å—Ç 21: –ú–∏–Ω–∏–º–∞–ª–Ω–∞: 65¬∞F (18¬∞C), –ú–∞–∫—Å–∏–º–∞–ª–Ω–∞: 97¬∞F (36¬∞C), –ü—Ä–æ–≥–Ω–æ–∑–∞: –ú–Ω–æ–≥–æ –≥–æ—Ä–µ—â–æ\n",
            "* –ø–µ—Ç—ä–∫, –∞–≤–≥—É—Å—Ç 22: –ú–∏–Ω–∏–º–∞–ª–Ω–∞: 63¬∞F (17¬∞C), –ú–∞–∫—Å–∏–º–∞–ª–Ω–∞: 93¬∞F (34¬∞C), –ü—Ä–æ–≥–Ω–æ–∑–∞: –°–ª—ä–Ω—á–µ–≤–æ —Å –º–∞—Ä–∞–Ω—è\n",
            "* —Å—ä–±–æ—Ç–∞, –∞–≤–≥—É—Å—Ç 23: –ú–∏–Ω–∏–º–∞–ª–Ω–∞: 55¬∞F (13¬∞C), –ú–∞–∫—Å–∏–º–∞–ª–Ω–∞: 86¬∞F (30¬∞C), –ü—Ä–æ–≥–Ω–æ–∑–∞: –ü—Ä–µ–¥–∏–º–Ω–æ —Å–ª—ä–Ω—á–µ–≤–æ\n",
            "\n",
            "\n",
            "–ü—Ä–æ–≥–Ω–æ–∑–∞—Ç–∞ –ø–æ–∫–∞–∑–≤–∞, —á–µ –≤ —Å–ª–µ–¥–≤–∞—â–∏—Ç–µ –¥–Ω–∏ —â–µ –∏–º–∞ –ø—Ä–µ–¥–∏–º–Ω–æ —Å–ª—ä–Ω—á–µ–≤–æ –≤—Ä–µ–º–µ —Å –≤—ä–∑–º–æ–∂–Ω–∏ –≥—Ä—ä–º–æ—Ç–µ–≤–∏—á–Ω–∏ –±—É—Ä–∏ –≤ –ø–æ–Ω–µ–¥–µ–ª–Ω–∏–∫ –∏ –≤—Ç–æ—Ä–Ω–∏–∫. –û—á–∞–∫–≤–∞ —Å–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏—Ç–µ –¥–∞ –≤–∞—Ä–∏—Ä–∞—Ç –º–µ–∂–¥—É 20¬∞C (68¬∞F) –∏ 36¬∞C (97¬∞F). \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34c84358"
      },
      "source": [
        "## 7. Frequency and Presence Penalty\n",
        "\n",
        "Frequency and presence penalties can be used to control the model's tendency to repeat tokens.\n",
        "\n",
        "*   **Frequency Penalty:** Penalizes tokens based on how many times they have appeared in the text so far. Higher values reduce repetition of frequently occurring tokens.\n",
        "*   **Presence Penalty:** Penalizes tokens based on whether they appear in the text so far. Higher values encourage the model to introduce new topics or terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a6a5db1",
        "outputId": "b5695d7f-2713-43f8-b54a-a64e93ebe267"
      },
      "source": [
        "# Frequency Penalty Example\n",
        "response_freq_penalty = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # Defines the role of the AI model\n",
        "    {\"role\": \"user\", \"content\": \"Give me a story about the Bulgarian history in Bulgarian.\"} # Defines the role of the user\n",
        "  ],\n",
        "  max_tokens=100,\n",
        "  frequency_penalty=1.0 # Increase frequency penalty to reduce repetition,\n",
        ")\n",
        "\n",
        "print(\"Response with Frequency Penalty:\")\n",
        "print(response_freq_penalty.choices[0].message.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with Frequency Penalty:\n",
            "–†–∞–∑–∫–∞–∑ –∑–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è\n",
            "\n",
            "–í –Ω–∞—á–∞–ª–æ—Ç–æ –Ω–∞ IX –≤–µ–∫, –±—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –¥—ä—Ä–∂–∞–≤–∞, –æ—Å–Ω–æ–≤–∞–Ω–∞ –æ—Ç —Ö–∞–Ω –ê—Å–ø–∞—Ä—É—Ö, –∑–∞–ø–æ—á–≤–∞ –¥–∞ —Å–µ —É—Ç–≤—ä—Ä–¥–∂–∞–≤–∞ –Ω–∞ –ë–∞–ª–∫–∞–Ω—Å–∫–∏—è –ø–æ–ª—É–æ—Å—Ç—Ä–æ–≤. –•–∞–Ω –ê—Å–ø–∞—Ä—É—Ö –µ –≤–æ–¥–∏–ª –±–∏—Ç–∫–∏ —Å –í–∏–∑–∞–Ω—Ç–∏–π—Å–∫–∞—Ç–∞ –∏–º–ø–µ—Ä–∏—è –∏ –ø—Ä–µ–∑ 681 –≥–æ–¥–∏–Ω–∞ —É—Å–ø—è–≤–∞ –¥–∞ —Å—ä–∑–¥–∞–¥–µ –ø—ä—Ä–≤–∞—Ç–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞ –¥—ä—Ä–∂–∞–≤–∞ –≤ –∑–µ–º–∏—Ç–µ –æ–∫–æ–ª–æ —Ä–µ–∫–∞ –î—É–Ω–∞–≤.\n",
            "\n",
            "–°–∫–æ—Ä–æ —Å–ª–µ–¥\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "818cd405",
        "outputId": "dc4777ad-0728-4863-d40d-f980a28f0777"
      },
      "source": [
        "# Negative Frequency Penalty Example\n",
        "response_neg_freq_penalty = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # Defines the role of the AI model\n",
        "    {\"role\": \"user\", \"content\": \"Give me a story about the Bulgarian history in Bulgarian.\"} # Defines the role of the user\n",
        "  ],\n",
        "  max_tokens=100,\n",
        "  frequency_penalty=-2.0 # Decrease frequency penalty to encourage repetition\n",
        ")\n",
        "\n",
        "print(\"\\nResponse with Negative Frequency Penalty:\")\n",
        "print(response_neg_freq_penalty.choices[0].message.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response with Negative Frequency Penalty:\n",
            "–†–∞–∑–±–∏—Ä–∞ —Å–µ! –ï—Ç–æ –µ–¥–Ω–∞ –∫—Ä–∞—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏—è –∑–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è:\n",
            "\n",
            "### –ò—Å—Ç–æ—Ä–∏—è –Ω–∞ –ë—ä–ª–≥–∞—Ä–∏—è\n",
            "\n",
            "–ë—ä–ª–≥–∞—Ä–∏—è –µ –µ–¥–Ω–∞ –æ—Ç –Ω–∞–π-–¥—Ä–µ–≤–Ω–∏—Ç–µ –¥—ä—Ä–∂–∞–≤–∏ –≤ –ï–≤—Ä–æ–ø–∞, —Å –∏—Å—Ç–æ—Ä–∏—è, –∫–æ—è—Ç–æ —Å–µ –ø—Ä–æ—Å—Ç–∏—Ä–∞ —Ö–∏–ª—è–¥–æ–ª–µ—Ç–∏—è –Ω–∞–∑–∞–¥. –ü—ä—Ä–≤–æ—Ç–æ –±—ä–ª–≥–∞—Ä—Å–∫–æ –¥—ä—Ä–∂–∞–≤–Ω–æ –æ–±—Ä–∞–∑—É–≤–∞–Ω–∏–µ, –ü—ä—Ä–≤–æ—Ç–æ –±—ä–ª–≥–∞—Ä—Å–∫–æ —Ü–∞—Ä—Å—Ç–≤–æ, –µ –æ—Å–Ω–æ–≤–∞–Ω–æ –ø—Ä–µ–∑ 681 –≥–æ–¥–∏–Ω–∞, –∫–æ–≥–∞—Ç–æ —Ö–∞–Ω –ê—Å–ø–∞—Ä—É—Ö, –≤–æ–∂–¥ –Ω–∞ –±—ä–ª–≥–∞—Ä–∏—Ç–µ, —Å–µ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61a0a7eb",
        "outputId": "33f105ca-0fe6-4ad9-f99c-862675c9073a"
      },
      "source": [
        "# Presence Penalty Example\n",
        "response_pres_penalty = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # Defines the role of the AI model\n",
        "    {\"role\": \"user\", \"content\": \"Give me a story about the Bulgarian history in Bulgarian.\"} # Defines the role of the user\n",
        "  ],\n",
        "  max_tokens=100,\n",
        "  presence_penalty=1.0 # Increase presence penalty to encourage new topics\n",
        ")\n",
        "\n",
        "print(\"\\nResponse with Presence Penalty:\")\n",
        "print(response_pres_penalty.choices[0].message.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response with Presence Penalty:\n",
            "–†–∞–∑–∫–∞–∑ –∑–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∞—Ç–∞ –∏—Å—Ç–æ—Ä–∏—è\n",
            "\n",
            "–ë—ä–ª–≥–∞—Ä–∏—è –∏–º–∞ –¥—ä–ª–≥–∞ –∏ –∑–∞–±–µ–ª–µ–∂–∏—Ç–µ–ª–Ω–∞ –∏—Å—Ç–æ—Ä–∏—è, –∫–æ—è—Ç–æ –∑–∞–ø–æ—á–≤–∞ –ø—Ä–µ–¥–∏ –ø–æ–≤–µ—á–µ –æ—Ç 1300 –≥–æ–¥–∏–Ω–∏. –ü—ä—Ä–≤–æ—Ç–æ –±—ä–ª–≥–∞—Ä—Å–∫–æ —Ü–∞—Ä—Å—Ç–≤–æ –µ –æ—Å–Ω–æ–≤–∞–Ω–æ –ø—Ä–µ–∑ 681 –≥. –æ—Ç —Ö–∞–Ω –ê—Å–ø–∞—Ä—É—Ö. –°—ä—Å —Å–≤–æ–µ—Ç–æ –æ–±–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ –ø–ª–µ–º–µ–Ω–∞, —Ç–æ–π –ø–æ–ª–∞–≥–∞ –æ—Å–Ω–æ–≤–∏—Ç–µ –Ω–∞ –µ–¥–Ω–∞ –æ—Ç –ø—ä—Ä–≤–∏—Ç–µ –¥—ä—Ä–∂–∞–≤–∏ –≤ –ï–≤—Ä–æ–ø–∞.\n",
            "\n",
            "–ü—Ä–µ–∑ 864 –≥. –∫–Ω—è–∑ –ë–æ—Ä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5197eb7",
        "outputId": "ea1b4114-2b05-4b7e-9087-99d809363d6a"
      },
      "source": [
        "# Negative Presence Penalty Example\n",
        "response_neg_pres_penalty = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # Defines the role of the AI model\n",
        "    {\"role\": \"user\", \"content\": \"Give me a story about the Bulgarian history in Bulgarian.\"} # Defines the role of the user\n",
        "  ],\n",
        "  max_tokens=100,\n",
        "  presence_penalty=-2.0 # Decrease presence penalty to discourage new topics\n",
        ")\n",
        "\n",
        "print(\"\\nResponse with Negative Presence Penalty:\")\n",
        "print(response_neg_pres_penalty.choices[0].message.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response with Negative Presence Penalty:\n",
            "–ò—Å—Ç–æ—Ä–∏—è—Ç–∞ –Ω–∞ –ë—ä–ª–≥–∞—Ä–∏—è –µ –¥—ä–ª–≥–∞ –∏ –æ—Å–ø–æ—Ä–≤–∞–Ω–∞, –∏–∑–ø—ä–ª–Ω–µ–Ω–∞ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–∞–∂–Ω–∏ —Å—ä–±–∏—Ç–∏—è –∏ –∫—É–ª—Ç—É—Ä–Ω–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n",
            "\n",
            "–°—ä–∑–¥–∞–≤–∞–Ω–µ—Ç–æ –Ω–∞ –ü—ä—Ä–≤–æ—Ç–æ –±—ä–ª–≥–∞—Ä—Å–∫–æ —Ü–∞—Ä—Å—Ç–≤–æ –µ –¥–∞—Ç–∏—Ä–∞–Ω–æ –æ–∫–æ–ª–æ 681 –≥–æ–¥–∏–Ω–∞, –∫–æ–≥–∞—Ç–æ —Ö–∞–Ω –ê—Å–ø–∞—Ä—É—Ö, —Ö–∞–Ω –Ω–∞ –±—ä–ª–≥–∞—Ä–∏—Ç–µ, —Å—ä–∑–¥–∞–≤–∞ –¥—ä—Ä–∂–∞–≤–∞ –Ω–∞ –ë–∞–ª–∫–∞–Ω–∏—Ç–µ. –ë—ä–ª–≥–∞—Ä–∏—Ç–µ, –∫–æ–∏—Ç–æ –ø—Ä–æ–∏–∑—Ö–æ–∂–¥–∞—Ç –æ—Ç –¶–µ–Ω—Ç—Ä–∞–ª–Ω–∞ –ê–∑–∏—è, —É—Å–ø—è–≤–∞—Ç\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36df057f"
      },
      "source": [
        "## 8. Streaming\n",
        "\n",
        "Streaming allows the model's response to be sent back in chunks as it's being generated, rather than waiting for the entire response to be ready. This can improve the perceived latency and user experience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cb31e3e",
        "outputId": "df1e4498-aff8-422e-ec5e-57e5e2f6046b"
      },
      "source": [
        "import sys\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain black holes in two short paragraphs.\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    stream=True,   # ‚Üê enable streaming\n",
        ")\n",
        "\n",
        "# Print tokens as they arrive\n",
        "for chunk in stream:\n",
        "    delta = chunk.choices[0].delta\n",
        "    if delta and delta.content: # Corrected access using delta.content\n",
        "        sys.stdout.write(delta.content)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "print()  # newline at the end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Black holes are regions in space where the gravitational pull is so strong that nothing, not even light, can escape from them. They are formed when massive stars exhaust their nuclear fuel and collapse under their own gravity at the end of their life cycle. The boundary surrounding a black hole is called the event horizon, beyond which no information or matter can escape. Black holes can vary in size, with stellar black holes formed from individual stars and supermassive black holes found at the centers of galaxies, containing millions to billions of times the mass of the Sun.\n",
            "\n",
            "Despite being invisible, black holes can be detected through their interactions with nearby matter. When a black hole pulls in gas and dust from a companion star, the material heats up and emits X-rays, which can be observed by telescopes. Additionally, black holes influence the motion of stars and gas in their vicinity, providing indirect evidence of their existence. The study of black holes not only deepens our understanding of gravity and the nature of space-time but also poses intriguing questions about the fundamental laws of physics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "150461fc"
      },
      "source": [
        "##9. Sending Images to GPT-4o Mini\n",
        "\n",
        "GPT-4o mini can process images. This section demonstrates how to send an image to the model and get a description of its content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06e83114",
        "outputId": "688c851c-fdb5-40ec-f0a5-16657147bfea"
      },
      "source": [
        "import base64\n",
        "\n",
        "# Function to encode the image to base64\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image file\n",
        "image_path = \"/content/image_1.jpeg\" # Make sure 'image_1' is uploaded to your Colab environment\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "response = client.responses.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  input=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes images.\"},\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "      {\"type\": \"input_text\", \"text\": \"Describe this image:\"},\n",
        "      {\"type\": \"input_image\", \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
        "    ]}\n",
        "  ],\n",
        "  max_output_tokens=300,\n",
        ")\n",
        "\n",
        "print(\"Description of the image:\")\n",
        "print(response.output_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description of the image:\n",
            "The image features a cat with a soft, fluffy coat that appears to be a mix of gray and cream colors. Its face is round with large, expressive eyes that give it a curious and endearing look. The cat is sitting with its front paws neatly together, and its ears are perked up, indicating attentiveness. The background is plain, emphasizing the cat's features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6894836b"
      },
      "source": [
        "## 10. OpenAI Assistants\n",
        "\n",
        "This section demonstrates how to use the OpenAI Assistants API, which allows you to build AI assistants that can understand context, maintain conversation history, and leverage tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88140149"
      },
      "source": [
        "### 1. Create a new thread\n",
        "\n",
        "Threads represent a conversation session between a user and an Assistant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeafeecf",
        "outputId": "5a35d830-a513-44b6-b89a-b000e1863d53"
      },
      "source": [
        "# 1. Create a new thread\n",
        "thread = client.beta.threads.create()\n",
        "print(\"Thread created with ID:\", thread.id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2209946075.py:2: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
            "  thread = client.beta.threads.create()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thread created with ID: thread_jcYbmapE1AoMjKIKMY7XouZu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "906ffcf2"
      },
      "source": [
        "### 2. Add a user message to the thread\n",
        "\n",
        "Messages are added to the thread. They can be from the user or the Assistant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d8a09ba",
        "outputId": "c8ad9afb-f577-493a-f420-b6ac9d55cd34"
      },
      "source": [
        "# 2. Add a user message to the thread\n",
        "client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"Hello, can you introduce yourself?\"\n",
        ")\n",
        "print(\"User message added to thread.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3897994378.py:2: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
            "  client.beta.threads.messages.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User message added to thread.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e091c1be"
      },
      "source": [
        "### 3. Run the assistant on the thread\n",
        "\n",
        "A Run is an execution of an Assistant on a Thread. The Assistant uses its configuration and the Thread's messages to generate a response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce614e23",
        "outputId": "838a1a35-9f3f-4509-8164-dee030e67538"
      },
      "source": [
        "# 3. Run the assistant on the thread\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=\"asst_lcgYb6wZaRp6UDxdhTi4qPoq\" # Replace with your Assistant ID if needed\n",
        ")\n",
        "print(\"Assistant run initiated with ID:\", run.id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4048927047.py:2: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
            "  run = client.beta.threads.runs.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant run initiated with ID: run_QJe5759huBqGZj8uvqzgmWku\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "367125d5"
      },
      "source": [
        "### 4. Wait for completion and fetch messages\n",
        "\n",
        "We poll the run status until it is completed, then retrieve the messages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da71f596",
        "outputId": "7bfb2c06-0322-425b-c7ba-b566a3cc2942"
      },
      "source": [
        "# 4. Wait for completion and fetch messages\n",
        "import time\n",
        "\n",
        "while True:\n",
        "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "    print(\"Run status:\", run_status.status)\n",
        "    if run_status.status == \"completed\":\n",
        "        break\n",
        "    time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2652278406.py:5: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
            "  run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run status: completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c82f2ba"
      },
      "source": [
        "### 5. Retrieve and display messages\n",
        "\n",
        "Finally, we retrieve all messages from the thread and display them in chronological order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "876770c1",
        "outputId": "19615650-a42d-492c-e7bc-672643e8899d"
      },
      "source": [
        "# 5. Retrieve all messages and display\n",
        "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "\n",
        "for m in reversed(messages.data):\n",
        "    role = m.role\n",
        "    # Check if content exists and is of type text\n",
        "    if m.content and m.content[0].type == 'text':\n",
        "        content = m.content[0].text.value\n",
        "        print(f\"{role}: {content}\")\n",
        "    elif m.content and m.content[0].type == 'image_file':\n",
        "        # Handle image files if necessary, or just note them\n",
        "        print(f\"{role}: [Image file]\")\n",
        "    else:\n",
        "        print(f\"{role}: [Unsupported content type]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-694709701.py:2: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
            "  messages = client.beta.threads.messages.list(thread_id=thread.id)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: Hello, can you introduce yourself?\n",
            "user: Hello, can you introduce yourself?\n",
            "assistant: Hello! I'm ChatGPT, an AI language model created by OpenAI. I'm here to help you with a wide range of tasks, from answering questions and providing explanations to generating creative text and assisting with problem-solving. How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c498b47"
      },
      "source": [
        "# Section 2: Anthropic API Set Up\n",
        "\n",
        "To use the Anthropic API, you'll need an API key. If you don't already have one, create a key [here](https://console.anthropic.com/settings/keys).\n",
        "\n",
        "In Colab, add the key to the secrets manager under the \"üîë\" in the left panel. Give it the name `ANTHROPIC_API_KEY`. Then pass the key to the SDK:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9c02083",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20587ec-debb-4214-d33b-8564a3fa057f"
      },
      "source": [
        "# Install the Anthropic Python library\n",
        "%pip install anthropic\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.64.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.64.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m297.2/297.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.64.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Anthropic library\n",
        "import anthropic\n",
        "import os\n",
        "\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')"
      ],
      "metadata": {
        "id": "eHmxctxWY1dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = anthropic.Anthropic()"
      ],
      "metadata": {
        "id": "5DgRMXDIZRhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=1500,\n",
        "    thinking={\n",
        "        \"type\": \"enabled\",\n",
        "        \"budget_tokens\": 1024\n",
        "    },\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Are there an infinite number of prime numbers such that n mod 4 == 3?\"\n",
        "    }]\n",
        ")\n",
        "\n",
        "# The response will contain summarized thinking blocks and text blocks\n",
        "for block in response.content:\n",
        "    if block.type == \"thinking\":\n",
        "        print(f\"\\nThinking summary: {block.thinking}\")\n",
        "    elif block.type == \"text\":\n",
        "        print(f\"\\nResponse: {block.text}\")"
      ],
      "metadata": {
        "id": "qv1UFFbAZMpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1377bbb-1856-42c3-f68a-14bded993875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Thinking summary: This is asking about primes that are congruent to 3 modulo 4. Let me think about this.\n",
            "\n",
            "First, let me recall what we know about primes modulo 4:\n",
            "- The only even prime is 2\n",
            "- All other primes are odd, so they're either ‚â° 1 (mod 4) or ‚â° 3 (mod 4)\n",
            "\n",
            "The question is asking whether there are infinitely many primes p such that p ‚â° 3 (mod 4).\n",
            "\n",
            "This is actually a classic theorem in number theory. The answer is yes, and there's a beautiful proof similar to Euclid's proof that there are infinitely many primes.\n",
            "\n",
            "Let me sketch the proof:\n",
            "\n",
            "Suppose there are only finitely many primes congruent to 3 modulo 4. Let's call them p‚ÇÅ, p‚ÇÇ, ..., p‚Çñ.\n",
            "\n",
            "Consider the number N = 4(p‚ÇÅp‚ÇÇ...p‚Çñ) - 1.\n",
            "\n",
            "Note that N ‚â° 4(p‚ÇÅp‚ÇÇ...p‚Çñ) - 1 ‚â° -1 ‚â° 3 (mod 4).\n",
            "\n",
            "Now, N must have prime divisors. Since N is odd, all its prime divisors are odd, so they're either ‚â° 1 (mod 4) or ‚â° 3 (mod 4).\n",
            "\n",
            "If all prime divisors of N were ‚â° 1 (mod 4), then N would be ‚â° 1 (mod 4) (since the product of numbers ‚â° 1 (mod 4) is ‚â° 1 (mod 4)). But we showed N ‚â° 3 (mod 4), which is a contradiction.\n",
            "\n",
            "Therefore, N must have at least one prime divisor q that is ‚â° 3 (mod 4).\n",
            "\n",
            "But q cannot be any of p‚ÇÅ, p‚ÇÇ, ..., p‚Çñ because:\n",
            "\n",
            "\n",
            "N is not divisible by any of the original primes. This means q is a new prime ‚â° 3 (mod 4), contradicting the assumption of a complete list of such primes.\n",
            "\n",
            "The proof demonstrates there are infinitely many primes congruent to 3 modulo 4, using a clever construction that always finds a new prime of this type.\n",
            "\n",
            "Response: Yes, there are infinitely many prime numbers such that n ‚â° 3 (mod 4).\n",
            "\n",
            "This is a classic theorem in number theory, and here's a proof using a technique similar to Euclid's proof for the infinitude of primes:\n",
            "\n",
            "**Proof by contradiction:**\n",
            "\n",
            "Assume there are only finitely many primes congruent to 3 modulo 4. Let's call them p‚ÇÅ, p‚ÇÇ, ..., p‚Çñ.\n",
            "\n",
            "Consider the number:\n",
            "N = 4(p‚ÇÅp‚ÇÇ...p‚Çñ) - 1\n",
            "\n",
            "Note that N ‚â° -1 ‚â° 3 (mod 4).\n",
            "\n",
            "Since N is odd and greater than 1, it must have prime divisors. Every odd prime is either ‚â° 1 (mod 4) or ‚â° 3 (mod 4).\n",
            "\n",
            "**Key observation:** If all prime divisors of N were ‚â° 1 (mod 4), then N itself would be ‚â° 1 (mod 4), since the product of numbers congruent to 1 modulo 4 is also congruent to 1 modulo 4.\n",
            "\n",
            "But we established that N ‚â° 3 (mod 4), so N must have at least one prime divisor q where q ‚â° 3 (mod 4).\n",
            "\n",
            "However, this prime q cannot be any of our original primes p‚ÇÅ, p‚ÇÇ, ..., p‚Çñ because:\n",
            "- N = 4(p‚ÇÅp‚ÇÇ...p‚Çñ) - 1\n",
            "- So N ‚â° -1 (mod p·µ¢) for each i\n",
            "- Therefore, no p·µ¢ divides N\n",
            "\n",
            "This means q is a new prime ‚â° 3 (mod 4) not in our original finite list, contradicting our assumption.\n",
            "\n",
            "Therefore, there must be infinitely many primes congruent to 3 modulo 4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e82557f"
      },
      "source": [
        "## 1. Streaming\n",
        "\n",
        "This section demonstrates how to stream responses from the Anthropic API, which can provide a faster perceived response time by delivering the model's output in chunks as it is generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70e0d77c",
        "outputId": "0032b5d2-c45a-4344-8752-b06510f8c8c0"
      },
      "source": [
        "from anthropic import Anthropic\n",
        "\n",
        "client = Anthropic()\n",
        "\n",
        "with client.messages.stream(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=100,\n",
        "    temperature=0.3,\n",
        "    top_p=0.9,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Explain transformers in 3 bullets.\"}],\n",
        ") as stream:\n",
        "    for text in stream.text_stream:\n",
        "        print(text, end=\"\")   # live output\n",
        "    final = stream.get_final_message()\n",
        "\n",
        "# (Optional) print a newline after streaming\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚Ä¢ **Architecture**: Transformers use self-attention mechanisms to process all parts of an input sequence simultaneously, rather than sequentially, allowing them to capture relationships between any two positions regardless of distance.\n",
            "\n",
            "‚Ä¢ **Key Innovation**: The \"attention is all you need\" approach eliminates the need for recurrent or convolutional layers, using multi-head attention to focus on different aspects of the input and enabling much more efficient parallel processing.\n",
            "\n",
            "‚Ä¢ **Impact**: They've revolutionized N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a66f2c69"
      },
      "source": [
        "### 3. Prompt Caching\n",
        "\n",
        "Anthropic models support prompt caching to improve efficiency and reduce latency for repeated parts of the input. By marking certain system blocks with `cache_control`, you can indicate to the API that this content is stable and can potentially be cached for subsequent requests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3634bb27",
        "outputId": "42a1287b-88ce-47a1-cf31-13fb19c0f6b5"
      },
      "source": [
        "# Big reusable block (simulating large context)\n",
        "BIG_CONTEXT = \"Pride and Prejudice \" * 50\n",
        "\n",
        "system_blocks = [\n",
        "    {\"type\": \"text\", \"text\": \"You are a helpful literature assistant.\"},\n",
        "    {\"type\": \"text\", \"text\": BIG_CONTEXT, \"cache_control\": {\"type\": \"ephemeral\"}}\n",
        "]\n",
        "\n",
        "def ask(question: str):\n",
        "    resp = client.messages.create(\n",
        "        model=\"claude-sonnet-4-20250514\",\n",
        "        max_tokens=100,\n",
        "        system=system_blocks,\n",
        "        messages=[{\"role\": \"user\", \"content\": question}]\n",
        "    )\n",
        "    print(resp.content[0].text)\n",
        "    u = resp.usage\n",
        "    print(f\"Usage: input={u.input_tokens} output={u.output_tokens} \"\n",
        "          f\"cache_create={getattr(u, 'cache_creation_input_tokens', 0)} \"\n",
        "          f\"cache_read={getattr(u, 'cache_read_input_tokens', 0)}\\n\")\n",
        "\n",
        "print(\"---- First call (creates cache) ----\")\n",
        "ask(\"Name two central themes in the book.\")\n",
        "\n",
        "print(\"---- Second call (reads from cache) ----\")\n",
        "ask(\"Briefly describe Elizabeth Bennet‚Äôs character arc.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- First call (creates cache) ----\n",
            "Two central themes in \"Pride and Prejudice\n",
            "Usage: input=273 output=10 cache_create=0 cache_read=0\n",
            "\n",
            "---- Second call (reads from cache) ----\n",
            "Two central themes in \"Pride and Prejudice\n",
            "Usage: input=273 output=10 cache_create=0 cache_read=0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 3: Gemini API Set Up**"
      ],
      "metadata": {
        "id": "jdmnsB1ALVnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbyex7r4Lla2",
        "outputId": "75a15664-4493-4288-87c8-9fca12e72751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/43.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/229.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m225.3/229.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make your first request**\n",
        "\n",
        "Here is an example that uses the generateContent method to send a request to the Gemini API using the Gemini 2.5 Flash model."
      ],
      "metadata": {
        "id": "Wa9SjIbsLrdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
        "client = genai.Client()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l91RAp-ZLyyg",
        "outputId": "abf2d873-83a6-413c-ac7f-4f60ee9ae9bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI learns patterns from data to make smart decisions or predictions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gamini Flash Thinking"
      ],
      "metadata": {
        "id": "IXHUqFcWNSvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain how AI works in a few words\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking\n",
        "    ),\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08WwKxe1NWUs",
        "outputId": "023bc105-9a12-401e-8b2c-e70956d0ec46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI works by finding patterns in data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Provide a list of 3 famous physicists and their key contributions\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=1024) # Disables thinking\n",
        "    ),\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ0PDIOGNxsX",
        "outputId": "2d027981-f269-4f2e-d1bc-579065d69f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 3 famous physicists and their key contributions:\n",
            "\n",
            "1.  **Isaac Newton** (1642‚Äì1727)\n",
            "    *   Developed the **Laws of Motion**, forming the basis of classical mechanics.\n",
            "    *   Formulated the **Law of Universal Gravitation**, explaining the force that governs the motion of planets and falling objects.\n",
            "    *   Co-invented **calculus** (with Gottfried Leibniz), a fundamental tool in mathematics and science.\n",
            "    *   Pioneering work in **optics**, including the nature of light and color.\n",
            "\n",
            "2.  **Albert Einstein** (1879‚Äì1955)\n",
            "    *   Developed the **theories of Special and General Relativity**, revolutionizing our understanding of space, time, gravity, and the universe.\n",
            "    *   Famous for the mass-energy equivalence formula, **E=mc¬≤**.\n",
            "    *   Explained the **photoelectric effect** (for which he won the Nobel Prize), a crucial step in the development of quantum mechanics.\n",
            "    *   Provided a theoretical explanation for **Brownian motion**, confirming the existence of atoms.\n",
            "\n",
            "3.  **Marie Curie** (1867‚Äì1934)\n",
            "    *   Pioneered research on **radioactivity**, a term she coined.\n",
            "    *   Discovered the elements **polonium** and **radium**.\n",
            "    *   Developed techniques for isolating radioactive isotopes.\n",
            "    *   First woman to win a Nobel Prize, and the only person to win Nobel Prizes in two different scientific fields (Physics in 1903 and Chemistry in 1911).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "prompt = \"\"\"\n",
        "Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.\n",
        "The person who lives in the red house owns a cat.\n",
        "Bob does not live in the green house.\n",
        "Carol owns a dog.\n",
        "The green house is to the left of the red house.\n",
        "Alice does not own a cat.\n",
        "Who lives in each house, and what pet do they own?\n",
        "\"\"\"\n",
        "\n",
        "thoughts = \"\"\n",
        "answer = \"\"\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "      thinking_config=types.ThinkingConfig(\n",
        "        include_thoughts=True\n",
        "      )\n",
        "    )\n",
        "):\n",
        "  for part in chunk.candidates[0].content.parts:\n",
        "    if not part.text:\n",
        "      continue\n",
        "    elif part.thought:\n",
        "      if not thoughts:\n",
        "        print(\"Thoughts summary:\")\n",
        "      print(part.text)\n",
        "      thoughts += part.text\n",
        "    else:\n",
        "      if not answer:\n",
        "        print(\"Answer:\")\n",
        "      print(part.text)\n",
        "      answer += part.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IggsZOS3OWik",
        "outputId": "d491ae0f-8147-465a-b97f-9ad6dc12201f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thoughts summary:\n",
            "**Dissecting the Puzzle**\n",
            "\n",
            "I've started by deconstructing the request. My initial focus is on the core goal: matching each person to their house color and pet. I'm identifying the key entities‚ÄîAlice, Bob, Carol, red, green, blue, dog, cat, and bird. Now I am ready to begin interpreting the clues.\n",
            "\n",
            "\n",
            "\n",
            "**Organizing the Variables**\n",
            "\n",
            "I'm now implementing the table setup as the core framework. The goal is to visually represent the interconnected entities. The initial structure is in place, and I am populating the table with the information. I am also planning on updating the spatial relationship with a corresponding visual representation.\n",
            "\n",
            "\n",
            "\n",
            "**Interpreting Connections**\n",
            "\n",
            "My thinking has progressed to actively linking clues. Specifically, I'm integrating clues 3 and 5. I realize Carol owning a dog now directly contrasts with the red house/cat relationship established in clue 3, allowing me to conclude Carol doesn't live in the red house. I will now integrate this new knowledge into my framework.\n",
            "\n",
            "\n",
            "\n",
            "**Expanding Deductions**\n",
            "\n",
            "I've integrated clue 7, noting that Alice doesn't own a cat. This, coupled with the Red House/Cat link, means Alice doesn't live in the red house. I've updated my table, and now I'm starting to reason about who lives in the red house, and who owns a cat. I'm focusing on Carol and Bob now.\n",
            "\n",
            "\n",
            "\n",
            "**Analyzing Spatial Constraints**\n",
            "\n",
            "I'm now focusing on the green and blue houses and their relative positioning, as stated in clue 6. The green house being to the left of the red house gives us spatial information. Since the red house is Bob's, the green house can't be on the right. This restricts the possibilities for Alice and Carol.\n",
            "\n",
            "\n",
            "\n",
            "**Revising the Spatial Model**\n",
            "\n",
            "I'm revisiting the spatial constraints. Given that the green house is to the left of the red house, and Bob is in the red, the green house is before the red. I'll test each person as a possibility, checking the house order and the clues. This focused analysis will reveal who occupies which house and helps determine the remaining pet.\n",
            "\n",
            "\n",
            "\n",
            "**Confirming Deductions**\n",
            "\n",
            "I've just concluded that Bob lives in the red house and owns the cat. Leveraging the \"green to the left of red\" clue now, and recognizing Bob's placement, I deduced that Carol lives in the green house and Alice resides in the blue house. This, in turn, allowed me to deduce that Alice owns the bird, thus completing the puzzle!\n",
            "\n",
            "\n",
            "\n",
            "**Completing the Deductions**\n",
            "\n",
            "I've determined Bob lives in the red house and owns the cat. The spatial clue (green to the left of red) now helps. Since Bob's in red, Carol must be in green, and Alice in blue. Thus, Alice owns the bird and the puzzle is solved.\n",
            "\n",
            "\n",
            "\n",
            "**Developing the Solution**\n",
            "\n",
            "The spatial aspect is my current focus. I'm leveraging the \"green to the left of red\" clue in conjunction with Bob's placement in the red house. My next step will be determining where Carol and Alice reside. I'm focusing on Alice's pet and Carol's pet now.\n",
            "\n",
            "\n",
            "\n",
            "**Solving the Final Piece**\n",
            "\n",
            "I'm revisiting the green/blue house assignments. Since Bob is in the red house (owning the cat), and the green house is left of red, that means Carol must be in the green house and Alice must be in the blue house. Thus, Alice owns the bird and the puzzle is solved!\n",
            "\n",
            "\n",
            "\n",
            "**Finalizing the Resolution**\n",
            "\n",
            "I'm revisiting the green/blue house assignments. With Bob in red, the \"green to the left of red\" clue makes Carol's green house placement certain, with Alice in the blue. The only remaining task is to determine Alice's pet. Since Alice can't own a cat, the puzzle is solvable.\n",
            "\n",
            "\n",
            "\n",
            "**Refining Deductions**\n",
            "\n",
            "I've determined Bob lives in the red house and owns the cat. Leveraging the spatial \"green to the left of red\" clue, and knowing Bob's placement, Carol must be in the green house and Alice in the blue house. This implies Alice owns the bird and the puzzle is solvable.\n",
            "\n",
            "\n",
            "\n",
            "**Confirming the Deductions**\n",
            "\n",
            "I've determined Bob lives in the red house and owns the cat. Leveraging the spatial clue (\"green to the left of red\"), now that I know Bob's placement, I deduce Carol lives in the green house and Alice resides in the blue house. This, in turn, allows me to conclude that Alice owns the bird, thus completing the puzzle!\n",
            "\n",
            "\n",
            "\n",
            "**Solving the Final Step**\n",
            "\n",
            "I'm focusing on the spatial relationships and the remaining pets. Since Bob lives in the red house and owns the cat, and the green house is to the left, Carol resides in the green house and Alice must be in the blue house. This determines Alice's pet; it can't be a cat.\n",
            "\n",
            "\n",
            "\n",
            "**Solving the Final Step**\n",
            "\n",
            "I'm making progress in unraveling the puzzle. I've pinpointed Bob in the red house with the cat. Leveraging the \"green to the left of red\" spatial clue, Carol must occupy the green house, and Alice the blue house. This placement helps confirm Alice owns the bird!\n",
            "\n",
            "\n",
            "\n",
            "**Confirming the Logic**\n",
            "\n",
            "I'm solidifying my approach with the logic puzzle. Bob in the red house with a cat is my solid foundation, derived from the established clues. Now, I'll be working to determine Carol's green house or Alice's blue house assignment, and Alice's pet choice, to wrap it up.\n",
            "\n",
            "\n",
            "\n",
            "**Analyzing the Spatial Clues**\n",
            "\n",
            "I've integrated the \"green to the left of red\" clue. Given Bob in the red house, the green must be to his left. That places Carol in green. The only possible result can be the puzzle's solution.\n",
            "\n",
            "\n",
            "\n",
            "**Analyzing the Final Configuration**\n",
            "\n",
            "I'm revisiting the green/blue house assignments. Since Bob is in the red house, the spatial constraint demands Carol reside in the green house and Alice occupies the blue. With this, the puzzle's solved.\n",
            "\n",
            "\n",
            "\n",
            "**Refining the Logic**\n",
            "\n",
            "After thorough review, I've concluded Bob must reside in the red house, and thus owns the cat. Leveraging the spatial clue about the green house, I deduce Carol lives there, and by elimination, Alice resides in the blue house. This placement helps confirm Alice owns the bird and solves the logic puzzle.\n",
            "\n",
            "\n",
            "**Confirming the Deductions**\n",
            "\n",
            "I'm now certain Bob lives in the red house, and he owns the cat. Based on the spatial relationship, green is to the left of red, confirming Carol resides in the green house and Alice is in the blue house. This setup allows me to conclude Alice owns the bird and solve the puzzle.\n",
            "\n",
            "\n",
            "\n",
            "**Finalizing Deductions**\n",
            "\n",
            "I'm now certain Bob lives in the red house, and he owns the cat. Leveraging the spatial \"green to the left of red\" clue, and Bob's location, I've deduced Carol's in the green house and Alice's in the blue. With this, Alice must own the bird and the puzzle is solved!\n",
            "\n",
            "\n",
            "\n",
            "**Analyzing the Solution**\n",
            "\n",
            "The spatial aspect is my focus. Given Bob's red house placement, I'm working through possibilities. I'm focusing on Alice's pet choice and Carol's pet status. My recent efforts have revolved around confirming the solution with the constraints provided.\n",
            "\n",
            "\n",
            "\n",
            "**Reflecting on the Process**\n",
            "\n",
            "I've been meticulously analyzing the clues, starting with the spatial aspect. I'm now certain Bob lives in the red house, owning the cat. Carol must reside in the green house, given its placement relative to the red. Alice, therefore, occupies the blue house. Completing my deductions has been satisfying!\n",
            "\n",
            "\n",
            "\n",
            "**Reflecting and Finalizing**\n",
            "\n",
            "I've integrated all the clues, ensuring that Bob is in the red house with the cat. Leveraging the green-left-of-red spatial cue, I know Carol's green house placement. Therefore, Alice must be in the blue house, and her pet is the bird! Now, the puzzle is solved and I'm happy with my conclusion.\n",
            "\n",
            "\n",
            "\n",
            "**Finalizing the Resolution**\n",
            "\n",
            "I'm now revisiting the green/blue house assignments. With Bob in red, the \"green to the left of red\" clue makes Carol's green house placement certain, with Alice in the blue. The only remaining task is to determine Alice's pet. Since Alice can't own a cat, the puzzle is solved!\n",
            "\n",
            "\n",
            "\n",
            "**Unveiling the Key**\n",
            "\n",
            "I've integrated the spatial arrangement of the houses: green to the left of red (occupied by Bob). Carol must live in the green house, and Alice in the blue house. Thus, the puzzle is solved.\n",
            "\n",
            "\n",
            "\n",
            "**Analyzing the Puzzle**\n",
            "\n",
            "I'm now certain about Bob's place, in the red house with the cat. Leveraging the spatial clue (green to the left of red), I'm reasoning out Carol and Alice's placements. The path to the solution is nearing completion.\n",
            "\n",
            "\n",
            "\n",
            "**Unraveling the Solution**\n",
            "\n",
            "The pieces are now aligning. I've deduced Bob is in the red house, with the cat, a solid starting point. The spatial clue dictates Green to the left, thus Carol's location is clear. From this, Alice's blue house placement is evident, which solves the puzzle.\n",
            "\n",
            "\n",
            "\n",
            "**Analyzing the Final Configuration**\n",
            "\n",
            "I'm zeroing in on Bob's position in the red house, and his ownership of the cat. The spatial \"green to the left\" clue now defines Carol's green house placement, and Alice's blue house. The puzzle is resolved!\n",
            "\n",
            "\n",
            "\n",
            "**Targeting a Resolution**\n",
            "\n",
            "I'm focusing now on the spatial relationships and using this new foundation to solve the ambiguity. Bob is in the red house with the cat. Carol must be in the green, since it's left of the red. And Alice is in the blue. Now, I will solve the final pet assignment to lock in the answer.\n",
            "\n",
            "\n",
            "\n",
            "**Reflecting and Finalizing**\n",
            "\n",
            "I'm now certain Bob lives in the red house, and he owns the cat. Based on the spatial relationship, green is to the left of red, confirming Carol resides in the green house and Alice is in the blue house. This setup allows me to conclude Alice owns the bird and solve the puzzle.\n",
            "\n",
            "\n",
            "\n",
            "**Analyzing the Final Configuration**\n",
            "\n",
            "I'm zeroing in on Bob's place, the red house, and cat ownership. With the spatial clue, Carol is in green, leaving Alice in blue. Thus, Alice has the bird, and the puzzle is solved.\n",
            "\n",
            "\n",
            "\n",
            "**Analyzing Current Configuration**\n",
            "\n",
            "I'm now certain about Bob's place, in the red house with the cat. Leveraging the spatial clue (green to the left of red), I'm reasoning out Carol and Alice's placements. The path to the solution is nearing completion.\n",
            "\n",
            "\n",
            "\n",
            "**Finalizing the Resolution**\n",
            "\n",
            "I'm solidifying my approach and have deduced Bob's placement in the red house, with cat ownership. The spatial clue, \"green to the left of red,\" directs me towards placing Carol in green, Alice in blue, and solving the puzzle.\n",
            "\n",
            "\n",
            "\n",
            "**Reaching a Conclusion**\n",
            "\n",
            "I'm solidifying my approach and have deduced Bob's placement in the red house, with cat ownership. The spatial clue, \"green to the left of red,\" now steers me towards Carol in the green house and Alice in the blue. Thus, Alice owns the bird and the puzzle's solved!\n",
            "\n",
            "\n",
            "\n",
            "**Building the Framework**\n",
            "\n",
            "I've progressed towards setting up my solution framework. After mapping the relationships between houses, pets, and individuals, I'm integrating that information into the table. My next task involves cross-referencing these connections to narrow down the possibilities further.\n",
            "\n",
            "\n",
            "**Unraveling the Solution**\n",
            "\n",
            "I've integrated the \"green to the left of red\" clue. Given Bob in the red house, the green must be to his left. That places Carol in green. The only possible result can be the puzzle's solution.\n",
            "\n",
            "\n",
            "\n",
            "**Analyzing the Puzzle**\n",
            "\n",
            "I've integrated all the clues, ensuring that Bob is in the red house with the cat. Leveraging the green-left-of-red spatial cue, I know Carol's green house placement. Therefore, Alice must be in the blue house, and her pet is the bird! Now, the puzzle is solved and I'm happy with my conclusion.\n",
            "\n",
            "\n",
            "\n",
            "**Analyzing a Logic Puzzle**\n",
            "\n",
            "I'm now integrating the clues, especially the spatial one about the green and red houses. My current reasoning suggests Bob lives in the red house, leaving Alice and Carol in green and blue. I'll continue to analyze those options to complete the puzzle.\n",
            "\n",
            "\n",
            "\n",
            "**Simplifying Deductions**\n",
            "\n",
            "I've streamlined the logic to identify Bob in the red house, owning the cat. Leveraging the spatial clue (green to the left of red), I conclude that Carol is in the green house and Alice must be in the blue house. This placement helps confirm Alice owns the bird and solves the logic puzzle.\n",
            "\n",
            "\n",
            "\n",
            "**Unraveling the Final Steps**\n",
            "\n",
            "Now I'm focusing on the spatial relationships and using this new foundation to solve the ambiguity. Bob is in the red house with the cat. Carol must be in the green, since it's left of the red. And Alice is in the blue. Now, I will solve the final pet assignment to lock in the answer.\n",
            "\n",
            "\n",
            "\n",
            "**Refining the Solution**\n",
            "\n",
            "I'm solidifying my approach and have deduced Bob's placement in the red house, with cat ownership. The spatial clue, \"green to the left of red,\" directs me towards placing Carol in the green house and Alice in the blue. With that, I conclude the puzzle is solved.\n",
            "\n",
            "\n",
            "\n",
            "**Unraveling the Solution**\n",
            "\n",
            "The puzzle's central idea is to match people, houses, and pets. Bob lives in the red house, and owns a cat, which are established facts. My focus is now the placement of Alice, Carol, and the relative positions of green and blue houses to solve the puzzle.\n",
            "\n",
            "\n",
            "\n",
            "**Unraveling the Conundrum**\n",
            "\n",
            "My thinking has moved toward a definitive path. After establishing Bob in the red house with the cat, the \"Green to the Left of Red\" clue, points Carol toward the green house and Alice to the blue. This approach is my current, and hopefully final, step to the solution.\n",
            "\n",
            "\n",
            "\n",
            "**Reaching a Solution**\n",
            "\n",
            "I'm solidifying my approach and have deduced Bob's placement in the red house, with cat ownership. Leveraging the spatial clue, now that Bob's placed, I'm confident Carol resides in the green house and Alice must be in the blue house. Therefore, the puzzle is solvable.\n",
            "\n",
            "\n",
            "\n",
            "**Reflecting and Summarizing**\n",
            "\n",
            "I've determined Bob's place in the red house with the cat, a solid start. The spatial clue solidifies Carol's green house placement, leaving Alice in blue. However, I have not logically proven the remaining assignments.\n",
            "\n",
            "\n",
            "**Reflecting and Summarizing**\n",
            "\n",
            "I've integrated all the clues, ensuring that Bob is in the red house with the cat. Leveraging the green-left-of-red spatial cue, I know Carol's green house placement. Therefore, Alice must be in the blue house, and her pet is the bird! Now, the puzzle is solved and I'm happy with my conclusion.\n",
            "\n",
            "\n",
            "\n",
            "**Concluding the Deduction**\n",
            "\n",
            "I've finally found the solution! Bob is in the red house with the cat, a solid foundation. Given the spatial clue, Carol is in the green house (with her dog), and Alice must be in the blue, completing the puzzle!\n",
            "\n",
            "\n",
            "\n",
            "**Unraveling the Conundrum**\n",
            "\n",
            "I'm now solidifying my analysis, deducing that Bob resides in the red house with the cat. Leveraging the green-to-the-left-of-red spatial clue, I conclude that Carol must live in the green house and Alice in the blue house, effectively solving the puzzle.\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "Of course! Let's solve this step by step.\n",
            "\n",
            "Here's the breakdown of the solution:\n",
            "\n",
            "1.\n",
            "  **Find the Cat Owner:** We know the person in the red house owns a cat. The clues state that Alice does\n",
            " not own a cat, and Carol owns a dog. By elimination, **Bob must be the one who owns a cat.**\n",
            "\n",
            "\n",
            "2.  **Find Bob's House:** Since the person in the red house owns a cat, and we'\n",
            "ve determined that Bob is the cat owner, **Bob must live in the red house.**\n",
            "\n",
            "3.  **Place\n",
            " the Houses:** The puzzle states that the green house is to the left of the red house. Now that we know Bob is\n",
            " in the red house, we have this arrangement:\n",
            "    [Green House] ‚Äî [Red House (Bob)]\n",
            "\n",
            "4\n",
            ".  **Place Carol and Alice:** This leaves Alice and Carol, and the green and blue houses.\n",
            "    *   \n",
            "We know Bob does not live in the green house (which is correct, he's in the red one).\n",
            "    *\n",
            "   Since Carol is the only other person with a known attribute (owning a dog), and she can't be in\n",
            " the red house, she must be in one of the remaining two.\n",
            "    *   This leaves **Carol in the green\n",
            " house** and **Alice in the blue house**.\n",
            "\n",
            "The final arrangement is:\n",
            "\n",
            "*   **Alice** lives in\n",
            " the **blue house**. The puzzle doesn't specify her pet, only that it is not a cat.\n",
            "*   **\n",
            "Bob** lives in the **red house** and owns a **cat**.\n",
            "*   **Carol** lives in the\n",
            " **green house** and owns a **dog**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QWEN 3"
      ],
      "metadata": {
        "id": "_fCIm2Pmhncm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen3-30B-A3B\"\n",
        "\n",
        "# load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# prepare the model input\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=32768\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content)\n",
        "print(\"content:\", content)\n",
        "To disable thinking, you just need to make changes to the argument enable_thinking like the following:\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=False  # True is the default value for enable_thinking.\n",
        ")"
      ],
      "metadata": {
        "id": "i47mrTenhrWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cerebras"
      ],
      "metadata": {
        "id": "WssEDcGS5PEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade cerebras_cloud_sdk\n",
        "import os\n",
        "from cerebras.cloud.sdk import Cerebras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EUE3Z285Wva",
        "outputId": "753a22ae-d3ab-466a-f727-aa5d70ab9bfe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cerebras_cloud_sdk\n",
            "  Downloading cerebras_cloud_sdk-1.49.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from cerebras_cloud_sdk) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from cerebras_cloud_sdk) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from cerebras_cloud_sdk) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from cerebras_cloud_sdk) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from cerebras_cloud_sdk) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from cerebras_cloud_sdk) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->cerebras_cloud_sdk) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->cerebras_cloud_sdk) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->cerebras_cloud_sdk) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->cerebras_cloud_sdk) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->cerebras_cloud_sdk) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->cerebras_cloud_sdk) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->cerebras_cloud_sdk) (0.4.1)\n",
            "Downloading cerebras_cloud_sdk-1.49.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.4/91.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cerebras_cloud_sdk\n",
            "Successfully installed cerebras_cloud_sdk-1.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CEREBRAS_API_KEY'] = userdata.get('CEREBRAS_API_KEY')\n",
        "\n",
        "client = Cerebras(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"CEREBRAS_API_KEY\")\n",
        ")\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Hello there, please tell me 3 dad jokes\"\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-oss-120b\",\n",
        "    stream=True,\n",
        "    max_completion_tokens=65536,\n",
        "    temperature=1,\n",
        "    top_p=1,\n",
        "    reasoning_effort=\"medium\"\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "  print(chunk.choices[0].delta.content or \"\", end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96a4QJBr6K7-",
        "outputId": "e09a6593-484c-4c8a-f8ed-1562a3a10cb7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure thing! Here are three classic dad jokes for you:\n",
            "\n",
            "1. **Why did the scarecrow win an award?**  \n",
            "   *Because he was outstanding in his field!*\n",
            "\n",
            "2. **I‚Äôm reading a book about anti‚Äëgravity.**  \n",
            "   *It‚Äôs impossible to put down!*\n",
            "\n",
            "3. **Why do you never see elephants hiding in trees?**  \n",
            "   *Because they‚Äôre really good at it.* üòÑ"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fireworks AI"
      ],
      "metadata": {
        "id": "Spdv_GpNEVGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade fireworks-ai openai\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti7EVGdFEYq8",
        "outputId": "48d92117-3151-4cc2-9266-69ff873a7629"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fireworks-ai in /usr/local/lib/python3.12/dist-packages (0.19.18)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.78.1)\n",
            "Collecting openai\n",
            "  Using cached openai-1.102.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (0.28.1)\n",
            "Requirement already satisfied: httpx-ws in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (0.7.2)\n",
            "Requirement already satisfied: httpx-sse in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (0.4.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (2.11.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (4.15.0)\n",
            "Requirement already satisfied: mmh3>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (5.2.0)\n",
            "Requirement already satisfied: betterproto-fw>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from betterproto-fw[compiler]>=2.0.3->fireworks-ai) (2.0.3)\n",
            "Requirement already satisfied: asyncstdlib-fw>=3.13.2 in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (3.13.2)\n",
            "Requirement already satisfied: grpcio>=1.71.0 in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (1.74.0)\n",
            "Requirement already satisfied: protobuf==5.29.3 in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (5.29.3)\n",
            "Requirement already satisfied: rich>=14.0.0 in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (14.1.0)\n",
            "Requirement already satisfied: reward-kit>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (0.4.1)\n",
            "Requirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (0.10.2)\n",
            "Requirement already satisfied: aiohttp>=3.12.11 in /usr/local/lib/python3.12/dist-packages (from aiohttp[speedups]>=3.12.11->fireworks-ai) (3.12.15)\n",
            "Requirement already satisfied: attrs==23.2.0 in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (23.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.60.0 in /usr/local/lib/python3.12/dist-packages (from fireworks-ai) (1.70.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (1.20.1)\n",
            "Requirement already satisfied: aiodns>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp[speedups]>=3.12.11->fireworks-ai) (3.5.0)\n",
            "Requirement already satisfied: Brotli in /usr/local/lib/python3.12/dist-packages (from aiohttp[speedups]>=3.12.11->fireworks-ai) (1.1.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: grpclib<0.5.0,>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (0.4.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (2.9.0.post0)\n",
            "Requirement already satisfied: ruff~=0.9.1 in /usr/local/lib/python3.12/dist-packages (from betterproto-fw[compiler]>=2.0.3->fireworks-ai) (0.9.10)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.12/dist-packages (from betterproto-fw[compiler]>=2.0.3->fireworks-ai) (3.1.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->fireworks-ai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->fireworks-ai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->fireworks-ai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->fireworks-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->fireworks-ai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->fireworks-ai) (0.4.1)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (2.32.4)\n",
            "Requirement already satisfied: dataclasses-json>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.6.7)\n",
            "Requirement already satisfied: fastapi>=0.68.0 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.116.1)\n",
            "Requirement already satisfied: uvicorn>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.35.0)\n",
            "Requirement already satisfied: python-dotenv>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (1.1.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.21.0)\n",
            "Requirement already satisfied: mcp>=1.9.2 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (1.13.1)\n",
            "Requirement already satisfied: PyYAML>=5.0 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (6.0.2)\n",
            "Requirement already satisfied: datasets==3.6.0 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (3.6.0)\n",
            "Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (2025.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (1.3.2)\n",
            "Requirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (2.3.0)\n",
            "Requirement already satisfied: gymnasium>=0.29.0 in /usr/local/lib/python3.12/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (25.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=14.0.0->fireworks-ai) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=14.0.0->fireworks-ai) (2.19.2)\n",
            "Requirement already satisfied: wsproto in /usr/local/lib/python3.12/dist-packages (from httpx-ws->fireworks-ai) (1.2.0)\n",
            "Requirement already satisfied: pycares>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from aiodns>=3.3.0->aiohttp[speedups]>=3.12.11->fireworks-ai) (4.10.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json>=0.5.7->reward-kit>=0.3.1->fireworks-ai) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json>=0.5.7->reward-kit>=0.3.1->fireworks-ai) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.68.0->reward-kit>=0.3.1->fireworks-ai) (0.47.3)\n",
            "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from grpclib<0.5.0,>=0.4.1->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (4.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.0->reward-kit>=0.3.1->fireworks-ai) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.0->reward-kit>=0.3.1->fireworks-ai) (0.0.4)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->reward-kit>=0.3.1->fireworks-ai) (4.9.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=14.0.0->fireworks-ai) (0.1.2)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.0->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->reward-kit>=0.3.1->fireworks-ai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->reward-kit>=0.3.1->fireworks-ai) (2.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.15.0->reward-kit>=0.3.1->fireworks-ai) (8.2.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (4.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (1.1.8)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (0.27.0)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from pycares>=4.9.0->aiodns>=3.3.0->aiohttp[speedups]>=3.12.11->fireworks-ai) (1.17.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.5.7->reward-kit>=0.3.1->fireworks-ai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2025.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.5.0->pycares>=4.9.0->aiodns>=3.3.0->aiohttp[speedups]>=3.12.11->fireworks-ai) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fireworks import LLM\n",
        "\n",
        "os.environ['FIREWORKS_API_KEY'] = userdata.get('FIREWORKS_API_KEY')\n",
        "\n",
        "# Basic usage - SDK automatically selects optimal deployment type\n",
        "llm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"auto\", api_key=os.environ.get(\"FIREWORKS_API_KEY\"))\n",
        "\n",
        "response = llm.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello there, please tell me 3 dad jokes\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQlAZpFXEenV",
        "outputId": "b092f191-df7b-4884-f2d3-a2b7786a8705"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You want to groan and roll your eyes, huh? Here are three dad jokes for you:\n",
            "\n",
            "1. Why did the scarecrow win an award? Because he was outstanding in his field! (get it?)\n",
            "2. I told my wife she was drawing her eyebrows too high. She looked surprised.\n",
            "3. Why did the mushroom go to the party? Because he was a fun-gi!\n",
            "\n",
            "Hope these corny jokes made you chuckle (or at least roll your eyes in amusement)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FvPAprzgEexe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}