{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 0 — Environment Setup"
      ],
      "metadata": {
        "id": "VKsiBpzxS3GG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 — Install Required Libraries\n",
        "\n",
        "Description:\n",
        "We’ll install the core dependencies for LangChain, OpenAI, Anthropic, and vector DBs (Chroma)."
      ],
      "metadata": {
        "id": "r1zB2BTtS4vD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jiPuxSgoSUe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7909fed1-25c5-410b-b75f-377538bc64c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-anthropic\n",
            "  Downloading langchain_anthropic-0.3.19-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.23)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/langsmith/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting langsmith\n",
            "  Downloading langsmith-0.4.25-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.104.2)\n",
            "Collecting anthropic<1,>=0.64.0 (from langchain-anthropic)\n",
            "  Downloading anthropic-0.66.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.24.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.64.0->langchain-anthropic) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.64.0->langchain-anthropic) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.64.0->langchain-anthropic) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.64.0->langchain-anthropic) (1.3.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_anthropic-0.3.19-py3-none-any.whl (31 kB)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.4.25-py3-none-any.whl (379 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.4/379.4 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anthropic-0.66.0-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.0/308.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=0e171a710ab8f8d28699492c2da16d69e982ee08c17b3e60136ccff3dff5200b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, requests, pypdf, pybase64, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httptools, bcrypt, backoff, watchfiles, typing-inspect, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, onnxruntime, langsmith, kubernetes, dataclasses-json, anthropic, opentelemetry-exporter-otlp-proto-grpc, langchain-openai, langchain-anthropic, chromadb, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.4.23\n",
            "    Uninstalling langsmith-0.4.23:\n",
            "      Successfully uninstalled langsmith-0.4.23\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anthropic-0.66.0 backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.20 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 langchain-anthropic-0.3.19 langchain-community-0.3.29 langchain-openai-0.3.32 langsmith-0.4.25 marshmallow-3.26.1 mmh3-5.2.0 mypy-extensions-1.1.0 onnxruntime-1.22.1 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 posthog-5.4.0 pybase64-1.4.2 pypdf-6.0.0 pypika-0.48.9 requests-2.32.5 typing-inspect-0.9.0 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# Section 0.1 — Install Required Libraries\n",
        "# ======================================\n",
        "\n",
        "!pip install -U langchain langchain-openai langchain-anthropic langchain-community \\\n",
        "              chromadb pypdf tiktoken langsmith python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RecxGHNxu6Wm",
        "outputId": "15a6634d-8177-4f51-b942-ed66e68cca04"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (2.1.10)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.3.75)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (0.4.25)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.5)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"langchain[fireworks]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gsuB1Pzw4Z9M",
        "outputId": "3946a3c0-e19d-4555-e8ea-05ccd4904e34"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/570.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m419.8/570.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.3/570.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/319.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.6/370.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.9/680.9 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m641.0/641.0 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-openai 0.3.32 requires openai<2.0.0,>=1.99.9, but you have openai 1.78.1 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\n",
            "bigframes 2.17.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-fireworks"
      ],
      "metadata": {
        "id": "lUUR4l-983pe"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 — Import Dependencies\n",
        "\n",
        "Description:\n",
        "We import the necessary libraries for this lecture, including LangChain, OpenAI, and Anthropic integrations."
      ],
      "metadata": {
        "id": "lHyJgWyiTBSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 0.2 — Import Dependencies\n",
        "# ======================================\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "# LangChain Core\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_fireworks import ChatFireworks"
      ],
      "metadata": {
        "id": "NH3fibE5S7qC"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 — Configure API Keys\n",
        "\n",
        "Description:\n",
        "We’ll load API keys stored in Google Colab environment variables.\n",
        "Before running, set your keys in:\n",
        "Colab → Settings → Variables → Add OPENAI_API_KEY and ANTHROPIC_API_KEY."
      ],
      "metadata": {
        "id": "r3NHEUbYTI2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 0.3 — Configure API Keys\n",
        "# ======================================\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"FIREWORKS_API_KEY\"] = userdata.get(\"FIREWORKS_API_KEY\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get(\"LANGSMITH_API_KEY\")\n",
        "\n",
        "# Optional: LangSmith tracing (debugging & observability)\n",
        "os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://eu.api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-lecture\"\n",
        "\n",
        "print(\"✅ API keys configured successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQLfSZq9TKz_",
        "outputId": "261a3ac4-133b-4e8e-c7fb-0f2262a49a04"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API keys configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.4 — Verify Installation & LLM Connectivity\n",
        "\n",
        "Description:\n",
        "We’ll run a quick test to ensure both OpenAI and Anthropic models are working."
      ],
      "metadata": {
        "id": "-wRk63ZvTh0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 0.4 — Verify Installation & LLM Connectivity\n",
        "# ======================================\n",
        "\n",
        "# Test OpenAI connection gpt-5-nano, gpt-4o-mini\n",
        "openai_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "response_openai = openai_llm.invoke(\"Say 'ready' if you can hear me.\")\n",
        "print(\"🔹 OpenAI LLM:\", response_openai.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBIZ0Ij8Tll2",
        "outputId": "ab3f538b-2bc1-4599-8316-197327758894"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 OpenAI LLM: Ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Anthropic connection (optional)\n",
        "try:\n",
        "    anthropic_llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
        "    response_anthropic = anthropic_llm.invoke(\"Say 'ready' if you can hear me.\")\n",
        "    print(\"🔹 Anthropic LLM:\", response_anthropic.content)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Anthropic test skipped (no API key or model unavailable).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yiziSpq7GCW",
        "outputId": "bc246f90-702b-4ba1-dc76-91a71e6c5a1a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Anthropic LLM: Ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite\n",
        "    gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
        "    response_gemini = gemini_llm.invoke(\"Say 'ready' if this request reached you.\")\n",
        "    print(\"🔹 Gemini LLM:\", response_gemini.content)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Gemini test skipped (no API key or model unavailable).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HokuCpLr7GQ1",
        "outputId": "83f1459d-7d3c-41aa-ce54-75aad401b43b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Gemini LLM: Ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # accounts/fireworks/models/llama4-scout-instruct-basic\n",
        "    fireworks_general_llm = init_chat_model(\"accounts/fireworks/models/llama4-scout-instruct-basic\", temperature=0)\n",
        "    response_fireworks = fireworks_general_llm.invoke(\"Say 'ready' if this request reached you.\")\n",
        "    print(\"🔹 Fireworks LLM:\", response_fireworks.content)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Fireworks test skipped (no API key or model unavailable).\", \"Exception:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXTQYAtx7Ga9",
        "outputId": "2c3eca75-a1f1-4b9a-d273-04e257fcfc5e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a4337badf70>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a4337baf6b0>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a434d192db0>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a434d125e50>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Fireworks LLM: Ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # accounts/fireworks/models/llama4-scout-instruct-basic\n",
        "    # accounts/fireworks/models/qwen3-coder-30b-a3b-instruct\n",
        "    # accounts/fireworks/models/gpt-oss-20b\n",
        "    # accounts/fireworks/models/gpt-oss-120b\n",
        "    fireworks_llm = ChatFireworks(model=\"accounts/fireworks/models/qwen3-coder-30b-a3b-instruct\", temperature=0)\n",
        "    response_fireworks = fireworks_llm.invoke(\"Say 'ready' if this request reached you.\")\n",
        "    print(\"🔹 Fireworks LLM:\", response_fireworks.content)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Fireworks test skipped (no API key or model unavailable).\", \"Exception:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4kzWY3i8Zsg",
        "outputId": "ea7846f3-ef3b-415c-f3a6-2f71993efed2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a434d151e80>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a434d1519d0>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a434d126840>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a4337b0fd10>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Fireworks LLM: Ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1 — Understanding LLMs (Large Language Models)\n",
        "Goal: Understand what LLMs are, how LangChain integrates them, and how to initialize and test models from OpenAI and Anthropic."
      ],
      "metadata": {
        "id": "zUPoz-CoUAn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 — Initialize & Test OpenAI LLM\n",
        "\n",
        "Description:\n",
        "In this cell, we’ll set up and test an OpenAI chat model using LangChain’s ChatOpenAI wrapper.\n",
        "We’ll explain key parameters, even if we don’t modify them yet."
      ],
      "metadata": {
        "id": "LpuRekQXULD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 1.1 — Initialize & Test OpenAI LLM\n",
        "# ======================================\n",
        "\n",
        "# Import the OpenAI chat model integration for LangChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Create an OpenAI LLM instance\n",
        "# --------------------------------------\n",
        "# Parameters:\n",
        "# - model: The specific OpenAI model to use.\n",
        "#          Here we use \"gpt-4o-mini\" (fast, cost-effective, supports RAG).\n",
        "# - temperature: Controls randomness of output.\n",
        "#       0.0 → deterministic responses\n",
        "#       1.0 → more creative and diverse responses\n",
        "# - max_tokens: Maximum number of tokens the model can return in the response.\n",
        "# - timeout: Optional timeout for API call (not used here).\n",
        "# - api_key: Uses the API key we configured earlier via environment variables.\n",
        "openai_llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",   # Recommended for fast, cost-effective RAG tasks\n",
        "    temperature=0.2,       # Low randomness for more precise answers\n",
        "    max_tokens=500         # Reasonable cap for lecture demos\n",
        ")\n",
        "\n",
        "# Quick test to verify OpenAI is working\n",
        "response = openai_llm.invoke(\"Explain in one sentence what LangChain is.\")\n",
        "print(\"🔹 OpenAI LLM Response:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-00BcQYSUKLd",
        "outputId": "8eb58720-bf95-4037-ea26-cf2466d84fc3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 OpenAI LLM Response:\n",
            " LangChain is a framework designed to facilitate the development of applications that utilize language models by providing tools for chaining together various components like prompts, memory, and APIs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1.2 — Initialize & Test Anthropic LLM\n",
        "Description:\n",
        "In this cell, we’ll set up and test an Anthropic chat model using LangChain’s ChatAnthropic wrapper.\n",
        "Anthropic’s models (e.g., Claude 3) are great for reasoning, summarization, and multi-step problem solving."
      ],
      "metadata": {
        "id": "jVEXKD6RUjGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 1.2 — Initialize & Test Anthropic LLM\n",
        "# ======================================\n",
        "\n",
        "# Import the Anthropic chat model integration for LangChain\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "# Create an Anthropic LLM instance\n",
        "# --------------------------------------\n",
        "# Parameters:\n",
        "# - model: The Anthropic Claude model to use.\n",
        "#          Here we use \"claude-3-haiku-20240307\" (fast, cost-effective).\n",
        "#          Alternative: \"claude-3-sonnet-20240229\" (higher quality, slower).\n",
        "# - temperature: Controls creativity.\n",
        "#       0.0 → deterministic responses\n",
        "#       1.0 → more creative and diverse outputs\n",
        "# - max_tokens: Maximum number of tokens in the response.\n",
        "# - api_key: Uses the API key configured earlier via environment variables.\n",
        "anthropic_llm = ChatAnthropic(\n",
        "    model=\"claude-3-haiku-20240307\",  # Optimized for speed and low latency\n",
        "    temperature=0.2,                  # Keep responses focused and consistent\n",
        "    max_tokens=500                    # Reasonable cap for demos\n",
        ")\n",
        "\n",
        "# Quick test to verify Anthropic is working\n",
        "try:\n",
        "    response = anthropic_llm.invoke(\"Explain in one sentence what LangChain is.\")\n",
        "    print(\"🔹 Anthropic LLM Response:\\n\", response.content)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Anthropic model test skipped:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDvaJ3B6UmDl",
        "outputId": "94c003c4-aab1-43f7-c4b8-73b175f9923d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Anthropic LLM Response:\n",
            " LangChain is an open-source framework that helps developers build applications with large language models (LLMs) by providing a set of abstractions and tools to simplify the process of integrating LLMs into their applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1.3 — Initialize & Test Gemini LLM"
      ],
      "metadata": {
        "id": "DhIF2bHEuS6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 1.3 — Initialize & Test Gemini LLM\n",
        "# ======================================\n",
        "\n",
        "# Import the Gemini chat model integration for LangChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Create an Gemini LLM instance\n",
        "# --------------------------------------\n",
        "# Parameters:\n",
        "# - model: The specific Gemini model to use.\n",
        "#          Here we use \"gemini-2.5-flash\" or \"gemini-2.5-flash-lite\" (fast, cost-effective, supports RAG).\n",
        "# - temperature: Controls randomness of output.\n",
        "#       0.0 → deterministic responses\n",
        "#       1.0 → more creative and diverse responses\n",
        "# - max_tokens: Maximum number of tokens the model can return in the response.\n",
        "# - timeout: Optional timeout for API call (not used here).\n",
        "# - api_key: Uses the API key we configured earlier via environment variables.\n",
        "gemini_llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash-lite\",   # Recommended for fast, cost-effective RAG tasks\n",
        "    temperature=0.2,       # Low randomness for more precise answers\n",
        "    max_tokens=500,         # 500 Reasonable cap for gemini-2.5-flash-lite; 1500 tokens for gemini-2.5-flash because it is a thinking model and needs 500+ tokens to think on the topic\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")\n",
        "\n",
        "# Quick test to verify OpenAI is working\n",
        "response = gemini_llm.invoke(\"Explain in one sentence what LangChain is.\")\n",
        "print(\"🔹 Gemini LLM Response:\\n\", response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMb8WJmauYCE",
        "outputId": "dfe9a7d6-c0c7-4edd-da36-4e4fe096bfce"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Gemini LLM Response:\n",
            " LangChain is a framework that helps developers build applications powered by large language models by providing tools and abstractions for chaining together different components.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 1.4 — Initialize & Test Fireworks LLM\n",
        "# ======================================\n",
        "\n",
        "# Import the Fireworks chat model integration for LangChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Create an Fireworks LLM instance\n",
        "# --------------------------------------\n",
        "# Parameters:\n",
        "# - model: The specific Fireworks model to use.\n",
        "    # accounts/fireworks/models/llama4-scout-instruct-basic\n",
        "    # accounts/fireworks/models/qwen3-coder-30b-a3b-instruct\n",
        "    # accounts/fireworks/models/gpt-oss-20b\n",
        "    # accounts/fireworks/models/gpt-oss-120b\n",
        "# - temperature: Controls randomness of output.\n",
        "#       0.0 → deterministic responses\n",
        "#       1.0 → more creative and diverse responses\n",
        "# - max_tokens: Maximum number of tokens the model can return in the response.\n",
        "# - timeout: Optional timeout for API call (not used here).\n",
        "# - api_key: Uses the API key we configured earlier via environment variables.\n",
        "\n",
        "fireworks_llm = ChatFireworks(\n",
        "    model=\"accounts/fireworks/models/qwen3-coder-30b-a3b-instruct\",\n",
        "    temperature=0.2,       # Low randomness for more precise answers\n",
        "    max_tokens=500,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Quick test to verify OpenAI is working\n",
        "    response_fireworks = fireworks_llm.invoke(\"Explain in one sentence what LangChain is.\")\n",
        "    print(\"🔹 Fireworks LLM Response:\\n\", response_fireworks.content)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Fireworks test skipped (no API key or model unavailable).\", \"Exception:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uqxizlQ-ceZ",
        "outputId": "85e19409-63f6-48f9-ec64-fd3b23e050fd"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a434d1264e0>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a4337b0de20>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a4337b0dc70>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7a434d1535f0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Fireworks LLM Response:\n",
            " LangChain is a framework that enables developers to build applications powered by language models by providing tools for memory management, agent creation, and seamless integration with various LLM APIs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pydantic-ai-slim[duckduckgo]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ODbVQFwVDxV5",
        "outputId": "68f9ef3f-1f7d-4427-f622-1fd22afb2e08"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic-ai-slim[duckduckgo]\n",
            "  Downloading pydantic_ai_slim-1.0.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting genai-prices>=0.0.22 (from pydantic-ai-slim[duckduckgo])\n",
            "  Downloading genai_prices-0.0.25-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting griffe>=1.3.2 (from pydantic-ai-slim[duckduckgo])\n",
            "  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[duckduckgo]) (0.28.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.28.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[duckduckgo]) (1.36.0)\n",
            "Collecting pydantic-graph==1.0.1 (from pydantic-ai-slim[duckduckgo])\n",
            "  Downloading pydantic_graph-1.0.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[duckduckgo]) (2.11.7)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[duckduckgo]) (0.4.1)\n",
            "Collecting ddgs>=9.0.0 (from pydantic-ai-slim[duckduckgo])\n",
            "  Downloading ddgs-9.5.5-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting logfire-api>=3.14.1 (from pydantic-graph==1.0.1->pydantic-ai-slim[duckduckgo])\n",
            "  Downloading logfire_api-4.4.0-py3-none-any.whl.metadata (971 bytes)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs>=9.0.0->pydantic-ai-slim[duckduckgo]) (8.2.1)\n",
            "Collecting primp>=0.15.0 (from ddgs>=9.0.0->pydantic-ai-slim[duckduckgo])\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting lxml>=6.0.0 (from ddgs>=9.0.0->pydantic-ai-slim[duckduckgo])\n",
            "  Downloading lxml-6.0.1-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting colorama>=0.4 (from griffe>=1.3.2->pydantic-ai-slim[duckduckgo])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim[duckduckgo]) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim[duckduckgo]) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim[duckduckgo]) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim[duckduckgo]) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim[duckduckgo]) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim[duckduckgo]) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim[duckduckgo]) (4.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10->pydantic-ai-slim[duckduckgo]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10->pydantic-ai-slim[duckduckgo]) (2.33.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim[duckduckgo]) (3.23.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->pydantic-ai-slim[duckduckgo]) (1.3.1)\n",
            "Downloading pydantic_graph-1.0.1-py3-none-any.whl (27 kB)\n",
            "Downloading ddgs-9.5.5-py3-none-any.whl (37 kB)\n",
            "Downloading genai_prices-0.0.25-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading griffe-1.14.0-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_ai_slim-1.0.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading logfire_api-4.4.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-6.0.1-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, lxml, logfire-api, colorama, griffe, ddgs, pydantic-graph, genai-prices, pydantic-ai-slim\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.4.0\n",
            "    Uninstalling lxml-5.4.0:\n",
            "      Successfully uninstalled lxml-5.4.0\n",
            "Successfully installed colorama-0.4.6 ddgs-9.5.5 genai-prices-0.0.25 griffe-1.14.0 logfire-api-4.4.0 lxml-6.0.1 primp-0.15.0 pydantic-ai-slim-1.0.1 pydantic-graph-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent\n",
        "from pydantic_ai.common_tools.duckduckgo import duckduckgo_search_tool\n",
        "import asyncio # Import asyncio\n",
        "\n",
        "agent = Agent(\n",
        "    'openai:o3-mini',\n",
        "    tools=[duckduckgo_search_tool()],\n",
        "    system_prompt='Search DuckDuckGo for the given query and return the results.',\n",
        ")\n",
        "\n",
        "# Use asyncio.run() or await inside an async function\n",
        "# In Colab, you can use await directly in a code cell\n",
        "agent_result = await agent.run( # Changed from run_sync to run and added await\n",
        "    'Can you list the top five highest-grossing animated films of 2025?'\n",
        ")\n",
        "print(agent_result.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5-ol434DqhE",
        "outputId": "bfbcae46-9741-4d93-d790-7c64dfaedbe0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on a sampling of recent industry reporting and box‐office analysis, the following five animated films have emerged as 2025’s top earners (keeping in mind that final numbers may shift as the year wraps up):\n",
            "\n",
            "1. Ne Zha 2\n",
            " • This Chinese blockbuster has smashed previous records to become the highest‐grossing animated film ever—crossing the US$2‐billion mark globally.\n",
            "\n",
            "2. Inside Out 2\n",
            " • Pixar’s long-awaited sequel has performed exceptionally well overseas and domestically, ranking as a close second with multi‐billion-dollar worldwide takings.\n",
            "\n",
            "3. Frozen 2\n",
            " • Continuing Disney’s strong performance in animation, Frozen 2 has remained one of the year’s major box‐office draws.\n",
            "\n",
            "4. Elio\n",
            " • Garnering significant attention in the United States, Elio is reported to be the top earner among American animated features of 2025.\n",
            "\n",
            "5. Jumbo\n",
            " • An Indonesian animated feature that has surprised many critics and audiences alike, Jumbo has delivered record numbers for its market and gained international acclaim.\n",
            "\n",
            "Note that the rankings and exact revenue figures are based on mid‑year reports from sources like Deadline, Reuters, and industry analytics sites. For the most up‑to‑date figures and more detailed breakdowns, please consult the latest box‑office reports or dedicated entertainment industry resources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU duckduckgo-search langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5Xx_KmukFCXe",
        "outputId": "004dda1e-0182-4243-a7d9-849bbe5ec9a6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen posixpath>:82: RuntimeWarning: coroutine 'AbstractAgent.run' was never awaited\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "search.invoke(\"Iron Man's first name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "MfqREN8-GVJA",
        "outputId": "0602e9ed-13af-4024-bfb3-962cc4a663aa"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Feb 12, 2025 — Full name, Anthony Edward Stark ; Alias, Iron Man ; Occupation. Superhero; Inventor; Industrialist; Benefactor and leader of the Avengers; Founder of the Maria ... Chemically, the most common oxidation states of iron are iron (II) and iron (III). Iron shares many properties of other transition metals, including the other group 8 elements, ruthenium and … Just noticed Iron Man's full “real name” is Tony Edward Stark (no spoilers) · Something I love about Iron Man (2008) is how grounded it feels compared to later ... His full name is Tony Stark . This is because he was born with a genetic mutation that made it so his chest bone has a hole in it, just like iron man. It's not ... Aug 11, 2025 — Full name. Anthony Edward Stark ; Other names. Iron Man Tony Tony Stark Invincible Iron Man Shellhead Golden Avenger Armored Avenger Superior Iron Man Tetsujin\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "\n",
        "search = DuckDuckGoSearchResults()\n",
        "\n",
        "search.invoke(\"Iron man\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "v3N4wqswIwhv",
        "outputId": "711431a7-4025-489e-9e42-ad76360d1ef8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'snippet: Iron Man is the superhero persona of Anthony Edward \" Tony \" Stark, a businessman and engineer who runs the weapons manufacturing company Stark Industries. When Stark was captured in a war zone and sustained a severe heart wound, he built his Iron Man armor and escaped his captors., title: Iron Man - Wikipedia, link: https://en.wikipedia.org/wiki/Iron_Man, snippet: In the film, following his escape from captivity by a terrorist group, world-famous industrialist and master engineer Stark builds a mechanized suit of armor and becomes the superhero Iron Man ., title: Iron Man (2008 film) - Wikipedia, link: https://en.wikipedia.org/wiki/Iron_Man_(2008_film), snippet: Iron Man \" is a song by English rock band Black Sabbath , released in 1970 from the band\\'s second studio album, Paranoid , and as a single in the US ..., title: Iron Man (song) - Wikipedia, link: https://en.wikipedia.org/wiki/Iron_Man_(song), snippet: Learn about Tony Stark, the billionaire inventor who became Iron Man , a founding member of the Avengers. Find out his history, powers, suits, allies, enemies, and appearances in movies, TV shows, comics, and more. Other articles from marvelcinematicuniverse.fandom.com 1 day ago · Iron Man \\'s Marvel Cinematic Universe journey has led to many emotional, hilarious, and long-lasting quotes from the hero, but some emerge as the most profound. Robert Downey Jr.\\'s performances as Tony Stark have led to some of Marvel\\'s best movies. The actor\\'s take on the character is pitch-perfect ... Iron Man (Tony Stark) is an American comic-book superhero who is a mainstay of Marvel Comics. Because of the character’s widespread appeal, Iron Man has appeared in multiple comics, television series, and films. Why does Iron Man have the arc reactor? The arc reactor keeps Tony Stark alive. In the different versions of Iron Man , Stark’s group has been ambushed in Vietnam, the Persian Gulf, and Afghanistan. Stark sustained serious injuries as a result. He created the personal arc reactor to generate energy for an electromagnet keeping shrapnel away from his heart. What is Iron Man ’s suit made of? In the 2008 Iron Man film, the Iron Man Mark I suit was made of reused parts from Jericho missiles. However, the majority of the more-advanced suits, starting with Mark III, were made not of iron but of a titanium or titanium-gold alloy. In the 2019 film Avengers: Endgame, the suit was constructed of vibranium and nanotech. How does Iron Man die in Endgame? See full list on britannica.com Iron Man ’s alter ego of Tony Stark—wealthy playboy inventor, owner of Stark International, and international arms manufacturer—was partly based on the wealthy inventor, business mogul, and defense contractor Howard Hughes. In Marvel’s early days, much was made of the company’s creation of “heroes with problems,” and Stark’s problem was potentially fatal: while demonstrating some new weapons in the jungles of Vietnam, he is injured by a bomb and captured by a Viet Cong warlord. With his life ebbing away, Stark is forced to work for his captors, creating new weapons, but unknown to them he secretly builds himself a high-tech suit of armour that will both keep him alive and make him a walking arsenal. Once in the gray clanking suit, Stark defeats the warlord and returns to the United States to assume the role of a superhero, but his tragedy is that he can never remove the chest plate that keeps him alive. To compound his dilemma, the armour needs constant recharging and has the unfortunate tendency to run out of power at the most inconvenient moments, usually in the middle of a pitched battle. Britannica Quiz Marvel or DC? Among those in Iron Man ’s supporting cast are Stark’s chauffeur, “Happy” Hogan; his perky secretary, Virginia (“Pepper”) Potts; and James Rhodes, a former U.S. Marine Corps pilot who would eventually don his own suit of armour as the costumed hero War Machine. Iron Man ’s major villains included Titanium Man , an armour-wearing Soviet giant (later immortalized by Paul McCartney in a song on his Venus and Mars album); rival industrialists Obadiah Stane and Justin Hammer; the Maggia crime cartel; and his archenemy, the Mandarin. The Mandarin was a sinister mastermind who rivaled Stark in scientific genius, and he wielded 10 rings of alien origin that granted him an array of powers. See full list on britannica.com The story line took a dramatic turn in the 1980s, under the writing team of David Michelinie and Bob Layton and artist John Romita, Jr. Stark International was suddenly hit by industrial espionage, and, as a despairing Stark took to drinking, Rhodes took his place in the Iron Man armour. Stark’s ongoing battle with alcoholism would become a recurring theme in subsequent years. One especially notable story in this era was the “Armor Wars” saga, which pitted Iron Man against a stable of armoured villains who had capitalized on stolen Stark designs. The 1990s were characterized by uneven stories that too frequently relied on Stark’s apparent death as a plot device. As the Vietnam War became an increasingly distant historical event, Iron Man ’s origin was reimagined to have taken place during the Persian Gulf War. In the early stories of the 21st century, Tony Stark publicly revealed his identity as Iron Man and even served as U.S. secretary of defense. Stark played a major role in Marvel’s Civil War (2006–07) event, and he briefly served as the director of the law-enforcement agency S.H.I.E.L.D. Fan backlash in the wake of Civil War—a story that pitted hero against hero, with Stark serving as the primary antagonist—led to the rebooting of the Iron Man franchise, and writer Matt Fraction and artist Salvador Larroca redefined the character with their award-winning run on Invincible Iron Man (2008–12). When Stark was beaten into a coma during the second Civil War event (2016), the Iron Man persona was assumed by Riri Williams, a brilliant African American teenager who had reverse engineered an Iron Man suit. She eventually adopted the crime-fighting name Ironheart. An alternate version of Iron Man was one of the original members of the title team in The Ultimates, a series about the Avengers’ counterparts in a parallel universe, and he was featured in two Ultimate Iron Man comics miniseries (2005–06 and 2007–08), written by science-fiction author Orson Scott Card. Special offer for students! Check out our special academic rate and excel this spring semester! Learn More See full list on britannica.com Iron Man Inventor Tony Stark applies his genius for high-tech solutions to problems as Iron Man , the armored Avenger. Who is Tony Stark in Iron Man? After surviving an unexpected attack in enemy territory, jet-setting industrialist Tony Stark builds a high-tech suit of armor and vows to protect the world as Iron Man. Straight from the pages of the legendary comic book, Iron Man is a hero who is built - not born - to be unlike any other. Can Tony Stark fly in Iron Man? Iron Man’s suit enables Tony Stark to fly , with upgrades increasing the speed capabilities. An onboard AI system – originally JARVIS, then replaced by FRIDAY – helps Tony control his armor and connect to other systems. Where can I watch Iron Man? Visitors can watch a screening of Iron Man on the southeast lawn of the Thomas Jefferson Building, and view a live drumming performance from Batalá Washington pre-screening. The series will continue through August showing films from the library’s National Film Registry (Thurs, free, Library of Congress). “Soul Divas Reprise.” In the film, following his escape from captivity by a terrorist group, world-famous industrialist and master engineer Stark builds a mechanized suit of armor and becomes the superhero Iron Man ., title: Iron Man - Marvel Cinematic Universe Wiki 10 Genius Robert Downey Jr. Iron Man Quotes From The MCU, Ranked Iron Man | Creators, Stories, Movies, & Facts | Britannica Iron Man (Tony Stark) | Characters | Marvel IRON MAN Gameplay Walkthrough Part 1 FULL GAME [1080p HD Iron Man (Tony Stark) On Screen Profile | Marvel 38 Things to Do in DC: Alicia Keys, Sneaker Con, Movies - Washingtonian Iron Man (2008 film) - Wikipedia, link: https://marvelcinematicuniverse.fandom.com/wiki/Iron_Man'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search = DuckDuckGoSearchResults(output_format=\"list\")\n",
        "\n",
        "search.invoke(\"Iron Man, Tony Stark\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXyLXunjGiQe",
        "outputId": "4d1c34d8-3b98-4eb3-8c94-02c07721c4d3"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'snippet': 'The dual role of Iron Man and Tony Stark allows for the examination of both the perspective of an individual inventor and that of the bureaucracy of governments and corporations, respectively.[142] His business Stark Industries is depicted as a force for good that advances scientific...',\n",
              "  'title': 'Iron Man - Wikipedia',\n",
              "  'link': 'https://en.wikipedia.org/wiki/Iron_Man'},\n",
              " {'snippet': 'Feb 12, 2025 — Anthony Edward Stark is a fictional character primarily portrayed by Robert Downey Jr. in the Marvel Cinematic Universe (MCU) media franchise',\n",
              "  'title': 'Tony Stark (Marvel Cinematic Universe)',\n",
              "  'link': 'https://en.wikipedia.org/wiki/Tony_Stark_(Marvel_Cinematic_Universe)'},\n",
              " {'snippet': \"Iron Man ( Tony Stark ). Tony StarkIron Man . Genius. Billionaire. Philanthropist. Tony Stark 's confidence is only matched by his high-flying abilities as the hero called Iron Man .\",\n",
              "  'title': 'Iron Man ( Tony Stark ) | Characters | Marvel',\n",
              "  'link': 'https://www.marvel.com/characters/iron-man-tony-stark'},\n",
              " {'snippet': 'For More Information: Machine Man , Iron Man 2020. In this reality, Iron Man is Arno Stark , nephew of Tony Stark .',\n",
              "  'title': 'Iron Man (Character) - Comic Vine',\n",
              "  'link': 'https://comicvine.gamespot.com/iron-man/4005-1455/'}]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search = DuckDuckGoSearchResults(backend=\"news\")\n",
        "\n",
        "search.invoke(\"Iron Man, Tony Stark\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "1rzjkeriJJhA",
        "outputId": "0292b33e-024c-45ba-d7b8-a67e4fa10747"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'snippet: Tony Stark is well-known as the \"Invincible\" Iron Man. However, there have been plenty of times in the comics when Tony doesn\\'t live up to that name. With his vast array of heavily armed suits of armor and his long history of battling evil,, title: The 5 Dumbest Ways Iron Man Has Been Taken Down in the Comics, link: https://comicbook.com/comics/news/iron-man-dumbest-defeats-marvel-comics/, date: 2025-09-04T15:00:00+00:00, source: ComicBook.com, snippet: Once Marvel Studios introduces a rebooted Iron Man, the MCU\\'s next Tony Stark needs to fight all the comic book enemies Robert Downey Jr. never met., title: 10 Iron Man Villains Robert Downey Jr.\\'s Tony Stark Replacement Should Fight, link: https://www.msn.com/en-us/movies/news/10-iron-man-villains-robert-downey-jr-s-tony-stark-replacement-should-fight/ar-AA1LXCXw, date: 2025-09-05T02:06:20+00:00, source: Screen Rant, snippet: The Jordan \"Iron Man Mark V\" steps away from Stark\\'s classic red and yellow colours, embracing an elegant blend of silver and red instead. Air Jordan 1 \"Iron Man Mark V\" Captures The Tony Stark Spirit, title: Air Jordan 1 \"Iron Man Mark V\" Captures The Tony Stark Spirit, link: https://www.msn.com/en-us/lifestyle/shopping/air-jordan-1-iron-man-mark-v-captures-the-tony-stark-spirit/ar-AA1LSJgu, date: 2025-09-04T15:05:43+00:00, source: Sneaker Fortress, snippet: We rank the best Iron Man movies & explain why they work so well. We look at both his solo movies and his appearances in other Marvel movies., title: Best Iron Man Movies (Updated: August 2025), link: https://www.superherohype.com/features/607353-best-iron-man-movies, date: 2025-08-23T01:39:00+00:00, source: SuperHeroHype'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "\n",
        "wrapper = DuckDuckGoSearchAPIWrapper(region=\"de-de\", time=\"d\", max_results=2)\n",
        "\n",
        "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source=\"events\")\n",
        "\n",
        "search.invoke(\"Iron Man, Tony Stark\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "sBGu9gndJKxC",
        "outputId": "1fb7715b-6a17-46e6-cb3f-c8afc432136e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'snippet: vor 10 Stunden — Multimillionär, Erfinder und Lebemann Tony Stark (Robert Downey Jr.) alias Iron Man ist in der Krise: Nachdem er bei seinem letzten Versuch, die Welt zu ..., title: Iron Man 3 – Jetzt streamen, link: https://www.sky.de/film/iron-man-3-7884, snippet: vor 4 Stunden — Nach der großen Alien-Inva-sion in New York ist Milliardär Tony Stark (Robert Downey jr.) nicht mehr in Top-Form und leidet verstärkt unter Panikattacken. Als ..., title: Iron Man 3 am 06. September 2025 um 00:05 Uhr auf PULS 4, link: https://www.tvmovie.de/tv/iron-man-3-epg-208511026, snippet: vor 18 Stunden — Fury taucht dabei aus heiterem Himmel im Haus des milliardenschweren Erfinders Tony Stark auf, um ihn für SHIELD zu rekrutieren. Und es bleibt nicht sein ..., title: Entertainment: Iron Man (2008), link: https://www.netzwelt.de/buzz/218584_2-secret-invasion-streamt-10-filme-disney-plus-bevor-neue-marvel-serie-schaut-563.html, snippet: vor 6 Stunden — Wie es scheint, darf Superman bald schon wieder ran, denn James Gunn verkündet seinen nächsten Filmauftritt mit Konzept-Zeichnungen., title: \"Man of Tomorrow\": \"Superman\"-Nachfolger angekündigt + ..., link: https://www.moviejones.de/news/news-man-of-tomorrow-superman-nachfolger-angekuendigt-fortschritte-beim-skript_49864.html'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TDs2kCZsKHRy"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2 — Prompt Engineering with LangChain\n",
        "Goal: Learn how to create effective prompts using LangChain’s PromptTemplate and ChatPromptTemplate to control LLM behavior."
      ],
      "metadata": {
        "id": "C7YeYoLCVPgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 — Using PromptTemplate (Single-Turn Prompts)\n",
        "\n",
        "Description:\n",
        "In this cell, we’ll create a basic PromptTemplate in LangChain, which lets us define a prompt structure with placeholders that are later filled with dynamic inputs.\n",
        "PromptTemplate is ideal for single-turn interactions where you send one prompt and get one response."
      ],
      "metadata": {
        "id": "aK9JF0MfVSb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 2.1 — Using PromptTemplate (Single-Turn Prompts)\n",
        "# ======================================\n",
        "\n",
        "# Import PromptTemplate from LangChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a simple prompt template\n",
        "# ------------------------------------------------------\n",
        "# Parameters:\n",
        "# - input_variables: A list of placeholder names that will be dynamically replaced.\n",
        "# - template: The actual prompt text with placeholders in curly braces {}.\n",
        "# ------------------------------------------------------\n",
        "simple_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Explain {topic} in simple terms for a beginner.\"\n",
        ")\n",
        "\n",
        "# Test the prompt template by formatting it with an example input\n",
        "formatted_prompt = simple_prompt.format(topic=\"LangChain\")\n",
        "print(\"🔹 Formatted Prompt:\\n\", formatted_prompt)\n",
        "\n",
        "# Send the formatted prompt to OpenAI LLM for testing\n",
        "response = openai_llm.invoke(formatted_prompt)\n",
        "print(\"\\n🔹 LLM Response:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOFr01DUVR1X",
        "outputId": "490f3a8d-b314-430f-ffbf-7a06ef5186f3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Formatted Prompt:\n",
            " Explain LangChain in simple terms for a beginner.\n",
            "\n",
            "🔹 LLM Response:\n",
            " Sure! Think of LangChain as a toolkit that helps you build smart computer programs (called \"applications\") that can understand and work with language, like chatting with a chatbot or analyzing text.\n",
            "\n",
            "Here's a simple way to understand it:\n",
            "\n",
            "- **Imagine you want to create a helpful assistant** that can answer questions, summarize articles, or even hold conversations. To do this, you need to connect different parts: understanding language, retrieving information, and generating responses.\n",
            "\n",
            "- **LangChain provides the building blocks** to connect these parts easily. It helps you organize how your program gets information, processes it, and responds.\n",
            "\n",
            "- **In short:** LangChain makes it easier to build apps that use AI language models (like ChatGPT) by providing tools to manage data, handle conversations, and connect different functions smoothly.\n",
            "\n",
            "So, if you're starting out, think of LangChain as a helpful set of Lego pieces that lets you build smarter language-based applications more easily!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 — Using ChatPromptTemplate (Multi-Turn Conversational Prompts)\n",
        "Description:\n",
        "Unlike PromptTemplate, which creates a single string prompt,\n",
        "ChatPromptTemplate is designed for chat-based models like OpenAI’s GPT and Anthropic’s Claude.\n",
        "It allows you to structure prompts using roles (system, human, ai) and makes multi-turn conversations easier to manage."
      ],
      "metadata": {
        "id": "IidTOLhZVrR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 2.2 — Using ChatPromptTemplate (Multi-Turn Conversational Prompts)\n",
        "# ======================================\n",
        "\n",
        "# Import ChatPromptTemplate from LangChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create a chat-based prompt template\n",
        "# ------------------------------------------------------\n",
        "# Here we define two message roles:\n",
        "# 1. System message → sets the assistant's behavior/personality.\n",
        "# 2. Human message → takes a dynamic input from the user.\n",
        "# ------------------------------------------------------\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert AI tutor who explains concepts simply and clearly.\"),\n",
        "    (\"human\", \"Explain {topic} in simple terms for a beginner.\")\n",
        "])\n",
        "\n",
        "# Test the chat prompt template by formatting it with an example input\n",
        "formatted_chat_prompt = chat_prompt.format_messages(topic=\"LangChain\")\n",
        "\n",
        "# The formatted prompt will be a list of structured messages with roles\n",
        "print(\"🔹 Formatted Chat Messages:\\n\", formatted_chat_prompt)\n",
        "\n",
        "# Send the chat prompt to OpenAI LLM for testing\n",
        "response = openai_llm.invoke(formatted_chat_prompt)\n",
        "print(\"\\n🔹 LLM Response:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGc6qOrDVtOr",
        "outputId": "d0770a66-c6a9-46ed-f2fe-e77e774518ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Formatted Chat Messages:\n",
            " [SystemMessage(content='You are an expert AI tutor who explains concepts simply and clearly.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain LangChain in simple terms for a beginner.', additional_kwargs={}, response_metadata={})]\n",
            "\n",
            "🔹 LLM Response:\n",
            " Sure! LangChain is a framework designed to help developers build applications that use language models, like those from OpenAI (such as ChatGPT). Think of it as a toolkit that makes it easier to create programs that can understand and generate human-like text.\n",
            "\n",
            "Here are some key points to understand LangChain:\n",
            "\n",
            "1. **Language Models**: At its core, LangChain works with language models, which are AI systems trained to understand and generate text. These models can answer questions, write stories, summarize information, and more.\n",
            "\n",
            "2. **Chains**: The term \"chain\" in LangChain refers to the idea of linking together different tasks or steps in a process. For example, you might first ask a question, then get a response, and finally process that response in some way. LangChain helps you organize these steps efficiently.\n",
            "\n",
            "3. **Components**: LangChain provides various components that you can use to build your application. These include tools for managing conversations, retrieving information from databases, and formatting responses.\n",
            "\n",
            "4. **Flexibility**: One of the strengths of LangChain is its flexibility. You can customize how the language model interacts with users, how it retrieves information, and how it processes data, allowing you to create unique applications.\n",
            "\n",
            "5. **Use Cases**: Developers use LangChain for various applications, such as chatbots, virtual assistants, content generation, and more. It helps streamline the development process and makes it easier to integrate language models into different projects.\n",
            "\n",
            "In summary, LangChain is a helpful framework for building applications that use language models, allowing developers to create interactive and intelligent text-based applications more easily.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3 — Working with Chains in LangChain\n",
        "🎯 Goal\n",
        "\n",
        "Students will learn:\n",
        "\n",
        "What chains are and why we need them.\n",
        "\n",
        "How to build simple LLM chains.\n",
        "\n",
        "How to combine prompt templates + LLMs.\n",
        "\n",
        "How to extend chains for retrieval (RAG) later."
      ],
      "metadata": {
        "id": "O59q1-ySWeKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 — Introduction to Chains & First Simple LLMChain\n",
        "\n",
        "Description:\n",
        "Chains in LangChain allow us to connect multiple components (like prompt templates, LLMs, retrievers, memory, etc.) into a single pipeline.\n",
        "Without chains, we’d have to:\n",
        "\n",
        "Manually format the prompt\n",
        "\n",
        "Call the LLM directly\n",
        "\n",
        "Parse the result ourselves\n",
        "\n",
        "With chains, LangChain automates these steps and makes the workflow cleaner and reusable.\n",
        "\n",
        "In this demo, we’ll create a basic LLMChain that connects:\n",
        "PromptTemplate → OpenAI LLM → Final Answer"
      ],
      "metadata": {
        "id": "mQC9HYmOWhZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 3.1 — Introduction to Chains (New Runnable Interface)\n",
        "# ======================================\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Step 1: Define a prompt template\n",
        "# ------------------------------------------------------\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Explain {topic} in one sentence, as if teaching a beginner.\"\n",
        ")\n",
        "\n",
        "# Step 2: Combine Prompt + LLM into a runnable chain\n",
        "# ------------------------------------------------------\n",
        "# Instead of using LLMChain, we now connect components using the | operator.\n",
        "chain = prompt | openai_llm\n",
        "\n",
        "# Step 3: Run the chain with an example input\n",
        "topic = \"LangChain\"\n",
        "response = chain.invoke({\"topic\": topic})\n",
        "\n",
        "print(f\"🔹 Prompted Topic: {topic}\")\n",
        "print(\"🔹 LLM Response:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X91j9fawWfKa",
        "outputId": "86fef840-04c8-4f39-b584-9ea62938eeba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Prompted Topic: LangChain\n",
            "🔹 LLM Response:\n",
            " LangChain is a framework that helps developers build applications using language models by providing tools to manage prompts, handle data, and integrate with various APIs and services.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 — Multi-Input Chain Using PromptTemplate | LLM\n",
        "\n",
        "Description:\n",
        "In this cell, we’ll create a prompt that accepts two variables instead of one.\n",
        "This helps students understand how to handle dynamic inputs for more complex prompt engineering."
      ],
      "metadata": {
        "id": "6wR9ZSBOW8UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 3.2 — Multi-Input Chain Using PromptTemplate | LLM\n",
        "# ======================================\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Step 1: Create a multi-variable prompt template\n",
        "# ------------------------------------------------------\n",
        "# Parameters:\n",
        "# - input_variables: List of placeholders we want to replace dynamically.\n",
        "# - template: The prompt with two placeholders: {concept} and {audience}.\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\", \"audience\"],\n",
        "    template=\"Explain {concept} to a {audience} in two sentences.\"\n",
        ")\n",
        "\n",
        "# Step 2: Combine prompt + LLM into a runnable chain\n",
        "# ------------------------------------------------------\n",
        "chain = prompt | openai_llm\n",
        "\n",
        "# Step 3: Run the chain with dynamic inputs\n",
        "inputs = {\n",
        "    \"concept\": \"LangChain\",\n",
        "    \"audience\": \"high school student\"\n",
        "}\n",
        "\n",
        "response = chain.invoke(inputs)\n",
        "\n",
        "# Step 4: Show the results\n",
        "print(f\"🔹 Concept: {inputs['concept']}\")\n",
        "print(f\"🔹 Audience: {inputs['audience']}\")\n",
        "print(\"\\n🔹 LLM Response:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njBfg2DHW-fT",
        "outputId": "3634faec-6c63-4f24-c0cb-81f8645b7a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Concept: LangChain\n",
            "🔹 Audience: high school student\n",
            "\n",
            "🔹 LLM Response:\n",
            " LangChain is a tool that helps developers create applications using language models, like chatbots or virtual assistants, by connecting different parts of the software together. It makes it easier to build complex systems that can understand and generate human-like text, allowing for more interactive and intelligent user experiences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 — Sequential Chains Using RunnableSequence\n",
        "\n",
        "Description:\n",
        "Sometimes, you need multiple reasoning steps.\n",
        "For example:\n",
        "\n",
        "Step 1: Generate an outline for a topic.\n",
        "\n",
        "Step 2: Summarize that outline into a short description.\n",
        "\n",
        "Step 3: Produce the final answer in a clean format.\n",
        "\n",
        "Instead of calling the LLM separately for each step, we chain them together into one pipeline."
      ],
      "metadata": {
        "id": "7n4E_gAYXMyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 3.3 — Sequential Chains Using Runnable Composition\n",
        "# ======================================\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Step 1: Prompt for generating an outline\n",
        "outline_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Create a short outline with 3 bullet points about {topic}.\"\n",
        ")\n",
        "\n",
        "# Step 2: Prompt for summarizing the outline\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"outline\"],\n",
        "    template=\"Summarize the following outline into two sentences:\\n\\n{outline}\"\n",
        ")\n",
        "\n",
        "# Step 3: Create the chain using dictionary mapping\n",
        "# ------------------------------------------------------\n",
        "# Explanation:\n",
        "# 1. First, we generate the outline → outline_prompt | openai_llm\n",
        "# 2. Then, we pass that outline to summary_prompt | openai_llm\n",
        "# 3. RunnablePassthrough lets us carry variables forward without recomputing them.\n",
        "chain = (\n",
        "    {\"outline\": outline_prompt | openai_llm}  # Step 1: Generate outline\n",
        "    | summary_prompt                         # Step 2: Inject outline into summary prompt\n",
        "    | openai_llm                             # Step 3: Summarize\n",
        ")\n",
        "\n",
        "# Step 4: Run the chain\n",
        "topic = \"LangChain framework\"\n",
        "response = chain.invoke({\"topic\": topic})\n",
        "\n",
        "# Step 5: Show the result\n",
        "print(f\"🔹 Topic: {topic}\")\n",
        "print(\"\\n🔹 Final Summary:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3SrxQdKXN2L",
        "outputId": "783c9a24-938b-486e-baa5-7954108c787d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Topic: LangChain framework\n",
            "\n",
            "🔹 Final Summary:\n",
            " The LangChain framework is designed to simplify the integration of language models into various applications, featuring core components like chains, agents, and memory that facilitate complex workflows. It has practical applications in areas such as chatbots, content generation, and data analysis, enhancing productivity and creativity in leveraging language models for diverse tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 — Advanced Multi-Step Reasoning Chain\n",
        "\n",
        "Description:\n",
        "This example demonstrates how to combine multiple prompts + LLMs into a single pipeline,\n",
        "where each step depends on the output of the previous one."
      ],
      "metadata": {
        "id": "konSnY3fY_jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 3.4 — Advanced Multi-Step Reasoning Chain (FINAL FIX)\n",
        "# ======================================\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Step 1: Prompt for generating 3 creative ideas\n",
        "ideas_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=(\n",
        "        \"Generate 3 creative and unique ideas related to the topic: {topic}.\\n\"\n",
        "        \"Number each idea clearly.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Step 2: Prompt for choosing the best idea\n",
        "choose_best_prompt = PromptTemplate(\n",
        "    input_variables=[\"ideas\"],\n",
        "    template=(\n",
        "        \"You are a critical evaluator. Review the following 3 ideas:\\n\\n{ideas}\\n\\n\"\n",
        "        \"Select the SINGLE best idea and explain briefly why you chose it.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Step 3: Prompt for expanding the best idea into an article\n",
        "expand_prompt = PromptTemplate(\n",
        "    input_variables=[\"best_idea\"],\n",
        "    template=\"Write a short, engaging article (5-6 sentences) based on this idea:\\n\\n{best_idea}\"\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Step 4: Build the chain properly\n",
        "# ----------------------------\n",
        "# First: Generate ideas from topic\n",
        "generate_ideas_chain = ideas_prompt | openai_llm\n",
        "\n",
        "# Second: Choose the best idea from generated ideas\n",
        "choose_best_chain = (\n",
        "    {\"ideas\": generate_ideas_chain}           # Take output from first step\n",
        "    | choose_best_prompt\n",
        "    | openai_llm\n",
        ")\n",
        "\n",
        "# Third: Expand the chosen best idea into an article\n",
        "final_chain = (\n",
        "    {\"best_idea\": choose_best_chain}          # Pass the best idea forward\n",
        "    | expand_prompt\n",
        "    | openai_llm\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Step 5: Run the chain\n",
        "# ----------------------------\n",
        "topic = \"Applications of LangChain in real-world business\"\n",
        "response = final_chain.invoke({\"topic\": topic})\n",
        "\n",
        "# ----------------------------\n",
        "# Step 6: Display result\n",
        "# ----------------------------\n",
        "print(f\"🔹 Topic: {topic}\")\n",
        "print(\"\\n🔹 Final Article:\\n\")\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9pIltjqZAM2",
        "outputId": "69d2192e-11df-434d-bb09-8b828680ae70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Topic: Applications of LangChain in real-world business\n",
            "\n",
            "🔹 Final Article:\n",
            "\n",
            "In today's fast-paced business landscape, **Dynamic Customer Support Chatbots** are revolutionizing the way companies interact with their customers. These advanced chatbots, powered by LangChain, offer personalized and context-aware responses that significantly enhance the customer experience. By integrating with CRM systems, they gain valuable insights into customer needs and preferences, allowing them to provide tailored solutions and escalate issues when necessary. This not only streamlines operations by alleviating the workload on human agents but also fosters more engaging and responsive interactions. As these chatbots learn from each interaction, they continuously adapt to evolving customer expectations, ensuring businesses remain competitive. Ultimately, the implementation of dynamic chatbots can lead to immediate and substantial improvements in customer service operations, driving satisfaction and retention.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4 — Working with Documents\n",
        "\n",
        "Goal:\n",
        "Teach students how to load PDFs, split them into chunks, and prepare them for retrieval-augmented generation (RAG).\n",
        "This is the foundation for the next section where we’ll connect everything into a full RAG pipeline."
      ],
      "metadata": {
        "id": "WA2qKh3UbAyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 — Load and Preview a PDF\n",
        "\n",
        "Description:\n",
        "In this cell, we’ll let students upload a PDF into Colab, load it using LangChain’s PyPDFLoader, and preview the first page."
      ],
      "metadata": {
        "id": "KzH_29Cma-42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 4.1 — Load and Preview a PDF\n",
        "# ======================================\n",
        "\n",
        "from google.colab import files\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Step 1: Upload a PDF\n",
        "# ------------------------------------------------------\n",
        "# Students select a file from their computer.\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file path\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "print(f\"✅ Uploaded PDF: {pdf_path}\")\n",
        "\n",
        "# Step 2: Load the PDF using LangChain's PyPDFLoader\n",
        "# ------------------------------------------------------\n",
        "# PyPDFLoader splits the PDF into LangChain Document objects.\n",
        "# - Each Document has .page_content (text) and .metadata (page info).\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "pages = loader.load()\n",
        "\n",
        "# Step 3: Preview the document\n",
        "print(f\"✅ Total pages loaded: {len(pages)}\\n\")\n",
        "print(\"🔹 Preview of the first page:\\n\")\n",
        "print(pages[0].page_content[:800])  # Show first 800 characters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "vll0SJo6bD5L",
        "outputId": "d7c07078-5511-4ff1-ae0d-03f0e127e484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e20d97b9-b1fb-4b40-b3c4-9787fe15bfb1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e20d97b9-b1fb-4b40-b3c4-9787fe15bfb1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf to 2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf\n",
            "✅ Uploaded PDF: 2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf\n",
            "✅ Total pages loaded: 65\n",
            "\n",
            "🔹 Preview of the first page:\n",
            "\n",
            "Prompt  \n",
            "Engineering\n",
            "Author: Lee Boonstra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 — Split Documents into Chunks\n",
        "\n",
        "Description:\n",
        "In this cell, we’ll use LangChain’s RecursiveCharacterTextSplitter to split the PDF into manageable chunks.\n",
        "Why we need chunking:\n",
        "\n",
        "Embeddings work best with shorter text.\n",
        "\n",
        "Smaller chunks improve recall during retrieval.\n",
        "\n",
        "Overlapping chunks preserve context continuity."
      ],
      "metadata": {
        "id": "2ofzEjn8bbgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 4.2 — Split Documents into Chunks\n",
        "# ======================================\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Step 1: Create a text splitter\n",
        "# ------------------------------------------------------\n",
        "# Parameters:\n",
        "# - chunk_size: Maximum characters per chunk (adjust based on model context).\n",
        "# - chunk_overlap: Number of characters repeated between chunks to preserve context.\n",
        "# - separators: Order of preferred split points, largest to smallest.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,       # ~1-2 paragraphs per chunk\n",
        "    chunk_overlap=200,     # Preserve context across chunks\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Step 2: Split the PDF pages into chunks\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "# Step 3: Preview the result\n",
        "print(f\"✅ Total chunks created: {len(chunks)}\")\n",
        "print(\"\\n🔹 First chunk preview:\\n\")\n",
        "print(chunks[0].page_content[:500])  # Show first 500 characters\n",
        "print(\"\\n🔹 Metadata example:\", chunks[0].metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgjbJEELbcyU",
        "outputId": "462dfc2b-66aa-4555-9810-7d4d886f2862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total chunks created: 103\n",
            "\n",
            "🔹 First chunk preview:\n",
            "\n",
            "Prompt  \n",
            "Engineering\n",
            "Author: Lee Boonstra\n",
            "\n",
            "🔹 Metadata example: {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-10-31T13:52:57-06:00', 'moddate': '2024-10-31T13:53:02-06:00', 'trapped': '/False', 'source': '2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf', 'total_pages': 65, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 — Generate Embeddings for Chunks\n",
        "\n",
        "Description:\n",
        "In this cell, we’ll use OpenAI’s text-embedding-3-small model via LangChain’s OpenAIEmbeddings class.\n",
        "We generate embeddings for all chunks so we can later store them in a vector database."
      ],
      "metadata": {
        "id": "fqXVw9NKbCy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 4.3 — Generate Embeddings for Chunks\n",
        "# ======================================\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Step 1: Initialize the embeddings model\n",
        "# ------------------------------------------------------\n",
        "# Parameters:\n",
        "# - model: The OpenAI embeddings model to use.\n",
        "#   Options:\n",
        "#     \"text-embedding-3-small\"  → fast & cost-effective (~1,536 dimensions)\n",
        "#     \"text-embedding-3-large\"  → more accurate (~3,072 dimensions, higher cost)\n",
        "# - api_key: Uses the environment variable configured earlier.\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "# Step 2: Generate embeddings for a sample query (sanity check)\n",
        "sample_vector = embedding_model.embed_query(\"What is LangChain?\")\n",
        "print(f\"✅ Sample embedding vector length: {len(sample_vector)}\")\n",
        "print(f\"🔹 First 10 dimensions: {sample_vector[:10]}\")\n",
        "\n",
        "# Step 3: We'll use this embedding model later when storing chunks in a vector database\n",
        "print(\"\\n✅ Embedding model initialized successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExOUKYopbn-U",
        "outputId": "4cec41cd-cec2-44e9-be26-b46b5b2ab03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sample embedding vector length: 1536\n",
            "🔹 First 10 dimensions: [0.003030850552022457, -0.0035995321813970804, -0.020205670967698097, 0.0034851604141294956, 0.0002958574041258544, -0.0026686734054237604, -0.000631030066870153, 0.019671935588121414, -0.03939470276236534, -0.01785469613969326]\n",
            "\n",
            "✅ Embedding model initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 — Store Chunks in Chroma Vector Database\n",
        "\n",
        "Description:\n",
        "In this cell, we’ll use ChromaDB (a fast in-memory vector database) to store embeddings and their associated chunks.\n",
        "Once stored, we’ll be able to query the database using semantic similarity search."
      ],
      "metadata": {
        "id": "xdUQY5ZSbv7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 4.4 — Store Chunks in Chroma Vector Database\n",
        "# ======================================\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Step 1: Create and populate Chroma DB\n",
        "# ------------------------------------------------------\n",
        "# Parameters:\n",
        "# - documents: The chunks we generated earlier.\n",
        "# - embedding: The OpenAI embeddings model.\n",
        "# - persist_directory: If set to a folder path, Chroma will store data persistently.\n",
        "#   Here we keep it in-memory for simplicity.\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=None   # Set to \"./chroma_db\" if you want to persist locally\n",
        ")\n",
        "\n",
        "print(\"✅ Chroma vector database created successfully!\")\n",
        "\n",
        "# Step 2: Quick test: semantic similarity search\n",
        "# ------------------------------------------------------\n",
        "query = \"Explain the purpose of LangChain\"\n",
        "results = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "print(f\"\\n🔹 Query: {query}\\n\")\n",
        "print(\"🔹 Top 2 retrieved chunks:\\n\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"--- Chunk {i} ---\")\n",
        "    print(doc.page_content[:300], \"...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTNrqo6Tb4u2",
        "outputId": "d5853a8e-dab3-4744-b9fb-26d924df77a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chroma vector database created successfully!\n",
            "\n",
            "🔹 Query: Explain the purpose of LangChain\n",
            "\n",
            "🔹 Top 3 retrieved chunks:\n",
            "\n",
            "--- Chunk 1 ---\n",
            "agent.run(prompt)\n",
            "Snippet 1. Creating a ReAct Agent with LangChain and VertexAI\n",
            "Code\tSnippet\t2\tshows\tthe\tresult.\tNotice\tthat\tReAct\tmakes\ta\tchain\tof\tfive\tsearches.\tIn\tfact,\t\n",
            "the\tLLM\tis\tscraping\tGoogle\tsearch\tresults\tto\tfigure\tout\tthe\tband\tnames.\tThen,\tit\tlists\tthe\t\n",
            "results\tas\tobservations\tand\tchains\t ...\n",
            "\n",
            "--- Chunk 2 ---\n",
            "agent.run(prompt)\n",
            "Snippet 1. Creating a ReAct Agent with LangChain and VertexAI\n",
            "Code\tSnippet\t2\tshows\tthe\tresult.\tNotice\tthat\tReAct\tmakes\ta\tchain\tof\tfive\tsearches.\tIn\tfact,\t\n",
            "the\tLLM\tis\tscraping\tGoogle\tsearch\tresults\tto\tfigure\tout\tthe\tband\tnames.\tThen,\tit\tlists\tthe\t\n",
            "results\tas\tobservations\tand\tchains\t ...\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Prompt Engineering\n",
            "September 2024\n",
            "29\n",
            "Chain of Thought (CoT)\n",
            "Chain of Thought (CoT) 9\tprompting\tis\ta\ttechnique\tfor\timproving\tthe\treasoning\tcapabilities\t\n",
            "of LLMs by generating intermediate reasoning steps.\tThis\thelps\tthe\tLLM\tgenerate\tmore\t\n",
            "accurate\tanswers.\tYou\tcan\tcombine\tit\twith\tfew-shot\tprompting\tt ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5 — Working with Memory\n",
        "\n",
        "Goal\n",
        "Teach students how to add memory to LangChain applications so LLMs can remember previous user inputs and maintain conversational context."
      ],
      "metadata": {
        "id": "kLgUrgL_cUwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 — Using LLMs Without Memory\n",
        "\n",
        "Description:\n",
        "By default, LLMs are stateless — they don’t remember previous messages unless you explicitly resend the context every time."
      ],
      "metadata": {
        "id": "TR8AbXuXcaa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 5.1 — Using LLMs Without Memory\n",
        "# ======================================\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Step 1: Create a simple LLM instance\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Step 2: Create a simple chat prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Step 3: Build a simple runnable chain (no memory)\n",
        "chain_no_memory = prompt | llm\n",
        "\n",
        "# Step 4: Ask two related questions\n",
        "q1 = \"My name is Alex.\"\n",
        "q2 = \"What is my name?\"\n",
        "\n",
        "resp1 = chain_no_memory.invoke({\"input\": q1})\n",
        "resp2 = chain_no_memory.invoke({\"input\": q2})\n",
        "\n",
        "print(\"=== Without Memory ===\\n\")\n",
        "print(f\"Q1: {q1}\\nA1: {resp1.content}\\n\")\n",
        "print(f\"Q2: {q2}\\nA2: {resp2.content}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daAb6tWEcW0E",
        "outputId": "58017d8d-5090-4837-df4e-3895c75cd85e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Without Memory ===\n",
            "\n",
            "Q1: My name is Alex.\n",
            "A1: Nice to meet you, Alex! How can I assist you today?\n",
            "\n",
            "Q2: What is my name?\n",
            "A2: I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 — Using ConversationBufferMemory\n",
        "\n",
        "Description:\n",
        "Now we’ll use ConversationBufferMemory so the LLM remembers the full conversation history."
      ],
      "metadata": {
        "id": "alLkyFgYcuDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 5.2 — Using ConversationBufferMemory\n",
        "# ======================================\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Step 1: Create a prompt that EXPLICITLY shows chat history\n",
        "\n",
        "\n",
        "\n",
        "# Updated prompt with explicit chat_history variable\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"ai\", \"This is the conversation so far:\\n{chat_history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "\n",
        "# Step 1: Create memory object\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",   # Inject history into {chat_history} variable\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# Step 2: Create LLMChain with memory\n",
        "chain_with_memory = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Step 3: Ask two related questions\n",
        "q1 = \"My name is Alex.\"\n",
        "q2 = \"What is my name?\"\n",
        "\n",
        "resp1_mem = chain_with_memory.invoke({\"input\": q1})\n",
        "resp2_mem = chain_with_memory.invoke({\"input\": q2})\n",
        "\n",
        "print(\"=== With Memory (Updated Prompt) ===\\n\")\n",
        "print(f\"Q1: {q1}\\nA1: {resp1_mem['text']}\\n\")\n",
        "print(f\"Q2: {q2}\\nA2: {resp2_mem['text']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QQ6lErYcw-T",
        "outputId": "8642c655-2fa8-472b-9a7d-b3924952f439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2131633540.py:22: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n",
            "/tmp/ipython-input-2131633540.py:28: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain_with_memory = LLMChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== With Memory (Updated Prompt) ===\n",
            "\n",
            "Q1: My name is Alex.\n",
            "A1: Nice to meet you, Alex! How can I assist you today?\n",
            "\n",
            "Q2: What is my name?\n",
            "A2: Your name is Alex.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 — Inspecting Conversation History\n",
        "\n",
        "Description:\n",
        "In this cell, we’ll ask the LLM multiple questions and then print the stored chat history from ConversationBufferMemory.\n",
        "This helps students understand what LangChain remembers and how it injects the history into the prompt."
      ],
      "metadata": {
        "id": "7_U1Z02ydT__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 5.3 — Inspecting Conversation History (Updated)\n",
        "# ======================================\n",
        "\n",
        "# Step 1: Ask multiple related questions\n",
        "q1 = \"I live in Sofia.\"\n",
        "q2 = \"My favorite programming language is Python.\"\n",
        "q3 = \"Where do I live and what is my favorite language?\"\n",
        "\n",
        "resp1 = chain_with_memory.invoke({\"input\": q1})\n",
        "resp2 = chain_with_memory.invoke({\"input\": q2})\n",
        "resp3 = chain_with_memory.invoke({\"input\": q3})\n",
        "\n",
        "# Step 2: Show responses\n",
        "print(\"=== Responses ===\\n\")\n",
        "print(f\"Q1: {q1}\\nA1: {resp1['text']}\\n\")\n",
        "print(f\"Q2: {q2}\\nA2: {resp2['text']}\\n\")\n",
        "print(f\"Q3: {q3}\\nA3: {resp3['text']}\\n\")\n",
        "\n",
        "# Step 3: Inspect stored chat history\n",
        "print(\"=== Stored Conversation History ===\\n\")\n",
        "for msg in memory.chat_memory.messages:\n",
        "    role = \"User\" if msg.type == \"human\" else \"Assistant\"\n",
        "    print(f\"{role}: {msg.content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79RFv9jXdUzz",
        "outputId": "b18e7afd-d112-47b8-b411-8400bbac3980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Responses ===\n",
            "\n",
            "Q1: I live in Sofia.\n",
            "A1: That's great, Alex! Sofia is a beautiful city with a rich history and culture. What do you enjoy most about living there?\n",
            "\n",
            "Q2: My favorite programming language is Python.\n",
            "A2: Python is a fantastic choice! It's known for its readability and versatility. What do you enjoy most about using Python?\n",
            "\n",
            "Q3: Where do I live and what is my favorite language?\n",
            "A3: You live in Sofia, and your favorite programming language is Python.\n",
            "\n",
            "=== Stored Conversation History ===\n",
            "\n",
            "User: My name is Alex.\n",
            "Assistant: Nice to meet you, Alex! How can I assist you today?\n",
            "User: What is my name?\n",
            "Assistant: Your name is Alex.\n",
            "User: I live in Sofia.\n",
            "Assistant: That's great, Alex! Sofia is a beautiful city with a rich history and culture. What do you enjoy most about living there?\n",
            "User: My favorite programming language is Python.\n",
            "Assistant: Python is a fantastic choice! It's known for its readability and versatility. What do you enjoy most about using Python?\n",
            "User: Where do I live and what is my favorite language?\n",
            "Assistant: You live in Sofia, and your favorite programming language is Python.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 — Using ConversationBufferWindowMemory\n",
        "\n",
        "Description:\n",
        "By default, ConversationBufferMemory stores the entire chat history, which can become expensive for long conversations.\n",
        "ConversationBufferWindowMemory solves this by keeping only the last N message pairs."
      ],
      "metadata": {
        "id": "J734MslVefeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 5.4 — Using ConversationBufferWindowMemory (Updated)\n",
        "# ======================================\n",
        "\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Step 1: Create a prompt that EXPLICITLY shows chat history\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"system\", \"Only the remembered conversation is shown here:\\n{chat_history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Step 2: Create a windowed memory object (keep last 2 message pairs)\n",
        "window_memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    k=2,                    # Keep only last 2 message pairs\n",
        "    return_messages=True    # Store structured messages instead of plain text\n",
        ")\n",
        "\n",
        "# Step 3: Create an LLM chain with windowed memory\n",
        "chain_with_window_memory = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=window_memory\n",
        ")\n",
        "\n",
        "# Step 4: Ask multiple questions\n",
        "q1 = \"My name is Alex.\"\n",
        "q2 = \"I live in Sofia.\"\n",
        "q3 = \"I work at Ethera Technologies.\"\n",
        "q4 = \"What is my name?\"\n",
        "\n",
        "resp1 = chain_with_window_memory.invoke({\"input\": q1})\n",
        "resp2 = chain_with_window_memory.invoke({\"input\": q2})\n",
        "resp3 = chain_with_window_memory.invoke({\"input\": q3})\n",
        "resp4 = chain_with_window_memory.invoke({\"input\": q4})\n",
        "\n",
        "# Step 5: Print assistant responses\n",
        "print(\"=== Responses ===\\n\")\n",
        "print(f\"Q1: {q1}\\nA1: {resp1['text']}\\n\")\n",
        "print(f\"Q2: {q2}\\nA2: {resp2['text']}\\n\")\n",
        "print(f\"Q3: {q3}\\nA3: {resp3['text']}\\n\")\n",
        "print(f\"Q4: {q4}\\nA4: {resp4['text']}\\n\")\n",
        "\n",
        "# Step 6: Inspect what is ACTUALLY stored in memory\n",
        "print(\"=== RAW MEMORY STATE (Last 2 Message Pairs) ===\\n\")\n",
        "for msg in window_memory.chat_memory.messages:\n",
        "    role = \"User\" if msg.type == \"human\" else \"Assistant\"\n",
        "    print(f\"{role}: {msg.content}\")\n",
        "\n",
        "# Step 7: Show the chat history variable injected into the prompt\n",
        "print(\"\\n=== CHAT HISTORY IN PROMPT ===\\n\")\n",
        "chat_history_messages = window_memory.load_memory_variables({})[\"chat_history\"]\n",
        "for msg in chat_history_messages:\n",
        "    role = \"User\" if msg.type == \"human\" else \"Assistant\"\n",
        "    print(f\"{role}: {msg.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAPwh6xoegf7",
        "outputId": "d99e2f88-ab0d-46aa-b3ec-ae2344c84b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Responses ===\n",
            "\n",
            "Q1: My name is Alex.\n",
            "A1: Nice to meet you, Alex! How can I assist you today?\n",
            "\n",
            "Q2: I live in Sofia.\n",
            "A2: That's great, Alex! Sofia is a beautiful city with a rich history. What do you enjoy most about living there?\n",
            "\n",
            "Q3: I work at Ethera Technologies.\n",
            "A3: That sounds interesting, Alex! What do you do at Ethera Technologies?\n",
            "\n",
            "Q4: What is my name?\n",
            "A4: Your name is Alex.\n",
            "\n",
            "=== RAW MEMORY STATE (Last 2 Message Pairs) ===\n",
            "\n",
            "User: My name is Alex.\n",
            "Assistant: Nice to meet you, Alex! How can I assist you today?\n",
            "User: I live in Sofia.\n",
            "Assistant: That's great, Alex! Sofia is a beautiful city with a rich history. What do you enjoy most about living there?\n",
            "User: I work at Ethera Technologies.\n",
            "Assistant: That sounds interesting, Alex! What do you do at Ethera Technologies?\n",
            "User: What is my name?\n",
            "Assistant: Your name is Alex.\n",
            "\n",
            "=== CHAT HISTORY IN PROMPT ===\n",
            "\n",
            "User: I work at Ethera Technologies.\n",
            "Assistant: That sounds interesting, Alex! What do you do at Ethera Technologies?\n",
            "User: What is my name?\n",
            "Assistant: Your name is Alex.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6 — Advanced Memory Types\n",
        "\n",
        "In this section, we’ll demonstrate how LangChain offers multiple memory strategies depending on:\n",
        "\n",
        "Chat length\n",
        "\n",
        "Token cost\n",
        "\n",
        "Desired behavior (remember everything vs. summarize vs. extract facts)"
      ],
      "metadata": {
        "id": "Tu5MGohTfqBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 6.1 — Using ConversationSummaryMemory\n",
        "\n",
        "Goal:\n",
        "Show students how LangChain can summarize previous conversations automatically\n",
        "instead of storing the entire history.\n",
        "\n",
        "This is useful when:\n",
        "\n",
        "Conversations are very long.\n",
        "\n",
        "You want the model to have compressed context."
      ],
      "metadata": {
        "id": "ky1uaD-OftHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 6.1 — Using ConversationSummaryMemory\n",
        "# ======================================\n",
        "\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Step 1: Create the prompt with explicit {chat_history}\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"system\", \"Here is the summary of the conversation so far:\\n{chat_history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "\n",
        "summary_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_completion_tokens=50)\n",
        "\n",
        "# Step 2: Create ConversationSummaryMemory\n",
        "# ------------------------------------------------------\n",
        "# Instead of storing full messages, it generates a summary using the LLM.\n",
        "summary_memory = ConversationSummaryMemory(\n",
        "    llm=llm,                   # Uses the same LLM for summarization\n",
        "    memory_key=\"chat_history\", # Injected into the prompt\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# Step 3: Create a chain using summary memory\n",
        "summary_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=summary_memory\n",
        ")\n",
        "\n",
        "# Step 4: Ask multiple questions to demonstrate automatic summarization\n",
        "q1 = \"My name is Alex, and I live in Sofia.\"\n",
        "q2 = \"I work at Ethera Technologies as an AI consultant.\"\n",
        "q3 = \"Can you summarize my personal details?\"\n",
        "\n",
        "resp1 = summary_chain.invoke({\"input\": q1})\n",
        "resp2 = summary_chain.invoke({\"input\": q2})\n",
        "resp3 = summary_chain.invoke({\"input\": q3})\n",
        "\n",
        "# Step 5: Print responses\n",
        "print(\"=== Responses ===\\n\")\n",
        "print(f\"Q1: {q1}\\nA1: {resp1['text']}\\n\")\n",
        "print(f\"Q2: {q2}\\nA2: {resp2['text']}\\n\")\n",
        "print(f\"Q3: {q3}\\nA3: {resp3['text']}\\n\")\n",
        "\n",
        "# Step 6: Inspect the stored memory summary\n",
        "print(\"\\n=== STORED MEMORY (Summary) ===\\n\")\n",
        "print(summary_memory.load_memory_variables({})[\"chat_history\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0_lDthBfqzY",
        "outputId": "4d8a8366-0a7c-4400-bd1c-894a314b211c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Responses ===\n",
            "\n",
            "Q1: My name is Alex, and I live in Sofia.\n",
            "A1: Nice to meet you, Alex! How can I assist you today?\n",
            "\n",
            "Q2: I work at Ethera Technologies as an AI consultant.\n",
            "A2: That's great to hear, Alex! Working as an AI consultant must be quite interesting, especially with the rapid advancements in the field. What kind of projects or areas do you focus on at Ethera Technologies?\n",
            "\n",
            "Q3: Can you summarize my personal details?\n",
            "A3: Sure! Here are the personal details you've shared so far:\n",
            "\n",
            "- Your name is Alex.\n",
            "- You live in Sofia.\n",
            "- You work at Ethera Technologies as an AI consultant.\n",
            "\n",
            "If there's anything else you'd like to add or modify, just let me know!\n",
            "\n",
            "\n",
            "=== STORED MEMORY (Summary) ===\n",
            "\n",
            "[SystemMessage(content=\"The human introduces themselves as Alex and mentions they live in Sofia. The AI expresses pleasure in meeting Alex and asks how it can assist them today. Alex shares that they work at Ethera Technologies as an AI consultant. The AI then summarizes Alex's personal\", additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 — Using ConversationKGMemory (Entity & Fact Memory)\n",
        "\n",
        "Description:\n",
        "ConversationKGMemory extracts structured facts (subject–predicate–object triplets) from the dialog (e.g., “Alex — lives_in — Sofia”).\n",
        "This is useful when you want the assistant to remember stable facts about a user or topic without carrying the entire chat text."
      ],
      "metadata": {
        "id": "R24FZ--4iOtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 6.2 — Using ConversationKGMemory (Fixed)\n",
        "# ======================================\n",
        "\n",
        "# 1) Use the community package (recommended in recent LangChain versions)\n",
        "from langchain_community.memory.kg import ConversationKGMemory\n",
        "from langchain_community.graphs.networkx_graph import NetworkxEntityGraph  # for direct triples insight\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# 2) Prompt uses the *same* memory_key we give to KG memory below (\"history\")\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"system\", \"Known facts extracted so far:\\n{history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# 3) KG memory: align keys with LLMChain (input='input', output='text'); keep a few turns in view (k)\n",
        "kg_memory = ConversationKGMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"history\",   # <- matches prompt\n",
        "    input_key=\"input\",      # <- LLMChain input dict will have {\"input\": \"...\"}\n",
        "    output_key=\"text\",      # <- LLMChain default output key is \"text\"\n",
        "    return_messages=False,  # you can set True if you want message objects instead of string\n",
        "    k=4                     # consider last 4 utterances (2 pairs) when extracting\n",
        ")\n",
        "\n",
        "kg_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=kg_memory,\n",
        "    output_key=\"text\"       # explicit to match memory.output_key\n",
        ")\n",
        "\n",
        "# 4) Mini-dialog populating facts\n",
        "u1 = \"My name is Alex.\"\n",
        "u2 = \"I live in Sofia, Bulgaria.\"\n",
        "u3 = \"I work at Ethera Technologies as an AI consultant.\"\n",
        "u4 = \"What do you know about me so far?\"\n",
        "\n",
        "r1 = kg_chain.invoke({\"input\": u1})\n",
        "r2 = kg_chain.invoke({\"input\": u2})\n",
        "r3 = kg_chain.invoke({\"input\": u3})\n",
        "r4 = kg_chain.invoke({\"input\": u4})\n",
        "\n",
        "print(\"=== Responses ===\\n\")\n",
        "print(f\"U1: {u1}\\nA1: {r1['text']}\\n\")\n",
        "print(f\"U2: {u2}\\nA2: {r2['text']}\\n\")\n",
        "print(f\"U3: {u3}\\nA3: {r3['text']}\\n\")\n",
        "print(f\"U4: {u4}\\nA4: {r4['text']}\\n\")\n",
        "\n",
        "# 5) Inspect the raw triples directly from the graph (bypasses the entity filter)\n",
        "print(\"=== RAW TRIPLES IN KG ===\")\n",
        "print(kg_memory.kg.get_triples())  # list of (subject, relation, object)\n",
        "# (If this prints [], the extractor didn't parse your inputs; try rephrasing u1–u3 slightly.)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTKB-DcLiPQh",
        "outputId": "298db15a-fc0e-4a2f-fa64-f7c893f3d99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Responses ===\n",
            "\n",
            "U1: My name is Alex.\n",
            "A1: Nice to meet you, Alex! How can I assist you today?\n",
            "\n",
            "U2: I live in Sofia, Bulgaria.\n",
            "A2: That's great! Sofia is a beautiful city with a rich history and vibrant culture. If you have any questions about Sofia or need information about anything specific, feel free to ask!\n",
            "\n",
            "U3: I work at Ethera Technologies as an AI consultant.\n",
            "A3: That's great! As an AI consultant at Ethera Technologies, you likely work on various projects involving artificial intelligence, machine learning, and data analysis. If you have any specific questions or topics you'd like to discuss related to your work or AI in general, feel free to ask!\n",
            "\n",
            "U4: What do you know about me so far?\n",
            "A4: I don't have any information about you. I don't have access to personal data unless you share it with me during our conversation. How can I assist you today?\n",
            "\n",
            "=== RAW TRIPLES IN KG ===\n",
            "[('Nevada', 'state', 'is a'), ('Sofia', 'Bulgaria', 'is in'), ('Ethera Technologies', 'company', 'is a'), ('Alex', 'Ethera Technologies', 'works at'), ('Alex', 'AI consultant', 'is an')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7 — LangChain Agents\n",
        "Goal\n",
        "Teach how LangChain agents work, what tools are, and how to build a simple example where an agent decides what to do step by step."
      ],
      "metadata": {
        "id": "CbjVwsH1pE2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 7.1 — Basic Agent with Built-in Tools\n",
        "\n",
        "Description:\n",
        "In this section, we introduce LangChain agents and demonstrate how to create a simple agent that can reason, choose between multiple tools, and combine their outputs to answer complex questions.\n",
        "Students will learn:\n",
        "\n",
        "How to initialize an agent with an LLM.\n",
        "\n",
        "How to register built-in tools like Wikipedia search and Calculator.\n",
        "\n",
        "How the agent decides which tool to use based on the tool descriptions.\n",
        "\n",
        "How the agent combines results from multiple tools to provide a final answer.\n",
        "\n",
        "The code cell I shared above stays the same — we only add this description at the start of the section."
      ],
      "metadata": {
        "id": "8ZQvdYdwpson"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U wikipedia\n",
        "!pip install -U langchain-experimental"
      ],
      "metadata": {
        "id": "NpVaOx_Xqc9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 7.1 — Using Custom Tools (Function Calling)\n",
        "# ======================================\n",
        "\n",
        "from langchain.tools import StructuredTool\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableMap\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# -------------------------\n",
        "# Step 1: Custom Function\n",
        "# -------------------------\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Simulated weather info provider.\"\"\"\n",
        "    fake_data = {\n",
        "        \"Sofia\": \"Sunny, 28°C\",\n",
        "        \"Plovdiv\": \"Partly cloudy, 25°C\",\n",
        "        \"Varna\": \"Windy, 22°C\"\n",
        "    }\n",
        "    return fake_data.get(city, f\"Weather data for {city} is not available.\")\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Wrap the Function as a LangChain Tool\n",
        "# -------------------------\n",
        "weather_tool = StructuredTool.from_function(\n",
        "    func=get_weather,\n",
        "    name=\"WeatherTool\",\n",
        "    description=\"Use this tool to get the current weather for a specific city.\"\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Step 3: Prompt to Extract City Name\n",
        "# -------------------------\n",
        "city_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Extract ONLY the city name from the user's message.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# Step 4: Create the Routing Function\n",
        "# -------------------------\n",
        "def route_question(inputs: dict):\n",
        "    \"\"\"\n",
        "    Decides whether to call the weather tool or the LLM.\n",
        "\n",
        "    Parameters:\n",
        "        inputs (dict):\n",
        "            - \"input\" (str): The user question.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with {\"response\": <str>} containing the assistant's reply.\n",
        "    \"\"\"\n",
        "    query = inputs[\"input\"].lower()\n",
        "\n",
        "    # If the user asks about weather → extract the city → call tool\n",
        "    if \"weather\" in query:\n",
        "        city = llm.predict(city_prompt.format(input=inputs[\"input\"])).strip()\n",
        "        return {\"response\": weather_tool.func(city)}\n",
        "\n",
        "    # Otherwise → just answer normally with LLM\n",
        "    return {\"response\": llm.predict(inputs[\"input\"])}\n",
        "\n",
        "# -------------------------\n",
        "# Step 5: Build the Chain\n",
        "# -------------------------\n",
        "chain = RunnableMap({\n",
        "    \"input\": RunnablePassthrough(),\n",
        "    \"response\": route_question\n",
        "})\n",
        "\n",
        "# -------------------------\n",
        "# Step 6: Test It\n",
        "# -------------------------\n",
        "questions = [\n",
        "    \"What's the weather in Sofia?\",\n",
        "    \"What's the weather in Varna?\",\n",
        "    \"Who founded OpenAI?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    # ✅ Now we pass a dict to chain.invoke()\n",
        "    result = chain.invoke({\"input\": q})\n",
        "    print(f\"\\nUser: {q}\\nAssistant: {result['response']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQMLzk0dnAqw",
        "outputId": "14277426-028d-491a-a840-ca72b0b89a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1272579295.py:57: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  city = llm.predict(city_prompt.format(input=inputs[\"input\"])).strip()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: What's the weather in Sofia?\n",
            "Assistant: {'response': 'Sunny, 28°C'}\n",
            "\n",
            "User: What's the weather in Varna?\n",
            "Assistant: {'response': 'Windy, 22°C'}\n",
            "\n",
            "User: Who founded OpenAI?\n",
            "Assistant: {'response': 'OpenAI was founded in December 2015 by Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, Wojciech Zaremba, and John Schulman, among others. The organization was established with the goal of advancing artificial intelligence in a way that is safe and beneficial for humanity.'}\n"
          ]
        }
      ]
    }
  ]
}