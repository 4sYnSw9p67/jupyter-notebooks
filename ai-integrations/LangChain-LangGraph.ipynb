{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 0 ‚Äî Environment Setup"
      ],
      "metadata": {
        "id": "VKsiBpzxS3GG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 ‚Äî Install Required Libraries\n",
        "\n",
        "Description:\n",
        "We‚Äôll install the core dependencies for LangChain, OpenAI, Anthropic, and vector DBs (Chroma)."
      ],
      "metadata": {
        "id": "r1zB2BTtS4vD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiPuxSgoSUe5"
      },
      "outputs": [],
      "source": [
        "# ======================================\n",
        "# Section 0.1 ‚Äî Install Required Libraries\n",
        "# ======================================\n",
        "\n",
        "!pip install -U langchain langchain-openai langchain-anthropic langchain-community \\\n",
        "              chromadb pypdf tiktoken langsmith python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 ‚Äî Import Dependencies\n",
        "\n",
        "Description:\n",
        "We import the necessary libraries for this lecture, including LangChain, OpenAI, and Anthropic integrations."
      ],
      "metadata": {
        "id": "lHyJgWyiTBSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 0.2 ‚Äî Import Dependencies\n",
        "# ======================================\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# LangChain Core\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic"
      ],
      "metadata": {
        "id": "NH3fibE5S7qC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 ‚Äî Configure API Keys\n",
        "\n",
        "Description:\n",
        "We‚Äôll load API keys stored in Google Colab environment variables.\n",
        "Before running, set your keys in:\n",
        "Colab ‚Üí Settings ‚Üí Variables ‚Üí Add OPENAI_API_KEY and ANTHROPIC_API_KEY."
      ],
      "metadata": {
        "id": "r3NHEUbYTI2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 0.3 ‚Äî Configure API Keys\n",
        "# ======================================\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get(\"LANGSMITH_API_KEY\")\n",
        "\n",
        "# Optional: LangSmith tracing (debugging & observability)\n",
        "os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-lecture\"\n",
        "\n",
        "print(\"‚úÖ API keys configured successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQLfSZq9TKz_",
        "outputId": "c51ff887-4b8d-4c58-86b7-757438cddd90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API keys configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.4 ‚Äî Verify Installation & LLM Connectivity\n",
        "\n",
        "Description:\n",
        "We‚Äôll run a quick test to ensure both OpenAI and Anthropic models are working."
      ],
      "metadata": {
        "id": "-wRk63ZvTh0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 0.4 ‚Äî Verify Installation & LLM Connectivity\n",
        "# ======================================\n",
        "\n",
        "# Test OpenAI connection\n",
        "openai_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "response_openai = openai_llm.invoke(\"Say 'ready' if you can hear me.\")\n",
        "print(\"üîπ OpenAI LLM:\", response_openai.content)\n",
        "\n",
        "# Test Anthropic connection (optional)\n",
        "try:\n",
        "    anthropic_llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
        "    response_anthropic = anthropic_llm.invoke(\"Say 'ready' if you can hear me.\")\n",
        "    print(\"üîπ Anthropic LLM:\", response_anthropic.content)\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Anthropic test skipped (no API key or model unavailable).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBIZ0Ij8Tll2",
        "outputId": "aa1c9c9e-05e2-4b4e-f489-9f864ea4bf42"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ OpenAI LLM: Ready!\n",
            "üîπ Anthropic LLM: Ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1 ‚Äî Understanding LLMs (Large Language Models)\n",
        "Goal: Understand what LLMs are, how LangChain integrates them, and how to initialize and test models from OpenAI and Anthropic."
      ],
      "metadata": {
        "id": "zUPoz-CoUAn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 ‚Äî Initialize & Test OpenAI LLM\n",
        "\n",
        "Description:\n",
        "In this cell, we‚Äôll set up and test an OpenAI chat model using LangChain‚Äôs ChatOpenAI wrapper.\n",
        "We‚Äôll explain key parameters, even if we don‚Äôt modify them yet."
      ],
      "metadata": {
        "id": "LpuRekQXULD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 1.1 ‚Äî Initialize & Test OpenAI LLM\n",
        "# ======================================\n",
        "\n",
        "# Import the OpenAI chat model integration for LangChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Create an OpenAI LLM instance\n",
        "# --------------------------------------\n",
        "# Parameters:\n",
        "# - model: The specific OpenAI model to use.\n",
        "#          Here we use \"gpt-4o-mini\" (fast, cost-effective, supports RAG).\n",
        "# - temperature: Controls randomness of output.\n",
        "#       0.0 ‚Üí deterministic responses\n",
        "#       1.0 ‚Üí more creative and diverse responses\n",
        "# - max_tokens: Maximum number of tokens the model can return in the response.\n",
        "# - timeout: Optional timeout for API call (not used here).\n",
        "# - api_key: Uses the API key we configured earlier via environment variables.\n",
        "openai_llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",   # Recommended for fast, cost-effective RAG tasks\n",
        "    temperature=0.2,       # Low randomness for more precise answers\n",
        "    max_tokens=500         # Reasonable cap for lecture demos\n",
        ")\n",
        "\n",
        "# Quick test to verify OpenAI is working\n",
        "response = openai_llm.invoke(\"Explain in one sentence what LangChain is.\")\n",
        "print(\"üîπ OpenAI LLM Response:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-00BcQYSUKLd",
        "outputId": "3ada0142-e74d-49c6-fe0d-191dd12d49d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ OpenAI LLM Response:\n",
            " LangChain is a framework designed to facilitate the development of applications that leverage large language models by providing tools for chaining together various components like prompts, memory, and APIs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1.2 ‚Äî Initialize & Test Anthropic LLM\n",
        "Description:\n",
        "In this cell, we‚Äôll set up and test an Anthropic chat model using LangChain‚Äôs ChatAnthropic wrapper.\n",
        "Anthropic‚Äôs models (e.g., Claude 3) are great for reasoning, summarization, and multi-step problem solving."
      ],
      "metadata": {
        "id": "jVEXKD6RUjGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 1.2 ‚Äî Initialize & Test Anthropic LLM\n",
        "# ======================================\n",
        "\n",
        "# Import the Anthropic chat model integration for LangChain\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "# Create an Anthropic LLM instance\n",
        "# --------------------------------------\n",
        "# Parameters:\n",
        "# - model: The Anthropic Claude model to use.\n",
        "#          Here we use \"claude-3-haiku-20240307\" (fast, cost-effective).\n",
        "#          Alternative: \"claude-3-sonnet-20240229\" (higher quality, slower).\n",
        "# - temperature: Controls creativity.\n",
        "#       0.0 ‚Üí deterministic responses\n",
        "#       1.0 ‚Üí more creative and diverse outputs\n",
        "# - max_tokens: Maximum number of tokens in the response.\n",
        "# - api_key: Uses the API key configured earlier via environment variables.\n",
        "anthropic_llm = ChatAnthropic(\n",
        "    model=\"claude-3-haiku-20240307\",  # Optimized for speed and low latency\n",
        "    temperature=0.2,                  # Keep responses focused and consistent\n",
        "    max_tokens=500                    # Reasonable cap for demos\n",
        ")\n",
        "\n",
        "# Quick test to verify Anthropic is working\n",
        "try:\n",
        "    response = anthropic_llm.invoke(\"Explain in one sentence what LangChain is.\")\n",
        "    print(\"üîπ Anthropic LLM Response:\\n\", response.content)\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Anthropic model test skipped:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDvaJ3B6UmDl",
        "outputId": "e56d2c42-9854-48de-e4fc-196fd2980efd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Anthropic LLM Response:\n",
            " LangChain is a framework for building applications with large language models (LLMs) that provides a set of abstractions and tools to make it easier to build applications that interact with LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2 ‚Äî Prompt Engineering with LangChain\n",
        "Goal: Learn how to create effective prompts using LangChain‚Äôs PromptTemplate and ChatPromptTemplate to control LLM behavior."
      ],
      "metadata": {
        "id": "C7YeYoLCVPgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 ‚Äî Using PromptTemplate (Single-Turn Prompts)\n",
        "\n",
        "Description:\n",
        "In this cell, we‚Äôll create a basic PromptTemplate in LangChain, which lets us define a prompt structure with placeholders that are later filled with dynamic inputs.\n",
        "PromptTemplate is ideal for single-turn interactions where you send one prompt and get one response."
      ],
      "metadata": {
        "id": "aK9JF0MfVSb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 2.1 ‚Äî Using PromptTemplate (Single-Turn Prompts)\n",
        "# ======================================\n",
        "\n",
        "# Import PromptTemplate from LangChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a simple prompt template\n",
        "# ------------------------------------------------------\n",
        "# Parameters:\n",
        "# - input_variables: A list of placeholder names that will be dynamically replaced.\n",
        "# - template: The actual prompt text with placeholders in curly braces {}.\n",
        "# ------------------------------------------------------\n",
        "simple_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Explain {topic} in simple terms for a beginner.\"\n",
        ")\n",
        "\n",
        "# Test the prompt template by formatting it with an example input\n",
        "formatted_prompt = simple_prompt.format(topic=\"LangChain\")\n",
        "print(\"üîπ Formatted Prompt:\\n\", formatted_prompt)\n",
        "\n",
        "# Send the formatted prompt to OpenAI LLM for testing\n",
        "response = openai_llm.invoke(formatted_prompt)\n",
        "print(\"\\nüîπ LLM Response:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOFr01DUVR1X",
        "outputId": "45a6731a-b543-44e3-8bcf-c499c958669e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Formatted Prompt:\n",
            " Explain LangChain in simple terms for a beginner.\n",
            "\n",
            "üîπ LLM Response:\n",
            " LangChain is a framework designed to help developers build applications that use language models, like those from OpenAI, in a more structured and efficient way. Here‚Äôs a simple breakdown:\n",
            "\n",
            "1. **Language Models**: These are AI systems that can understand and generate human-like text. They can answer questions, write stories, summarize information, and more.\n",
            "\n",
            "2. **Building Blocks**: LangChain provides various components (or \"chains\") that you can use to create applications. These components can handle tasks like retrieving information, processing text, and managing conversations.\n",
            "\n",
            "3. **Integration**: It helps you connect different parts of your application easily. For example, you can combine a language model with a database to fetch information and then use the model to generate a response based on that data.\n",
            "\n",
            "4. **Flexibility**: LangChain is designed to be flexible, allowing developers to customize how they use language models. You can create simple applications or complex systems that involve multiple steps and interactions.\n",
            "\n",
            "5. **Use Cases**: You can use LangChain for various applications, such as chatbots, content generation tools, or any software that needs to understand and generate text.\n",
            "\n",
            "In summary, LangChain is a helpful toolkit for developers who want to create applications that leverage the power of language models, making it easier to build, connect, and customize their projects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 ‚Äî Using ChatPromptTemplate (Multi-Turn Conversational Prompts)\n",
        "Description:\n",
        "Unlike PromptTemplate, which creates a single string prompt,\n",
        "ChatPromptTemplate is designed for chat-based models like OpenAI‚Äôs GPT and Anthropic‚Äôs Claude.\n",
        "It allows you to structure prompts using roles (system, human, ai) and makes multi-turn conversations easier to manage."
      ],
      "metadata": {
        "id": "IidTOLhZVrR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 2.2 ‚Äî Using ChatPromptTemplate (Multi-Turn Conversational Prompts)\n",
        "# ======================================\n",
        "\n",
        "# Import ChatPromptTemplate from LangChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create a chat-based prompt template\n",
        "# ------------------------------------------------------\n",
        "# Here we define two message roles:\n",
        "# 1. System message ‚Üí sets the assistant's behavior/personality.\n",
        "# 2. Human message ‚Üí takes a dynamic input from the user.\n",
        "# ------------------------------------------------------\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert AI tutor who explains concepts simply and clearly.\"),\n",
        "    (\"human\", \"Explain {topic} in simple terms for a beginner.\")\n",
        "])\n",
        "\n",
        "# Test the chat prompt template by formatting it with an example input\n",
        "formatted_chat_prompt = chat_prompt.format_messages(topic=\"LangChain\")\n",
        "\n",
        "# The formatted prompt will be a list of structured messages with roles\n",
        "print(\"üîπ Formatted Chat Messages:\\n\", formatted_chat_prompt)\n",
        "\n",
        "# Send the chat prompt to OpenAI LLM for testing\n",
        "response = openai_llm.invoke(formatted_chat_prompt)\n",
        "print(\"\\nüîπ LLM Response:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGc6qOrDVtOr",
        "outputId": "d0770a66-c6a9-46ed-f2fe-e77e774518ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Formatted Chat Messages:\n",
            " [SystemMessage(content='You are an expert AI tutor who explains concepts simply and clearly.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain LangChain in simple terms for a beginner.', additional_kwargs={}, response_metadata={})]\n",
            "\n",
            "üîπ LLM Response:\n",
            " Sure! LangChain is a framework designed to help developers build applications that use language models, like those from OpenAI (such as ChatGPT). Think of it as a toolkit that makes it easier to create programs that can understand and generate human-like text.\n",
            "\n",
            "Here are some key points to understand LangChain:\n",
            "\n",
            "1. **Language Models**: At its core, LangChain works with language models, which are AI systems trained to understand and generate text. These models can answer questions, write stories, summarize information, and more.\n",
            "\n",
            "2. **Chains**: The term \"chain\" in LangChain refers to the idea of linking together different tasks or steps in a process. For example, you might first ask a question, then get a response, and finally process that response in some way. LangChain helps you organize these steps efficiently.\n",
            "\n",
            "3. **Components**: LangChain provides various components that you can use to build your application. These include tools for managing conversations, retrieving information from databases, and formatting responses.\n",
            "\n",
            "4. **Flexibility**: One of the strengths of LangChain is its flexibility. You can customize how the language model interacts with users, how it retrieves information, and how it processes data, allowing you to create unique applications.\n",
            "\n",
            "5. **Use Cases**: Developers use LangChain for various applications, such as chatbots, virtual assistants, content generation, and more. It helps streamline the development process and makes it easier to integrate language models into different projects.\n",
            "\n",
            "In summary, LangChain is a helpful framework for building applications that use language models, allowing developers to create interactive and intelligent text-based applications more easily.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3 ‚Äî Working with Chains in LangChain\n",
        "üéØ Goal\n",
        "\n",
        "Students will learn:\n",
        "\n",
        "What chains are and why we need them.\n",
        "\n",
        "How to build simple LLM chains.\n",
        "\n",
        "How to combine prompt templates + LLMs.\n",
        "\n",
        "How to extend chains for retrieval (RAG) later."
      ],
      "metadata": {
        "id": "O59q1-ySWeKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 ‚Äî Introduction to Chains & First Simple LLMChain\n",
        "\n",
        "Description:\n",
        "Chains in LangChain allow us to connect multiple components (like prompt templates, LLMs, retrievers, memory, etc.) into a single pipeline.\n",
        "Without chains, we‚Äôd have to:\n",
        "\n",
        "Manually format the prompt\n",
        "\n",
        "Call the LLM directly\n",
        "\n",
        "Parse the result ourselves\n",
        "\n",
        "With chains, LangChain automates these steps and makes the workflow cleaner and reusable.\n",
        "\n",
        "In this demo, we‚Äôll create a basic LLMChain that connects:\n",
        "PromptTemplate ‚Üí OpenAI LLM ‚Üí Final Answer"
      ],
      "metadata": {
        "id": "mQC9HYmOWhZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 3.1 ‚Äî Introduction to Chains (New Runnable Interface)\n",
        "# ======================================\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Step 1: Define a prompt template\n",
        "# ------------------------------------------------------\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Explain {topic} in one sentence, as if teaching a beginner.\"\n",
        ")\n",
        "\n",
        "# Step 2: Combine Prompt + LLM into a runnable chain\n",
        "# ------------------------------------------------------\n",
        "# Instead of using LLMChain, we now connect components using the | operator.\n",
        "chain = prompt | openai_llm\n",
        "\n",
        "# Step 3: Run the chain with an example input\n",
        "topic = \"LangChain\"\n",
        "response = chain.invoke({\"topic\": topic})\n",
        "\n",
        "print(f\"üîπ Prompted Topic: {topic}\")\n",
        "print(\"üîπ LLM Response:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X91j9fawWfKa",
        "outputId": "86fef840-04c8-4f39-b584-9ea62938eeba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Prompted Topic: LangChain\n",
            "üîπ LLM Response:\n",
            " LangChain is a framework that helps developers build applications using language models by providing tools to manage prompts, handle data, and integrate with various APIs and services.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 ‚Äî Multi-Input Chain Using PromptTemplate | LLM\n",
        "\n",
        "Description:\n",
        "In this cell, we‚Äôll create a prompt that accepts two variables instead of one.\n",
        "This helps students understand how to handle dynamic inputs for more complex prompt engineering."
      ],
      "metadata": {
        "id": "6wR9ZSBOW8UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 3.2 ‚Äî Multi-Input Chain Using PromptTemplate | LLM\n",
        "# ======================================\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Step 1: Create a multi-variable prompt template\n",
        "# ------------------------------------------------------\n",
        "# Parameters:\n",
        "# - input_variables: List of placeholders we want to replace dynamically.\n",
        "# - template: The prompt with two placeholders: {concept} and {audience}.\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\", \"audience\"],\n",
        "    template=\"Explain {concept} to a {audience} in two sentences.\"\n",
        ")\n",
        "\n",
        "# Step 2: Combine prompt + LLM into a runnable chain\n",
        "# ------------------------------------------------------\n",
        "chain = prompt | openai_llm\n",
        "\n",
        "# Step 3: Run the chain with dynamic inputs\n",
        "inputs = {\n",
        "    \"concept\": \"LangChain\",\n",
        "    \"audience\": \"high school student\"\n",
        "}\n",
        "\n",
        "response = chain.invoke(inputs)\n",
        "\n",
        "# Step 4: Show the results\n",
        "print(f\"üîπ Concept: {inputs['concept']}\")\n",
        "print(f\"üîπ Audience: {inputs['audience']}\")\n",
        "print(\"\\nüîπ LLM Response:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njBfg2DHW-fT",
        "outputId": "3634faec-6c63-4f24-c0cb-81f8645b7a3d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Concept: LangChain\n",
            "üîπ Audience: high school student\n",
            "\n",
            "üîπ LLM Response:\n",
            " LangChain is a tool that helps developers create applications using language models, like chatbots or virtual assistants, by connecting different parts of the software together. It makes it easier to build complex systems that can understand and generate human-like text, allowing for more interactive and intelligent user experiences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 ‚Äî Sequential Chains Using RunnableSequence\n",
        "\n",
        "Description:\n",
        "Sometimes, you need multiple reasoning steps.\n",
        "For example:\n",
        "\n",
        "Step 1: Generate an outline for a topic.\n",
        "\n",
        "Step 2: Summarize that outline into a short description.\n",
        "\n",
        "Step 3: Produce the final answer in a clean format.\n",
        "\n",
        "Instead of calling the LLM separately for each step, we chain them together into one pipeline."
      ],
      "metadata": {
        "id": "7n4E_gAYXMyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 3.3 ‚Äî Sequential Chains Using Runnable Composition\n",
        "# ======================================\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Step 1: Prompt for generating an outline\n",
        "outline_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Create a short outline with 3 bullet points about {topic}.\"\n",
        ")\n",
        "\n",
        "# Step 2: Prompt for summarizing the outline\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"outline\"],\n",
        "    template=\"Summarize the following outline into two sentences:\\n\\n{outline}\"\n",
        ")\n",
        "\n",
        "# Step 3: Create the chain using dictionary mapping\n",
        "# ------------------------------------------------------\n",
        "# Explanation:\n",
        "# 1. First, we generate the outline ‚Üí outline_prompt | openai_llm\n",
        "# 2. Then, we pass that outline to summary_prompt | openai_llm\n",
        "# 3. RunnablePassthrough lets us carry variables forward without recomputing them.\n",
        "chain = (\n",
        "    {\"outline\": outline_prompt | openai_llm}  # Step 1: Generate outline\n",
        "    | summary_prompt                         # Step 2: Inject outline into summary prompt\n",
        "    | openai_llm                             # Step 3: Summarize\n",
        ")\n",
        "\n",
        "# Step 4: Run the chain\n",
        "topic = \"LangChain framework\"\n",
        "response = chain.invoke({\"topic\": topic})\n",
        "\n",
        "# Step 5: Show the result\n",
        "print(f\"üîπ Topic: {topic}\")\n",
        "print(\"\\nüîπ Final Summary:\\n\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3SrxQdKXN2L",
        "outputId": "783c9a24-938b-486e-baa5-7954108c787d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Topic: LangChain framework\n",
            "\n",
            "üîπ Final Summary:\n",
            " The LangChain framework is designed to simplify the integration of language models into various applications, featuring core components like chains, agents, and memory that facilitate complex workflows. It has practical applications in areas such as chatbots, content generation, and data analysis, enhancing productivity and creativity in leveraging language models for diverse tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 ‚Äî Advanced Multi-Step Reasoning Chain\n",
        "\n",
        "Description:\n",
        "This example demonstrates how to combine multiple prompts + LLMs into a single pipeline,\n",
        "where each step depends on the output of the previous one."
      ],
      "metadata": {
        "id": "konSnY3fY_jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 3.4 ‚Äî Advanced Multi-Step Reasoning Chain (FINAL FIX)\n",
        "# ======================================\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Step 1: Prompt for generating 3 creative ideas\n",
        "ideas_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=(\n",
        "        \"Generate 3 creative and unique ideas related to the topic: {topic}.\\n\"\n",
        "        \"Number each idea clearly.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Step 2: Prompt for choosing the best idea\n",
        "choose_best_prompt = PromptTemplate(\n",
        "    input_variables=[\"ideas\"],\n",
        "    template=(\n",
        "        \"You are a critical evaluator. Review the following 3 ideas:\\n\\n{ideas}\\n\\n\"\n",
        "        \"Select the SINGLE best idea and explain briefly why you chose it.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Step 3: Prompt for expanding the best idea into an article\n",
        "expand_prompt = PromptTemplate(\n",
        "    input_variables=[\"best_idea\"],\n",
        "    template=\"Write a short, engaging article (5-6 sentences) based on this idea:\\n\\n{best_idea}\"\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Step 4: Build the chain properly\n",
        "# ----------------------------\n",
        "# First: Generate ideas from topic\n",
        "generate_ideas_chain = ideas_prompt | openai_llm\n",
        "\n",
        "# Second: Choose the best idea from generated ideas\n",
        "choose_best_chain = (\n",
        "    {\"ideas\": generate_ideas_chain}           # Take output from first step\n",
        "    | choose_best_prompt\n",
        "    | openai_llm\n",
        ")\n",
        "\n",
        "# Third: Expand the chosen best idea into an article\n",
        "final_chain = (\n",
        "    {\"best_idea\": choose_best_chain}          # Pass the best idea forward\n",
        "    | expand_prompt\n",
        "    | openai_llm\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Step 5: Run the chain\n",
        "# ----------------------------\n",
        "topic = \"Applications of LangChain in real-world business\"\n",
        "response = final_chain.invoke({\"topic\": topic})\n",
        "\n",
        "# ----------------------------\n",
        "# Step 6: Display result\n",
        "# ----------------------------\n",
        "print(f\"üîπ Topic: {topic}\")\n",
        "print(\"\\nüîπ Final Article:\\n\")\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9pIltjqZAM2",
        "outputId": "69d2192e-11df-434d-bb09-8b828680ae70"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Topic: Applications of LangChain in real-world business\n",
            "\n",
            "üîπ Final Article:\n",
            "\n",
            "In today's fast-paced business landscape, **Dynamic Customer Support Chatbots** are revolutionizing the way companies interact with their customers. These advanced chatbots, powered by LangChain, offer personalized and context-aware responses that significantly enhance the customer experience. By integrating with CRM systems, they gain valuable insights into customer needs and preferences, allowing them to provide tailored solutions and escalate issues when necessary. This not only streamlines operations by alleviating the workload on human agents but also fosters more engaging and responsive interactions. As these chatbots learn from each interaction, they continuously adapt to evolving customer expectations, ensuring businesses remain competitive. Ultimately, the implementation of dynamic chatbots can lead to immediate and substantial improvements in customer service operations, driving satisfaction and retention.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4 ‚Äî Working with Documents\n",
        "\n",
        "Goal:\n",
        "Teach students how to load PDFs, split them into chunks, and prepare them for retrieval-augmented generation (RAG).\n",
        "This is the foundation for the next section where we‚Äôll connect everything into a full RAG pipeline."
      ],
      "metadata": {
        "id": "WA2qKh3UbAyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 ‚Äî Load and Preview a PDF\n",
        "\n",
        "Description:\n",
        "In this cell, we‚Äôll let students upload a PDF into Colab, load it using LangChain‚Äôs PyPDFLoader, and preview the first page."
      ],
      "metadata": {
        "id": "KzH_29Cma-42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 4.1 ‚Äî Load and Preview a PDF\n",
        "# ======================================\n",
        "\n",
        "from google.colab import files\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Step 1: Upload a PDF\n",
        "# ------------------------------------------------------\n",
        "# Students select a file from their computer.\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file path\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "print(f\"‚úÖ Uploaded PDF: {pdf_path}\")\n",
        "\n",
        "# Step 2: Load the PDF using LangChain's PyPDFLoader\n",
        "# ------------------------------------------------------\n",
        "# PyPDFLoader splits the PDF into LangChain Document objects.\n",
        "# - Each Document has .page_content (text) and .metadata (page info).\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "pages = loader.load()\n",
        "\n",
        "# Step 3: Preview the document\n",
        "print(f\"‚úÖ Total pages loaded: {len(pages)}\\n\")\n",
        "print(\"üîπ Preview of the first page:\\n\")\n",
        "print(pages[0].page_content[:800])  # Show first 800 characters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "vll0SJo6bD5L",
        "outputId": "d7c07078-5511-4ff1-ae0d-03f0e127e484"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e20d97b9-b1fb-4b40-b3c4-9787fe15bfb1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e20d97b9-b1fb-4b40-b3c4-9787fe15bfb1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf to 2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf\n",
            "‚úÖ Uploaded PDF: 2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf\n",
            "‚úÖ Total pages loaded: 65\n",
            "\n",
            "üîπ Preview of the first page:\n",
            "\n",
            "Prompt  \n",
            "Engineering\n",
            "Author: Lee Boonstra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 ‚Äî Split Documents into Chunks\n",
        "\n",
        "Description:\n",
        "In this cell, we‚Äôll use LangChain‚Äôs RecursiveCharacterTextSplitter to split the PDF into manageable chunks.\n",
        "Why we need chunking:\n",
        "\n",
        "Embeddings work best with shorter text.\n",
        "\n",
        "Smaller chunks improve recall during retrieval.\n",
        "\n",
        "Overlapping chunks preserve context continuity."
      ],
      "metadata": {
        "id": "2ofzEjn8bbgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 4.2 ‚Äî Split Documents into Chunks\n",
        "# ======================================\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Step 1: Create a text splitter\n",
        "# ------------------------------------------------------\n",
        "# Parameters:\n",
        "# - chunk_size: Maximum characters per chunk (adjust based on model context).\n",
        "# - chunk_overlap: Number of characters repeated between chunks to preserve context.\n",
        "# - separators: Order of preferred split points, largest to smallest.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,       # ~1-2 paragraphs per chunk\n",
        "    chunk_overlap=200,     # Preserve context across chunks\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Step 2: Split the PDF pages into chunks\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "# Step 3: Preview the result\n",
        "print(f\"‚úÖ Total chunks created: {len(chunks)}\")\n",
        "print(\"\\nüîπ First chunk preview:\\n\")\n",
        "print(chunks[0].page_content[:500])  # Show first 500 characters\n",
        "print(\"\\nüîπ Metadata example:\", chunks[0].metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgjbJEELbcyU",
        "outputId": "462dfc2b-66aa-4555-9810-7d4d886f2862"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Total chunks created: 103\n",
            "\n",
            "üîπ First chunk preview:\n",
            "\n",
            "Prompt  \n",
            "Engineering\n",
            "Author: Lee Boonstra\n",
            "\n",
            "üîπ Metadata example: {'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-10-31T13:52:57-06:00', 'moddate': '2024-10-31T13:53:02-06:00', 'trapped': '/False', 'source': '2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf', 'total_pages': 65, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 ‚Äî Generate Embeddings for Chunks\n",
        "\n",
        "Description:\n",
        "In this cell, we‚Äôll use OpenAI‚Äôs text-embedding-3-small model via LangChain‚Äôs OpenAIEmbeddings class.\n",
        "We generate embeddings for all chunks so we can later store them in a vector database."
      ],
      "metadata": {
        "id": "fqXVw9NKbCy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 4.3 ‚Äî Generate Embeddings for Chunks\n",
        "# ======================================\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Step 1: Initialize the embeddings model\n",
        "# ------------------------------------------------------\n",
        "# Parameters:\n",
        "# - model: The OpenAI embeddings model to use.\n",
        "#   Options:\n",
        "#     \"text-embedding-3-small\"  ‚Üí fast & cost-effective (~1,536 dimensions)\n",
        "#     \"text-embedding-3-large\"  ‚Üí more accurate (~3,072 dimensions, higher cost)\n",
        "# - api_key: Uses the environment variable configured earlier.\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "# Step 2: Generate embeddings for a sample query (sanity check)\n",
        "sample_vector = embedding_model.embed_query(\"What is LangChain?\")\n",
        "print(f\"‚úÖ Sample embedding vector length: {len(sample_vector)}\")\n",
        "print(f\"üîπ First 10 dimensions: {sample_vector[:10]}\")\n",
        "\n",
        "# Step 3: We'll use this embedding model later when storing chunks in a vector database\n",
        "print(\"\\n‚úÖ Embedding model initialized successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExOUKYopbn-U",
        "outputId": "4cec41cd-cec2-44e9-be26-b46b5b2ab03c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Sample embedding vector length: 1536\n",
            "üîπ First 10 dimensions: [0.003030850552022457, -0.0035995321813970804, -0.020205670967698097, 0.0034851604141294956, 0.0002958574041258544, -0.0026686734054237604, -0.000631030066870153, 0.019671935588121414, -0.03939470276236534, -0.01785469613969326]\n",
            "\n",
            "‚úÖ Embedding model initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 ‚Äî Store Chunks in Chroma Vector Database\n",
        "\n",
        "Description:\n",
        "In this cell, we‚Äôll use ChromaDB (a fast in-memory vector database) to store embeddings and their associated chunks.\n",
        "Once stored, we‚Äôll be able to query the database using semantic similarity search."
      ],
      "metadata": {
        "id": "xdUQY5ZSbv7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 4.4 ‚Äî Store Chunks in Chroma Vector Database\n",
        "# ======================================\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Step 1: Create and populate Chroma DB\n",
        "# ------------------------------------------------------\n",
        "# Parameters:\n",
        "# - documents: The chunks we generated earlier.\n",
        "# - embedding: The OpenAI embeddings model.\n",
        "# - persist_directory: If set to a folder path, Chroma will store data persistently.\n",
        "#   Here we keep it in-memory for simplicity.\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=None   # Set to \"./chroma_db\" if you want to persist locally\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Chroma vector database created successfully!\")\n",
        "\n",
        "# Step 2: Quick test: semantic similarity search\n",
        "# ------------------------------------------------------\n",
        "query = \"Explain the purpose of LangChain\"\n",
        "results = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "print(f\"\\nüîπ Query: {query}\\n\")\n",
        "print(\"üîπ Top 2 retrieved chunks:\\n\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"--- Chunk {i} ---\")\n",
        "    print(doc.page_content[:300], \"...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTNrqo6Tb4u2",
        "outputId": "d5853a8e-dab3-4744-b9fb-26d924df77a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Chroma vector database created successfully!\n",
            "\n",
            "üîπ Query: Explain the purpose of LangChain\n",
            "\n",
            "üîπ Top 3 retrieved chunks:\n",
            "\n",
            "--- Chunk 1 ---\n",
            "agent.run(prompt)\n",
            "Snippet 1. Creating a ReAct Agent with LangChain and VertexAI\n",
            "Code\tSnippet\t2\tshows\tthe\tresult.\tNotice\tthat\tReAct\tmakes\ta\tchain\tof\tfive\tsearches.\tIn\tfact,\t\n",
            "the\tLLM\tis\tscraping\tGoogle\tsearch\tresults\tto\tfigure\tout\tthe\tband\tnames.\tThen,\tit\tlists\tthe\t\n",
            "results\tas\tobservations\tand\tchains\t ...\n",
            "\n",
            "--- Chunk 2 ---\n",
            "agent.run(prompt)\n",
            "Snippet 1. Creating a ReAct Agent with LangChain and VertexAI\n",
            "Code\tSnippet\t2\tshows\tthe\tresult.\tNotice\tthat\tReAct\tmakes\ta\tchain\tof\tfive\tsearches.\tIn\tfact,\t\n",
            "the\tLLM\tis\tscraping\tGoogle\tsearch\tresults\tto\tfigure\tout\tthe\tband\tnames.\tThen,\tit\tlists\tthe\t\n",
            "results\tas\tobservations\tand\tchains\t ...\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Prompt Engineering\n",
            "September 2024\n",
            "29\n",
            "Chain of Thought (CoT)\n",
            "Chain of Thought (CoT) 9\tprompting\tis\ta\ttechnique\tfor\timproving\tthe\treasoning\tcapabilities\t\n",
            "of LLMs by generating intermediate reasoning steps.\tThis\thelps\tthe\tLLM\tgenerate\tmore\t\n",
            "accurate\tanswers.\tYou\tcan\tcombine\tit\twith\tfew-shot\tprompting\tt ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5 ‚Äî Working with Memory\n",
        "\n",
        "Goal\n",
        "Teach students how to add memory to LangChain applications so LLMs can remember previous user inputs and maintain conversational context."
      ],
      "metadata": {
        "id": "kLgUrgL_cUwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 ‚Äî Using LLMs Without Memory\n",
        "\n",
        "Description:\n",
        "By default, LLMs are stateless ‚Äî they don‚Äôt remember previous messages unless you explicitly resend the context every time."
      ],
      "metadata": {
        "id": "TR8AbXuXcaa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 5.1 ‚Äî Using LLMs Without Memory\n",
        "# ======================================\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Step 1: Create a simple LLM instance\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Step 2: Create a simple chat prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Step 3: Build a simple runnable chain (no memory)\n",
        "chain_no_memory = prompt | llm\n",
        "\n",
        "# Step 4: Ask two related questions\n",
        "q1 = \"My name is Alex.\"\n",
        "q2 = \"What is my name?\"\n",
        "\n",
        "resp1 = chain_no_memory.invoke({\"input\": q1})\n",
        "resp2 = chain_no_memory.invoke({\"input\": q2})\n",
        "\n",
        "print(\"=== Without Memory ===\\n\")\n",
        "print(f\"Q1: {q1}\\nA1: {resp1.content}\\n\")\n",
        "print(f\"Q2: {q2}\\nA2: {resp2.content}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daAb6tWEcW0E",
        "outputId": "58017d8d-5090-4837-df4e-3895c75cd85e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Without Memory ===\n",
            "\n",
            "Q1: My name is Alex.\n",
            "A1: Nice to meet you, Alex! How can I assist you today?\n",
            "\n",
            "Q2: What is my name?\n",
            "A2: I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 ‚Äî Using ConversationBufferMemory\n",
        "\n",
        "Description:\n",
        "Now we‚Äôll use ConversationBufferMemory so the LLM remembers the full conversation history."
      ],
      "metadata": {
        "id": "alLkyFgYcuDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 5.2 ‚Äî Using ConversationBufferMemory\n",
        "# ======================================\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Step 1: Create a prompt that EXPLICITLY shows chat history\n",
        "\n",
        "\n",
        "\n",
        "# Updated prompt with explicit chat_history variable\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"ai\", \"This is the conversation so far:\\n{chat_history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "\n",
        "# Step 1: Create memory object\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",   # Inject history into {chat_history} variable\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# Step 2: Create LLMChain with memory\n",
        "chain_with_memory = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Step 3: Ask two related questions\n",
        "q1 = \"My name is Alex.\"\n",
        "q2 = \"What is my name?\"\n",
        "\n",
        "resp1_mem = chain_with_memory.invoke({\"input\": q1})\n",
        "resp2_mem = chain_with_memory.invoke({\"input\": q2})\n",
        "\n",
        "print(\"=== With Memory (Updated Prompt) ===\\n\")\n",
        "print(f\"Q1: {q1}\\nA1: {resp1_mem['text']}\\n\")\n",
        "print(f\"Q2: {q2}\\nA2: {resp2_mem['text']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QQ6lErYcw-T",
        "outputId": "8642c655-2fa8-472b-9a7d-b3924952f439"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2131633540.py:22: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n",
            "/tmp/ipython-input-2131633540.py:28: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain_with_memory = LLMChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== With Memory (Updated Prompt) ===\n",
            "\n",
            "Q1: My name is Alex.\n",
            "A1: Nice to meet you, Alex! How can I assist you today?\n",
            "\n",
            "Q2: What is my name?\n",
            "A2: Your name is Alex.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 ‚Äî Inspecting Conversation History\n",
        "\n",
        "Description:\n",
        "In this cell, we‚Äôll ask the LLM multiple questions and then print the stored chat history from ConversationBufferMemory.\n",
        "This helps students understand what LangChain remembers and how it injects the history into the prompt."
      ],
      "metadata": {
        "id": "7_U1Z02ydT__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 5.3 ‚Äî Inspecting Conversation History (Updated)\n",
        "# ======================================\n",
        "\n",
        "# Step 1: Ask multiple related questions\n",
        "q1 = \"I live in Sofia.\"\n",
        "q2 = \"My favorite programming language is Python.\"\n",
        "q3 = \"Where do I live and what is my favorite language?\"\n",
        "\n",
        "resp1 = chain_with_memory.invoke({\"input\": q1})\n",
        "resp2 = chain_with_memory.invoke({\"input\": q2})\n",
        "resp3 = chain_with_memory.invoke({\"input\": q3})\n",
        "\n",
        "# Step 2: Show responses\n",
        "print(\"=== Responses ===\\n\")\n",
        "print(f\"Q1: {q1}\\nA1: {resp1['text']}\\n\")\n",
        "print(f\"Q2: {q2}\\nA2: {resp2['text']}\\n\")\n",
        "print(f\"Q3: {q3}\\nA3: {resp3['text']}\\n\")\n",
        "\n",
        "# Step 3: Inspect stored chat history\n",
        "print(\"=== Stored Conversation History ===\\n\")\n",
        "for msg in memory.chat_memory.messages:\n",
        "    role = \"User\" if msg.type == \"human\" else \"Assistant\"\n",
        "    print(f\"{role}: {msg.content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79RFv9jXdUzz",
        "outputId": "b18e7afd-d112-47b8-b411-8400bbac3980"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Responses ===\n",
            "\n",
            "Q1: I live in Sofia.\n",
            "A1: That's great, Alex! Sofia is a beautiful city with a rich history and culture. What do you enjoy most about living there?\n",
            "\n",
            "Q2: My favorite programming language is Python.\n",
            "A2: Python is a fantastic choice! It's known for its readability and versatility. What do you enjoy most about using Python?\n",
            "\n",
            "Q3: Where do I live and what is my favorite language?\n",
            "A3: You live in Sofia, and your favorite programming language is Python.\n",
            "\n",
            "=== Stored Conversation History ===\n",
            "\n",
            "User: My name is Alex.\n",
            "Assistant: Nice to meet you, Alex! How can I assist you today?\n",
            "User: What is my name?\n",
            "Assistant: Your name is Alex.\n",
            "User: I live in Sofia.\n",
            "Assistant: That's great, Alex! Sofia is a beautiful city with a rich history and culture. What do you enjoy most about living there?\n",
            "User: My favorite programming language is Python.\n",
            "Assistant: Python is a fantastic choice! It's known for its readability and versatility. What do you enjoy most about using Python?\n",
            "User: Where do I live and what is my favorite language?\n",
            "Assistant: You live in Sofia, and your favorite programming language is Python.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 ‚Äî Using ConversationBufferWindowMemory\n",
        "\n",
        "Description:\n",
        "By default, ConversationBufferMemory stores the entire chat history, which can become expensive for long conversations.\n",
        "ConversationBufferWindowMemory solves this by keeping only the last N message pairs."
      ],
      "metadata": {
        "id": "J734MslVefeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 5.4 ‚Äî Using ConversationBufferWindowMemory (Updated)\n",
        "# ======================================\n",
        "\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Step 1: Create a prompt that EXPLICITLY shows chat history\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"system\", \"Only the remembered conversation is shown here:\\n{chat_history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Step 2: Create a windowed memory object (keep last 2 message pairs)\n",
        "window_memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    k=2,                    # Keep only last 2 message pairs\n",
        "    return_messages=True    # Store structured messages instead of plain text\n",
        ")\n",
        "\n",
        "# Step 3: Create an LLM chain with windowed memory\n",
        "chain_with_window_memory = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=window_memory\n",
        ")\n",
        "\n",
        "# Step 4: Ask multiple questions\n",
        "q1 = \"My name is Alex.\"\n",
        "q2 = \"I live in Sofia.\"\n",
        "q3 = \"I work at Ethera Technologies.\"\n",
        "q4 = \"What is my name?\"\n",
        "\n",
        "resp1 = chain_with_window_memory.invoke({\"input\": q1})\n",
        "resp2 = chain_with_window_memory.invoke({\"input\": q2})\n",
        "resp3 = chain_with_window_memory.invoke({\"input\": q3})\n",
        "resp4 = chain_with_window_memory.invoke({\"input\": q4})\n",
        "\n",
        "# Step 5: Print assistant responses\n",
        "print(\"=== Responses ===\\n\")\n",
        "print(f\"Q1: {q1}\\nA1: {resp1['text']}\\n\")\n",
        "print(f\"Q2: {q2}\\nA2: {resp2['text']}\\n\")\n",
        "print(f\"Q3: {q3}\\nA3: {resp3['text']}\\n\")\n",
        "print(f\"Q4: {q4}\\nA4: {resp4['text']}\\n\")\n",
        "\n",
        "# Step 6: Inspect what is ACTUALLY stored in memory\n",
        "print(\"=== RAW MEMORY STATE (Last 2 Message Pairs) ===\\n\")\n",
        "for msg in window_memory.chat_memory.messages:\n",
        "    role = \"User\" if msg.type == \"human\" else \"Assistant\"\n",
        "    print(f\"{role}: {msg.content}\")\n",
        "\n",
        "# Step 7: Show the chat history variable injected into the prompt\n",
        "print(\"\\n=== CHAT HISTORY IN PROMPT ===\\n\")\n",
        "chat_history_messages = window_memory.load_memory_variables({})[\"chat_history\"]\n",
        "for msg in chat_history_messages:\n",
        "    role = \"User\" if msg.type == \"human\" else \"Assistant\"\n",
        "    print(f\"{role}: {msg.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAPwh6xoegf7",
        "outputId": "d99e2f88-ab0d-46aa-b3ec-ae2344c84b38"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Responses ===\n",
            "\n",
            "Q1: My name is Alex.\n",
            "A1: Nice to meet you, Alex! How can I assist you today?\n",
            "\n",
            "Q2: I live in Sofia.\n",
            "A2: That's great, Alex! Sofia is a beautiful city with a rich history. What do you enjoy most about living there?\n",
            "\n",
            "Q3: I work at Ethera Technologies.\n",
            "A3: That sounds interesting, Alex! What do you do at Ethera Technologies?\n",
            "\n",
            "Q4: What is my name?\n",
            "A4: Your name is Alex.\n",
            "\n",
            "=== RAW MEMORY STATE (Last 2 Message Pairs) ===\n",
            "\n",
            "User: My name is Alex.\n",
            "Assistant: Nice to meet you, Alex! How can I assist you today?\n",
            "User: I live in Sofia.\n",
            "Assistant: That's great, Alex! Sofia is a beautiful city with a rich history. What do you enjoy most about living there?\n",
            "User: I work at Ethera Technologies.\n",
            "Assistant: That sounds interesting, Alex! What do you do at Ethera Technologies?\n",
            "User: What is my name?\n",
            "Assistant: Your name is Alex.\n",
            "\n",
            "=== CHAT HISTORY IN PROMPT ===\n",
            "\n",
            "User: I work at Ethera Technologies.\n",
            "Assistant: That sounds interesting, Alex! What do you do at Ethera Technologies?\n",
            "User: What is my name?\n",
            "Assistant: Your name is Alex.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6 ‚Äî Advanced Memory Types\n",
        "\n",
        "In this section, we‚Äôll demonstrate how LangChain offers multiple memory strategies depending on:\n",
        "\n",
        "Chat length\n",
        "\n",
        "Token cost\n",
        "\n",
        "Desired behavior (remember everything vs. summarize vs. extract facts)"
      ],
      "metadata": {
        "id": "Tu5MGohTfqBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 6.1 ‚Äî Using ConversationSummaryMemory\n",
        "\n",
        "Goal:\n",
        "Show students how LangChain can summarize previous conversations automatically\n",
        "instead of storing the entire history.\n",
        "\n",
        "This is useful when:\n",
        "\n",
        "Conversations are very long.\n",
        "\n",
        "You want the model to have compressed context."
      ],
      "metadata": {
        "id": "ky1uaD-OftHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 6.1 ‚Äî Using ConversationSummaryMemory\n",
        "# ======================================\n",
        "\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Step 1: Create the prompt with explicit {chat_history}\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"system\", \"Here is the summary of the conversation so far:\\n{chat_history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "\n",
        "summary_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_completion_tokens=50)\n",
        "\n",
        "# Step 2: Create ConversationSummaryMemory\n",
        "# ------------------------------------------------------\n",
        "# Instead of storing full messages, it generates a summary using the LLM.\n",
        "summary_memory = ConversationSummaryMemory(\n",
        "    llm=llm,                   # Uses the same LLM for summarization\n",
        "    memory_key=\"chat_history\", # Injected into the prompt\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# Step 3: Create a chain using summary memory\n",
        "summary_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=summary_memory\n",
        ")\n",
        "\n",
        "# Step 4: Ask multiple questions to demonstrate automatic summarization\n",
        "q1 = \"My name is Alex, and I live in Sofia.\"\n",
        "q2 = \"I work at Ethera Technologies as an AI consultant.\"\n",
        "q3 = \"Can you summarize my personal details?\"\n",
        "\n",
        "resp1 = summary_chain.invoke({\"input\": q1})\n",
        "resp2 = summary_chain.invoke({\"input\": q2})\n",
        "resp3 = summary_chain.invoke({\"input\": q3})\n",
        "\n",
        "# Step 5: Print responses\n",
        "print(\"=== Responses ===\\n\")\n",
        "print(f\"Q1: {q1}\\nA1: {resp1['text']}\\n\")\n",
        "print(f\"Q2: {q2}\\nA2: {resp2['text']}\\n\")\n",
        "print(f\"Q3: {q3}\\nA3: {resp3['text']}\\n\")\n",
        "\n",
        "# Step 6: Inspect the stored memory summary\n",
        "print(\"\\n=== STORED MEMORY (Summary) ===\\n\")\n",
        "print(summary_memory.load_memory_variables({})[\"chat_history\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0_lDthBfqzY",
        "outputId": "4d8a8366-0a7c-4400-bd1c-894a314b211c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Responses ===\n",
            "\n",
            "Q1: My name is Alex, and I live in Sofia.\n",
            "A1: Nice to meet you, Alex! How can I assist you today?\n",
            "\n",
            "Q2: I work at Ethera Technologies as an AI consultant.\n",
            "A2: That's great to hear, Alex! Working as an AI consultant must be quite interesting, especially with the rapid advancements in the field. What kind of projects or areas do you focus on at Ethera Technologies?\n",
            "\n",
            "Q3: Can you summarize my personal details?\n",
            "A3: Sure! Here are the personal details you've shared so far:\n",
            "\n",
            "- Your name is Alex.\n",
            "- You live in Sofia.\n",
            "- You work at Ethera Technologies as an AI consultant.\n",
            "\n",
            "If there's anything else you'd like to add or modify, just let me know!\n",
            "\n",
            "\n",
            "=== STORED MEMORY (Summary) ===\n",
            "\n",
            "[SystemMessage(content=\"The human introduces themselves as Alex and mentions they live in Sofia. The AI expresses pleasure in meeting Alex and asks how it can assist them today. Alex shares that they work at Ethera Technologies as an AI consultant. The AI then summarizes Alex's personal\", additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 ‚Äî Using ConversationKGMemory (Entity & Fact Memory)\n",
        "\n",
        "Description:\n",
        "ConversationKGMemory extracts structured facts (subject‚Äìpredicate‚Äìobject triplets) from the dialog (e.g., ‚ÄúAlex ‚Äî lives_in ‚Äî Sofia‚Äù).\n",
        "This is useful when you want the assistant to remember stable facts about a user or topic without carrying the entire chat text."
      ],
      "metadata": {
        "id": "R24FZ--4iOtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 6.2 ‚Äî Using ConversationKGMemory (Fixed)\n",
        "# ======================================\n",
        "\n",
        "# 1) Use the community package (recommended in recent LangChain versions)\n",
        "from langchain_community.memory.kg import ConversationKGMemory\n",
        "from langchain_community.graphs.networkx_graph import NetworkxEntityGraph  # for direct triples insight\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# 2) Prompt uses the *same* memory_key we give to KG memory below (\"history\")\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"system\", \"Known facts extracted so far:\\n{history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# 3) KG memory: align keys with LLMChain (input='input', output='text'); keep a few turns in view (k)\n",
        "kg_memory = ConversationKGMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"history\",   # <- matches prompt\n",
        "    input_key=\"input\",      # <- LLMChain input dict will have {\"input\": \"...\"}\n",
        "    output_key=\"text\",      # <- LLMChain default output key is \"text\"\n",
        "    return_messages=False,  # you can set True if you want message objects instead of string\n",
        "    k=4                     # consider last 4 utterances (2 pairs) when extracting\n",
        ")\n",
        "\n",
        "kg_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=kg_memory,\n",
        "    output_key=\"text\"       # explicit to match memory.output_key\n",
        ")\n",
        "\n",
        "# 4) Mini-dialog populating facts\n",
        "u1 = \"My name is Alex.\"\n",
        "u2 = \"I live in Sofia, Bulgaria.\"\n",
        "u3 = \"I work at Ethera Technologies as an AI consultant.\"\n",
        "u4 = \"What do you know about me so far?\"\n",
        "\n",
        "r1 = kg_chain.invoke({\"input\": u1})\n",
        "r2 = kg_chain.invoke({\"input\": u2})\n",
        "r3 = kg_chain.invoke({\"input\": u3})\n",
        "r4 = kg_chain.invoke({\"input\": u4})\n",
        "\n",
        "print(\"=== Responses ===\\n\")\n",
        "print(f\"U1: {u1}\\nA1: {r1['text']}\\n\")\n",
        "print(f\"U2: {u2}\\nA2: {r2['text']}\\n\")\n",
        "print(f\"U3: {u3}\\nA3: {r3['text']}\\n\")\n",
        "print(f\"U4: {u4}\\nA4: {r4['text']}\\n\")\n",
        "\n",
        "# 5) Inspect the raw triples directly from the graph (bypasses the entity filter)\n",
        "print(\"=== RAW TRIPLES IN KG ===\")\n",
        "print(kg_memory.kg.get_triples())  # list of (subject, relation, object)\n",
        "# (If this prints [], the extractor didn't parse your inputs; try rephrasing u1‚Äìu3 slightly.)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTKB-DcLiPQh",
        "outputId": "298db15a-fc0e-4a2f-fa64-f7c893f3d99c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Responses ===\n",
            "\n",
            "U1: My name is Alex.\n",
            "A1: Nice to meet you, Alex! How can I assist you today?\n",
            "\n",
            "U2: I live in Sofia, Bulgaria.\n",
            "A2: That's great! Sofia is a beautiful city with a rich history and vibrant culture. If you have any questions about Sofia or need information about anything specific, feel free to ask!\n",
            "\n",
            "U3: I work at Ethera Technologies as an AI consultant.\n",
            "A3: That's great! As an AI consultant at Ethera Technologies, you likely work on various projects involving artificial intelligence, machine learning, and data analysis. If you have any specific questions or topics you'd like to discuss related to your work or AI in general, feel free to ask!\n",
            "\n",
            "U4: What do you know about me so far?\n",
            "A4: I don't have any information about you. I don't have access to personal data unless you share it with me during our conversation. How can I assist you today?\n",
            "\n",
            "=== RAW TRIPLES IN KG ===\n",
            "[('Nevada', 'state', 'is a'), ('Sofia', 'Bulgaria', 'is in'), ('Ethera Technologies', 'company', 'is a'), ('Alex', 'Ethera Technologies', 'works at'), ('Alex', 'AI consultant', 'is an')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7 ‚Äî LangChain Agents\n",
        "Goal\n",
        "Teach how LangChain agents work, what tools are, and how to build a simple example where an agent decides what to do step by step."
      ],
      "metadata": {
        "id": "CbjVwsH1pE2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 7.1 ‚Äî Basic Agent with Built-in Tools\n",
        "\n",
        "Description:\n",
        "In this section, we introduce LangChain agents and demonstrate how to create a simple agent that can reason, choose between multiple tools, and combine their outputs to answer complex questions.\n",
        "Students will learn:\n",
        "\n",
        "How to initialize an agent with an LLM.\n",
        "\n",
        "How to register built-in tools like Wikipedia search and Calculator.\n",
        "\n",
        "How the agent decides which tool to use based on the tool descriptions.\n",
        "\n",
        "How the agent combines results from multiple tools to provide a final answer.\n",
        "\n",
        "The code cell I shared above stays the same ‚Äî we only add this description at the start of the section."
      ],
      "metadata": {
        "id": "8ZQvdYdwpson"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U wikipedia\n",
        "!pip install -U langchain-experimental"
      ],
      "metadata": {
        "id": "NpVaOx_Xqc9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# Section 7.1 ‚Äî Using Custom Tools (Function Calling)\n",
        "# ======================================\n",
        "\n",
        "from langchain.tools import StructuredTool\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableMap\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# -------------------------\n",
        "# Step 1: Custom Function\n",
        "# -------------------------\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Simulated weather info provider.\"\"\"\n",
        "    fake_data = {\n",
        "        \"Sofia\": \"Sunny, 28¬∞C\",\n",
        "        \"Plovdiv\": \"Partly cloudy, 25¬∞C\",\n",
        "        \"Varna\": \"Windy, 22¬∞C\"\n",
        "    }\n",
        "    return fake_data.get(city, f\"Weather data for {city} is not available.\")\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Wrap the Function as a LangChain Tool\n",
        "# -------------------------\n",
        "weather_tool = StructuredTool.from_function(\n",
        "    func=get_weather,\n",
        "    name=\"WeatherTool\",\n",
        "    description=\"Use this tool to get the current weather for a specific city.\"\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Step 3: Prompt to Extract City Name\n",
        "# -------------------------\n",
        "city_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Extract ONLY the city name from the user's message.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# Step 4: Create the Routing Function\n",
        "# -------------------------\n",
        "def route_question(inputs: dict):\n",
        "    \"\"\"\n",
        "    Decides whether to call the weather tool or the LLM.\n",
        "\n",
        "    Parameters:\n",
        "        inputs (dict):\n",
        "            - \"input\" (str): The user question.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with {\"response\": <str>} containing the assistant's reply.\n",
        "    \"\"\"\n",
        "    query = inputs[\"input\"].lower()\n",
        "\n",
        "    # If the user asks about weather ‚Üí extract the city ‚Üí call tool\n",
        "    if \"weather\" in query:\n",
        "        city = llm.predict(city_prompt.format(input=inputs[\"input\"])).strip()\n",
        "        return {\"response\": weather_tool.func(city)}\n",
        "\n",
        "    # Otherwise ‚Üí just answer normally with LLM\n",
        "    return {\"response\": llm.predict(inputs[\"input\"])}\n",
        "\n",
        "# -------------------------\n",
        "# Step 5: Build the Chain\n",
        "# -------------------------\n",
        "chain = RunnableMap({\n",
        "    \"input\": RunnablePassthrough(),\n",
        "    \"response\": route_question\n",
        "})\n",
        "\n",
        "# -------------------------\n",
        "# Step 6: Test It\n",
        "# -------------------------\n",
        "questions = [\n",
        "    \"What's the weather in Sofia?\",\n",
        "    \"What's the weather in Varna?\",\n",
        "    \"Who founded OpenAI?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    # ‚úÖ Now we pass a dict to chain.invoke()\n",
        "    result = chain.invoke({\"input\": q})\n",
        "    print(f\"\\nUser: {q}\\nAssistant: {result['response']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQMLzk0dnAqw",
        "outputId": "14277426-028d-491a-a840-ca72b0b89a1c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1272579295.py:57: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  city = llm.predict(city_prompt.format(input=inputs[\"input\"])).strip()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: What's the weather in Sofia?\n",
            "Assistant: {'response': 'Sunny, 28¬∞C'}\n",
            "\n",
            "User: What's the weather in Varna?\n",
            "Assistant: {'response': 'Windy, 22¬∞C'}\n",
            "\n",
            "User: Who founded OpenAI?\n",
            "Assistant: {'response': 'OpenAI was founded in December 2015 by Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, Wojciech Zaremba, and John Schulman, among others. The organization was established with the goal of advancing artificial intelligence in a way that is safe and beneficial for humanity.'}\n"
          ]
        }
      ]
    }
  ]
}