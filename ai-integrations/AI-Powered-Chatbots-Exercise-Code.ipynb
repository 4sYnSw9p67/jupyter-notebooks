{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Fetch the API key from Colab's secrets\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = openai.OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXss908DQTny",
        "outputId": "b35cc217-8185-40af-e8e8-7c48448b8c8c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.99.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Upload the PDF"
      ],
      "metadata": {
        "id": "ZukK0OgbMgOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "r_FHoD5mFIcq",
        "outputId": "661976aa-9451-4b0f-c0ac-d5743c3159d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b43a7208-50a2-40cc-ae91-dff11fcf6bd2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b43a7208-50a2-40cc-ae91-dff11fcf6bd2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf to 2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Extract the text from the pdf"
      ],
      "metadata": {
        "id": "1KiFqJLcMztJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b94559e",
        "outputId": "82f4f851-219f-4dd9-c058-966a0a438d97"
      },
      "source": [
        "!pip install pdfminer.six"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20250506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Assuming the uploaded PDF is the first (and only) file in the 'uploaded' dictionary\n",
        "pdf_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Extract text from the PDF\n",
        "raw_text = extract_text(pdf_filename)\n",
        "\n",
        "# Display the first 500 characters of the extracted text\n",
        "print(raw_text[:500])"
      ],
      "metadata": {
        "id": "oNUgzBtNM3gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Chunk the text"
      ],
      "metadata": {
        "id": "jKV5Vz2SNT2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Hamalski way"
      ],
      "metadata": {
        "id": "anhYP6jOPH7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size, overlap):\n",
        "    \"\"\"Chunks text into smaller pieces with overlap.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "        chunk_size (int): The desired size of each chunk.\n",
        "        overlap (int): The number of characters to overlap between chunks.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "# Example usage (replace with your desired chunk_size and overlap)\n",
        "chunk_size = 500 # 400 text # 100 next chunk\n",
        "overlap = 100\n",
        "text_chunks = chunk_text(raw_text, chunk_size, overlap)\n",
        "print(f\"Created {len(text_chunks)} chunks.\")\n",
        "print(\"First chunk:\", text_chunks[0][:200]) # Print first 200 characters of the first chunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uppcSmEsNX-w",
        "outputId": "832f7e5b-7bc0-42b7-dbe9-991fccf778e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 203 chunks.\n",
            "First chunk: Prompt  \n",
            "Engineering\n",
            "\n",
            "Author: Lee Boonstra\n",
            "\n",
            "\fAcknowledgements\n",
            "\n",
            "Reviewers and Contributors\n",
            "\n",
            "Michael Sherman\n",
            "\n",
            "Yuan Cao\n",
            "\n",
            "Erick Armbrust\n",
            "\n",
            "Anant Nawalgaria\n",
            "\n",
            "Antonio Gulli\n",
            "\n",
            "Simone Cammel\n",
            "\n",
            "Curators and Edito\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Puctual Chunking"
      ],
      "metadata": {
        "id": "WJAiwl_VPMIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def punctual_chunking(text, chunk_size=500, overlap=100):\n",
        "    \"\"\"Chunks text into smaller pieces based on punctuation with overlap.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "        chunk_size (int): The approximate desired size of each chunk.\n",
        "        overlap (int): The number of characters to overlap between chunks.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text chunks.\n",
        "    \"\"\"\n",
        "    # Split the text by punctuation that typically ends sentences or paragraphs\n",
        "    split_points = r'(?<=[.!?;\\n])\\s*' # Split after punctuation followed by optional whitespace\n",
        "    sentences = re.split(split_points, text)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    i = 0\n",
        "    while i < len(sentences):\n",
        "        sentence = sentences[i]\n",
        "\n",
        "        if len(current_chunk) + len(sentence) < chunk_size:\n",
        "            current_chunk += sentence\n",
        "            i += 1\n",
        "        else:\n",
        "            if current_chunk: # Add the current chunk if it's not empty\n",
        "                chunks.append(current_chunk.strip())\n",
        "                # Start the next chunk with overlap\n",
        "                overlap_start = max(0, len(current_chunk) - overlap)\n",
        "                current_chunk = current_chunk[overlap_start:] + sentence\n",
        "                i += 1\n",
        "            else: # If current_chunk is empty, the sentence is larger than chunk_size\n",
        "                 # In this case, just add the sentence as a chunk (or part of it)\n",
        "                 chunks.append(sentence[:chunk_size].strip())\n",
        "                 sentences[i] = sentence[chunk_size:] # Keep the rest of the sentence for the next iteration\n",
        "                 if not sentences[i]: # If the rest is empty, move to the next sentence\n",
        "                     i += 1\n",
        "\n",
        "\n",
        "    # Add the last chunk if it's not empty\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Example usage (replace raw_text with your variable containing the extracted text)\n",
        "punctual_chunks = punctual_chunking(raw_text, chunk_size=500, overlap=100)\n",
        "print(f\"Created {len(punctual_chunks)} chunks using punctual chunking.\")\n",
        "print(\"First chunk:\", punctual_chunks[0][:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2hrBx1dPHSj",
        "outputId": "8b1b55bd-483e-4a9f-f4b7-a1e15767e6d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 213 chunks using punctual chunking.\n",
            "First chunk: Prompt  \n",
            "Engineering\n",
            "Author: Lee Boonstra\n",
            "Acknowledgements\n",
            "Reviewers and Contributors\n",
            "Michael Sherman\n",
            "Yuan Cao\n",
            "Erick Armbrust\n",
            "Anant Nawalgaria\n",
            "Antonio Gulli\n",
            "Simone Cammel\n",
            "Curators and Editors\n",
            "Antonio \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. AI Chunking"
      ],
      "metadata": {
        "id": "Dta6ukZdQJby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ai_chunking(text, client, model=\"gpt-4o-mini\", max_input_chars=5000):\n",
        "    \"\"\"Chunks text using an AI model to preserve context.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text (up to max_input_chars).\n",
        "        client: The initialized OpenAI client.\n",
        "        model (str): The OpenAI model to use for chunking.\n",
        "        max_input_chars (int): The maximum number of characters to send to the model.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text chunks generated by the AI.\n",
        "    \"\"\"\n",
        "    if len(text) > max_input_chars:\n",
        "        text = text[:max_input_chars]\n",
        "        print(f\"Warning: Input text truncated to {max_input_chars} characters for AI processing.\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that chunks text while preserving context. Return the chunks as a Python list of strings in JSON format. The chunks will be used for RAG. Kepp in mind that overlap is also needed\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Please chunk the following text into meaningful sections, ensuring context is preserved. Provide the output as a JSON object with a key 'chunks' containing a Python list of strings:\\n\\n{text}\"}\n",
        "            ],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        # Assuming the model returns a JSON object with a key like \"chunks\" containing the list\n",
        "        # You might need to inspect the model's output format and adjust accordingly.\n",
        "        ai_generated_chunks = json.loads(response.choices[0].message.content).get(\"chunks\", [])\n",
        "        return ai_generated_chunks\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during AI chunking: {e}\")\n",
        "        return []\n",
        "\n",
        "import json\n",
        "\n",
        "# Example usage (assuming 'client' is your initialized OpenAI client and 'raw_text' is your extracted text)\n",
        "# Note: Sending a large amount of text to the API will incur costs.\n",
        "ai_chunks = ai_chunking(raw_text, client, max_input_chars=5000)\n",
        "print(f\"Created {len(ai_chunks)} chunks using AI chunking.\")\n",
        "if ai_chunks:\n",
        "  print(\"First AI chunk:\", ai_chunks[0][:200])\n",
        "\n",
        "# You can then combine these chunks with the other chunks if needed:\n",
        "# all_chunks = text_chunks + punctual_chunks + ai_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AjW0Q3wQ1Qj",
        "outputId": "aed41163-c102-432a-a931-c87eb134bff3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Input text truncated to 5000 characters for AI processing.\n",
            "Created 12 chunks using AI chunking.\n",
            "First AI chunk: Prompt Engineering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of chunks to print from the user\n",
        "num_chunks_to_print = int(input(\"Enter the number of chunks to print: \"))\n",
        "\n",
        "# Print the first n chunks\n",
        "print(f\"\\nFirst {num_chunks_to_print} AI chunks:\")\n",
        "for i, chunk in enumerate(ai_chunks[:num_chunks_to_print]):\n",
        "    print(f\"Chunk {i+1}:\\n{chunk}\\n---\")"
      ],
      "metadata": {
        "id": "7lw8J_kPSwnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Gerate embeddings"
      ],
      "metadata": {
        "id": "sHbSttl0TsX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import time\n",
        "\n",
        "def generate_embeddings(chunks, client, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"Generates embeddings for a list of text chunks using OpenAI.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): A list of text chunks (strings).\n",
        "        client: The initialized OpenAI client.\n",
        "        model (str): The OpenAI embedding model to use.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping each chunk to its embedding vector,\n",
        "              or None if an error occurs.\n",
        "    \"\"\"\n",
        "    chunk_embeddings = {}\n",
        "    try:\n",
        "        # OpenAI's embeddings endpoint can take a list of inputs\n",
        "        response = client.embeddings.create(\n",
        "            input=chunks,\n",
        "            model=model\n",
        "        )\n",
        "        # Assuming the response structure contains 'data' which is a list of embedding objects\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if i < len(response.data):\n",
        "                chunk_embeddings[chunk] = response.data[i].embedding\n",
        "            else:\n",
        "                print(f\"Warning: No embedding returned for chunk {i+1}.\")\n",
        "\n",
        "        return chunk_embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during embedding generation: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage (assuming 'client' is your initialized OpenAI client and 'punctual_chunks' is your list of chunks)\n",
        "# Note: Generating embeddings will incur costs.\n",
        "embeddings = generate_embeddings(punctual_chunks, client)\n",
        "\n",
        "if embeddings:\n",
        "   print(f\"Generated embeddings for {len(embeddings)} chunks.\")\n",
        "   # Example of accessing an embedding:\n",
        "   first_chunk = list(embeddings.keys())[0]\n",
        "   print(\"\\nFirst Chunk:\")\n",
        "   print(first_chunk)\n",
        "   print(\"\\nEmbedding for the first chunk (first 10 elements):\")\n",
        "   print(embeddings[first_chunk][:10]) # Print first 10 elements of the embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdiwsOdpT15z",
        "outputId": "4500aa14-9947-4235-9945-586ce0f06692"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings for 213 chunks.\n",
            "\n",
            "First Chunk:\n",
            "Prompt  \n",
            "Engineering\n",
            "Author: Lee Boonstra\n",
            "Acknowledgements\n",
            "Reviewers and Contributors\n",
            "Michael Sherman\n",
            "Yuan Cao\n",
            "Erick Armbrust\n",
            "Anant Nawalgaria\n",
            "Antonio Gulli\n",
            "Simone Cammel\n",
            "Curators and Editors\n",
            "Antonio Gulli\n",
            "Anant Nawalgaria\n",
            "Grace Mollison \n",
            "Technical Writer\n",
            "Joey Haymaker\n",
            "Designer\n",
            "Michael Lanning \n",
            "2\n",
            "Prompt EngineeringSeptember 2024\fTable of contents\n",
            "Introduction \n",
            "Prompt engineering \n",
            "LLM output configuration \n",
            "Output length \n",
            "Sampling controls \n",
            "Temperature \n",
            "Top-K and top-P \n",
            "Putting\tit\tall\ttogether\n",
            "\n",
            "Embedding for the first chunk (first 10 elements):\n",
            "[0.014929180964827538, 0.007073588203638792, -0.009895914234220982, -0.023090466856956482, -0.00027459030388854444, 0.02422792837023735, -0.010251371189951897, -0.005331850610673428, -0.04689184948801994, -0.03480631858110428]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Store the embeddings in Chroma DB"
      ],
      "metadata": {
        "id": "siDt_dqfZg3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Set up Chroma DB"
      ],
      "metadata": {
        "id": "iEcxNp1TZunj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "nHe7UwPycDVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "# Create a Chroma DB client\n",
        "client_db = chromadb.Client()\n",
        "\n",
        "# Create a collection (or get an existing one)\n",
        "collection = client_db.get_or_create_collection(name=\"my_document_embeddings\")\n",
        "\n",
        "print(f\"Chroma DB client created and collection '{collection.name}' is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doUfx-CPZxvG",
        "outputId": "10beec5b-c1ce-4dd9-e572-5814328d84f4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chroma DB client created and collection 'my_document_embeddings' is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Store the embeddings and the chunks"
      ],
      "metadata": {
        "id": "oeZpAo7YaMmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store embeddings in Chroma DB\n",
        "\n",
        "# Prepare data for Chroma DB\n",
        "ids = [f\"chunk_{i}\" for i in range(len(embeddings))]\n",
        "documents = list(embeddings.keys())\n",
        "embedding_vectors = list(embeddings.values())\n",
        "\n",
        "# Add to the collection\n",
        "collection.add(\n",
        "    embeddings=embedding_vectors,\n",
        "    documents=documents,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(f\"Added {len(documents)} documents and embeddings to the collection.\")\n",
        "print(f\"Collection count: {collection.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-wez9RyaQQu",
        "outputId": "ce5a883f-66c4-42a7-c643-5958a7636abc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 213 documents and embeddings to the collection.\n",
            "Collection count: 213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37f2f246",
        "outputId": "26f63bae-d161-45bf-b0ae-4e9d5c14a903"
      },
      "source": [
        "# Retrieve the first 2 items from the collection\n",
        "retrieved_items = collection.get(\n",
        "    ids=[f\"chunk_{i}\" for i in range(2)], # Assuming IDs are sequential as created\n",
        "    include=['embeddings', 'documents']\n",
        ")\n",
        "\n",
        "# Print the structured output\n",
        "if retrieved_items and retrieved_items['ids']:\n",
        "    print(\"Example of the first 2 stored entries in Chroma DB:\")\n",
        "    for i in range(len(retrieved_items['ids'])):\n",
        "        print(f\"\\n--- Entry {i+1} ---\")\n",
        "        print(f\"ID: {retrieved_items['ids'][i]}\")\n",
        "        print(f\"Document (Chunk):\")\n",
        "        print(retrieved_items['documents'][i][:500] + \"...\") # Print first 200 chars of document\n",
        "        print(f\"Embedding (first 10 elements):\")\n",
        "        print(retrieved_items['embeddings'][i][:10])\n",
        "else:\n",
        "    print(\"Could not retrieve items from the collection.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of the first 2 stored entries in Chroma DB:\n",
            "\n",
            "--- Entry 1 ---\n",
            "ID: chunk_0\n",
            "Document (Chunk):\n",
            "Prompt  \n",
            "Engineering\n",
            "Author: Lee Boonstra\n",
            "Acknowledgements\n",
            "Reviewers and Contributors\n",
            "Michael Sherman\n",
            "Yuan Cao\n",
            "Erick Armbrust\n",
            "Anant Nawalgaria\n",
            "Antonio Gulli\n",
            "Simone Cammel\n",
            "Curators and Editors\n",
            "Antonio Gulli\n",
            "Anant Nawalgaria\n",
            "Grace Mollison \n",
            "Technical Writer\n",
            "Joey Haymaker\n",
            "Designer\n",
            "Michael Lanning \n",
            "2\n",
            "Prompt EngineeringSeptember 2024\fTable of contents\n",
            "Introduction \n",
            "Prompt engineering \n",
            "LLM output configuration \n",
            "Output length \n",
            "Sampling controls \n",
            "Temperature \n",
            "Top-K and top-P \n",
            "Putting\tit\tall\ttogether...\n",
            "Embedding (first 10 elements):\n",
            "[ 0.01492918  0.00707359 -0.00989591 -0.02309047 -0.00027459  0.02422793\n",
            " -0.01025137 -0.00533185 -0.04689185 -0.03480632]\n",
            "\n",
            "--- Entry 2 ---\n",
            "ID: chunk_1\n",
            "Document (Chunk):\n",
            "iguration \n",
            "Output length \n",
            "Sampling controls \n",
            "Temperature \n",
            "Top-K and top-P \n",
            "Putting\tit\tall\ttogether \n",
            "Prompting techniques \n",
            "General prompting / zero shot \n",
            "One-shot & few-shot \n",
            "System, contextual and role prompting \n",
            "System prompting \n",
            "Role prompting \n",
            "Contextual prompting \n",
            "6\n",
            "7\n",
            "8\n",
            "8\n",
            "9\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "17\n",
            "18\n",
            "21\n",
            "23\n",
            "Step-back prompting \n",
            "Chain of Thought (CoT) \n",
            "Self-consistency \n",
            "Tree of Thoughts (ToT) \n",
            "ReAct (reason & act) \n",
            "Automatic Prompt Engineering \n",
            "Code prompting \n",
            "Prompts for writing code...\n",
            "Embedding (first 10 elements):\n",
            "[-0.00733014  0.01024193 -0.00515812 -0.0095731   0.00465143  0.03264447\n",
            " -0.01406577 -0.009742   -0.05429034 -0.02198367]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Vector Search functionality"
      ],
      "metadata": {
        "id": "dnUdGS7CbHiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Covert the query into embeddings"
      ],
      "metadata": {
        "id": "lJ3Lg8EpcS4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def generate_query_embedding(query_text, client, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"Generates an embedding for a text query using OpenAI.\n",
        "\n",
        "    Args:\n",
        "        query_text (str): The input query text.\n",
        "        client: The initialized OpenAI client.\n",
        "        model (str): The OpenAI embedding model to use.\n",
        "\n",
        "    Returns:\n",
        "        list: The embedding vector for the query text, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.embeddings.create(\n",
        "            input=[query_text], # OpenAI expects a list of inputs\n",
        "            model=model\n",
        "        )\n",
        "        # Assuming the response structure contains 'data' which is a list of embedding objects\n",
        "        if response.data and len(response.data) > 0:\n",
        "            return response.data[0].embedding\n",
        "        else:\n",
        "            print(\"Warning: No embedding returned for the query.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during query embedding generation: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage (assuming 'client' is your initialized OpenAI client)\n",
        "query = \"What is prompt engineering?\"\n",
        "query_embedding = generate_query_embedding(query, client)\n",
        "\n",
        "if query_embedding:\n",
        "   print(f\"Generated embedding for the query (first 10 elements):\")\n",
        "   print(query_embedding[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBh2glugazJ1",
        "outputId": "810c88d8-75af-4c0c-f6c1-382ccd0843e9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embedding for the query (first 10 elements):\n",
            "[-0.013495800085365772, -0.0033809461165219545, -0.011480874381959438, -0.011949624866247177, 0.004152284469455481, 0.007041743025183678, -0.00880130473524332, -0.007870800793170929, -0.021996265277266502, -0.024514921009540558]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Vector search"
      ],
      "metadata": {
        "id": "B_ugbBDFca0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query_embedding, collection, n_results=5, distance_threshold=None):\n",
        "    \"\"\"Performs a vector search in Chroma DB based on a text query embedding with an optional distance threshold.\n",
        "\n",
        "    Args:\n",
        "        query_embedding (list): The embedding vector for the query.\n",
        "        collection: The Chroma DB collection object.\n",
        "        n_results (int): The number of similar results to retrieve.\n",
        "        distance_threshold (float, optional): The maximum distance for retrieved results.\n",
        "                                              Results with a distance greater than this will be excluded.\n",
        "                                              Lower values mean higher similarity.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists: the list of most relevant document chunks\n",
        "               and the list of their corresponding distances, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    if query_embedding is None:\n",
        "        print(\"Error: Query embedding is None.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        # Build the where clause for distance filtering if a threshold is provided\n",
        "        where_clause = {}\n",
        "        if distance_threshold is not None:\n",
        "             where_clause = {\"distance\": {\"$lt\": distance_threshold}}\n",
        "\n",
        "\n",
        "        # Perform the vector search in Chroma DB\n",
        "        results = collection.query(\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=n_results,\n",
        "            include=['documents', 'distances'], # Include documents and distances in the results\n",
        "            where=where_clause # Apply the distance filter\n",
        "        )\n",
        "\n",
        "        # The results are returned as a dictionary, extract the documents and distances\n",
        "        # Note: The structure is a bit nested, results['documents'][0] and results['distances'][0] are lists for the first query\n",
        "        if results and results.get('documents') and results['documents'][0]:\n",
        "            # Return the lists of document chunks and distances\n",
        "            return results['documents'][0], results['distances'][0]\n",
        "        else:\n",
        "            print(\"No results found for the query.\")\n",
        "            return [], []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during vector search: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Example usage (assuming 'client' is your initialized OpenAI client, 'collection' is your Chroma DB collection and query_embedding is already generated)\n",
        "# query_text = \"What is prompt engineering?\"\n",
        "# query_embedding = generate_query_embedding(query_text, client) # Generate embedding separately\n",
        "\n",
        "# if query_embedding:\n",
        "#     search_results, distances = vector_search(query_embedding, collection, n_results=3, distance_threshold=0.2) # Example with a threshold\n",
        "\n",
        "#     if search_results:\n",
        "#         print(f\"\\nTop {len(search_results)} most relevant chunks for the query '{query_text}':\")\n",
        "#         for i, chunk in enumerate(search_results):\n",
        "#             print(f\"Result {i+1} (Distance: {distances[i]:.4f}):\\n{chunk}\\n---\")"
      ],
      "metadata": {
        "id": "nzZWY3Llchpg"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. AI Response"
      ],
      "metadata": {
        "id": "0vTqWzlXeXM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def generate_ai_response(query_text, search_results, client, model=\"gpt-4o-mini\"):\n",
        "    \"\"\"Generates an AI response based on a query and retrieved document chunks.\n",
        "\n",
        "    Args:\n",
        "        query_text (str): The original user query.\n",
        "        search_results (list): A list of relevant document chunks retrieved from the vector database.\n",
        "        client: The initialized OpenAI client.\n",
        "        model (str): The OpenAI model to use for response generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The AI-generated response, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    if not search_results:\n",
        "        return \"Could not find relevant information to answer the query.\"\n",
        "\n",
        "    # Combine the retrieved chunks into a single context string\n",
        "    context = \"\\n\\n\".join(search_results)\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            temperature=0.1,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context. You should use only the information from the provided context. If the information is avaliable in the context the say: I dont know\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Based on the following context, answer the query:\\n\\nContext:\\n{context}\\n\\nQuery: {query_text}\"}\n",
        "            ]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during AI response generation: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage (assuming 'client' is your initialized OpenAI client, 'search_results' are from the vector search)\n",
        "# query = \"What is prompt engineering?\"\n",
        "# search_results = vector_search(query, client, collection, n_results=3) # Get search results first\n",
        "\n",
        "# if search_results:\n",
        "#     ai_response = generate_ai_response(query, search_results, client)\n",
        "#     if ai_response:\n",
        "#         print(\"\\nAI Response:\")\n",
        "#         print(ai_response)\n",
        "# else:\n",
        "#     print(\"No search results to generate a response.\")"
      ],
      "metadata": {
        "id": "PRhAZuhXdSP1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Ask questions"
      ],
      "metadata": {
        "id": "AnI-NDf2f4PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Accept a user question\n",
        "query = input(\"Please enter your question about the document: \")\n",
        "print(f\"\\nUser Query: {query}\")\n",
        "\n",
        "# Step 2: Generate embedding for the query\n",
        "query_embedding = generate_query_embedding(query, client)\n",
        "\n",
        "if query_embedding:\n",
        "    print(f\"\\nQuery Embedding (first 10 elements): {query_embedding[:10]}...\")\n",
        "\n",
        "    # Step 3: Perform vector search in Chroma DB\n",
        "    # You can adjust n_results and distance_threshold as needed\n",
        "    search_results, distances = vector_search(query_embedding, collection, n_results=5, distance_threshold=0.4)\n",
        "\n",
        "    if search_results:\n",
        "        print(f\"\\nVector Search Results ({len(search_results)} chunks found):\") # Print the number of chunks found\n",
        "        # Print the first search result and its distance\n",
        "        print(f\"Result 1 (Distance: {distances[0]:.4f}):\\n{search_results[0][:200]}...\") # Print first 200 characters of the first result\n",
        "        print(f\"Result 1 (Distance: {distances[1]:.4f}):\\n{search_results[1][:200]}...\") # Print first 200 characters of the first result\n",
        "        # Step 4: Generate AI response based on search results\n",
        "        ai_response = generate_ai_response(query, search_results, client)\n",
        "\n",
        "        if ai_response:\n",
        "            print(\"\\nAI Response:\")\n",
        "            print(ai_response)\n",
        "        else:\n",
        "            print(\"Failed to generate AI response.\")\n",
        "    else:\n",
        "        print(\"No relevant information found in the document.\")\n",
        "else:\n",
        "    print(\"Failed to generate query embedding.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXqbIRxuf-BG",
        "outputId": "1402fc25-dcd4-4916-f606-33dc54a0c8eb"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your question about the document: kakwo e каварма\n",
            "\n",
            "User Query: kakwo e каварма\n",
            "\n",
            "Query Embedding (first 10 elements): [-0.0029184683226048946, 0.00248140306212008, -0.004828866571187973, -0.04328356310725212, -0.01948465220630169, -0.00897393748164177, -0.02202245220541954, -0.04057657718658447, -0.006524256896227598, -0.005688898265361786]...\n",
            "No results found for the query.\n",
            "No relevant information found in the document.\n"
          ]
        }
      ]
    }
  ]
}