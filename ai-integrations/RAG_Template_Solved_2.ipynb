{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Integrations for Developers ‚Äî Exam"
      ],
      "metadata": {
        "id": "8PySlKkufkth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Integrations for Developers ‚Äî Exam\n",
        "\n",
        "## Project Objective\n",
        "\n",
        "The main objective is to create an **AI chatbot** that can answer questions by retrieving information from the **provided PDF file**.  \n",
        "\n",
        "The chatbot should be able to:  \n",
        "- Parse and understand the content of the PDF.  \n",
        "- Use the extracted information to provide relevant and accurate answers.  \n",
        "- Respond specifically to the **questions in the final cell of the notebook**.  \n",
        "\n",
        "\n",
        "## General Instructions\n",
        "\n",
        "- This notebook is a **template** where you must put your code.  \n",
        "- You should **fill in all empty variables** and complete the code so that when I download your notebook and click **Run all**, all cells execute correctly and provide the answers.  \n",
        "- ‚ö†Ô∏è **Do NOT hardcode your API key**. Use Colab environment variables (`%env OPENAI_API_KEY=your_key_here`) and access them in your code.  \n",
        "- You may **create more cells** if needed. It is recommended that your code is well-structured and split logically into separate cells.  \n",
        "- The function **`ask_ai(query)`** must be implemented by you. All queries will call this function to check your solution.  \n",
        "- ‚úÖ **Test cases will be created by me (the instructor).** You are **not allowed to modify, remove, or add to the test cases cell**. Your code must work correctly with the provided test cases.  \n",
        "- You are **ONLY ALLOWED** to use only the following:  \n",
        "  - **Models:** OpenAI or Anthropic  \n",
        "  - **Technologies:** LangChain or vanilla Python code  \n",
        "  - **Vector Store:** Chroma DB\n",
        "\n",
        "‚Ñπ Before starting, please read the test queries in the final cell to understand the expected outputs.\n",
        "\n",
        "üö® **Any student who does not follow the template, does not stick to the required format, or whose code does not execute properly will be disqualified.**\n"
      ],
      "metadata": {
        "id": "SL2vj_IDfuxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important\n",
        "\n",
        "Fill in **all the variables** in the cell.  \n",
        "‚ùå **Do NOT put your API key directly in the code.**  \n",
        "‚úÖ The cell must be set up to take the API key from the Colab environment variables.\n"
      ],
      "metadata": {
        "id": "a6q8k6WrgeNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6USRQ_svffQK"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# üîß RAG Configuration Variables\n",
        "# ================================\n",
        "\n",
        "# ‚ö†Ô∏è Do NOT put your API key here directly.\n",
        "# Make sure you set your API key in Colab like this:\n",
        "# %env OPENAI_API_KEY=your_key_here\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# API Key (taken from Colab environment variables)\n",
        "API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Prompt & Model Settings\n",
        "PROMPT = \"\"\"\n",
        "<context>\n",
        "You are provided with context chunks retrieved from a company handbook.\n",
        "Only use this context to answer questions.\n",
        "If the answer is not in the context, reply exactly: \"Not in the guide.\"\n",
        "</context>\n",
        "\n",
        "<role>\n",
        "You are a helpful assistant answering questions from a company handbook.\n",
        "Only use the provided context to answer.\n",
        "If the answer is not found in the context, say: 'Not in the guide.'\n",
        "Keep answers concise and factual.\n",
        "</role>\n",
        "\n",
        "<user_info>\n",
        "The user is a generative AI enthusiast with a strong interest in practical applications of LLMs.\n",
        "They enjoy designing and refining Retrieval-Augmented Generation (RAG) pipelines and experimenting\n",
        "with advanced prompt engineering techniques.\n",
        "</user_info>\n",
        "\n",
        "<examples>\n",
        "Q: How many words should effective prompts average?\n",
        "A: Effective prompts should average around 21 words.\n",
        "\n",
        "Q: What does 'persona' mean in prompt writing?\n",
        "A: 'Persona' refers to the role or identity assigned to the AI, which influences its responses.\n",
        "</examples>\n",
        "\n",
        "<style>\n",
        "- Answer in clear, professional business language.\n",
        "</style>\n",
        "\n",
        "<format>\n",
        "Your response must be plain text.\n",
        "Do not include explanations of your reasoning.\n",
        "</format>\n",
        "\"\"\"                                          # e.g. \"Summarize the document in 3 sentences\"\n",
        "MODEL = \"gpt-4o-mini\"                        # e.g. \"gpt-4\"\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"   # e.g. \"text-embedding-ada-002\"\n",
        "\n",
        "# Chunking Parameters\n",
        "CHUNK_SIZE = 300            # e.g. 500\n",
        "CHUNK_OVERLAP = 50         # e.g. 50\n",
        "TOP_N_RESULTS = 8         # e.g. 3\n",
        "\n",
        "# Generation Parameters\n",
        "OUTPUT_LENGTH = 420          # e.g. 200\n",
        "TEMPERATURE = 0.2            # e.g. 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Organization\n",
        "\n",
        "Create more cells if needed and put your code in them.  \n",
        "It is **recommended** that your code is well-structured, split logically, and kept in separate cells for clarity.\n"
      ],
      "metadata": {
        "id": "b0u-pPNriGQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üîß Install packages\n",
        "# ================================\n",
        "\n",
        "!pip install chromadb pypdf openai tiktoken\n",
        "\n"
      ],
      "metadata": {
        "id": "W0aZh8315g55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "49762c2b-92e8-4636-8580-445b5fcee39c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.107.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=97941a2ed94b996f04d2d6466e2cf7d44e38e61b8c6f324fad3812bca5a7941a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pypdf, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.36.0\n",
            "    Uninstalling opentelemetry-api-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.36.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.57b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.57b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.57b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.36.0\n",
            "    Uninstalling opentelemetry-sdk-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.36.0\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.1.0 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 posthog-5.4.0 pybase64-1.4.2 pypdf-6.0.0 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üìÇ File Upload (PDF)\n",
        "# ================================\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Upload a PDF file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get filename\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "print(f\"‚úÖ Uploaded file: {pdf_path}\")\n"
      ],
      "metadata": {
        "id": "Sr7fql2C5g2x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "2798ecd3-44d8-4882-f80d-e63c181166ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a4a97f13-8a2f-441c-a4b3-01eab61db619\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a4a97f13-8a2f-441c-a4b3-01eab61db619\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Gemini-Prompting-Guide.pdf to Gemini-Prompting-Guide.pdf\n",
            "‚úÖ Uploaded file: Gemini-Prompting-Guide.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üìñ Extract text from PDF\n",
        "# ================================\n",
        "\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Read PDF\n",
        "reader = PdfReader(pdf_path)\n",
        "\n",
        "# Extract text from all pages\n",
        "extracted_text = \"\"\n",
        "for page in reader.pages:\n",
        "    extracted_text += page.extract_text() + \"\\n\"\n",
        "\n",
        "# Save extracted text to a file for verification\n",
        "text_file = \"extracted_text.txt\"\n",
        "with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(extracted_text)\n",
        "\n",
        "print(f\"‚úÖ Text extracted and saved to {text_file} (length: {len(extracted_text)} chars)\")\n",
        "\n",
        "# Option to download file\n",
        "# from google.colab import files\n",
        "# files.download(text_file)\n"
      ],
      "metadata": {
        "id": "gEzWeB945g0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb425aef-8cd2-40da-9d33-3ba6fc1fc128"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Text extracted and saved to extracted_text.txt (length: 111461 chars)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ‚úÇÔ∏è Sentence-aware Chunking\n",
        "# ================================\n",
        "\n",
        "import re\n",
        "import tiktoken\n",
        "\n",
        "# Load tokenizer for the embedding model\n",
        "enc = tiktoken.encoding_for_model(EMBEDDING_MODEL)\n",
        "\n",
        "def num_tokens(text: str) -> int:\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "def split_into_sentences(text: str):\n",
        "    # Simple regex-based sentence splitter\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "    return [s for s in sentences if s]\n",
        "\n",
        "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
        "    sentences = split_into_sentences(text)\n",
        "    chunks, current_chunk, current_tokens = [], [], 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent_tokens = num_tokens(sent)\n",
        "\n",
        "        # If adding this sentence exceeds chunk size, save current chunk\n",
        "        if current_tokens + sent_tokens > chunk_size:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            # Start new chunk with overlap from last chunk\n",
        "            overlap_tokens = []\n",
        "            while current_chunk and num_tokens(\" \".join(overlap_tokens)) < overlap:\n",
        "                overlap_tokens.insert(0, current_chunk.pop())\n",
        "            current_chunk = overlap_tokens.copy()\n",
        "            current_tokens = num_tokens(\" \".join(current_chunk))\n",
        "\n",
        "        # Add sentence to current chunk\n",
        "        current_chunk.append(sent)\n",
        "        current_tokens += sent_tokens\n",
        "\n",
        "    # Add last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Create chunks\n",
        "chunks = chunk_text(extracted_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "\n",
        "# Save chunks to file\n",
        "chunks_file = \"chunks_preview.txt\"\n",
        "with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\n\\n\")\n",
        "\n",
        "# Print statistics\n",
        "print(f\"‚úÖ Total Chunks: {len(chunks)}\")\n",
        "chunk_lengths = [num_tokens(c) for c in chunks]\n",
        "print(f\"üìä Avg tokens per chunk: {sum(chunk_lengths)//len(chunk_lengths)}\")\n",
        "print(f\"üìä Min tokens: {min(chunk_lengths)}, Max tokens: {max(chunk_lengths)}\")\n",
        "\n",
        "# Preview first chunks\n",
        "for i, chunk in enumerate(chunks[:3]):\n",
        "    print(f\"\\nüîç Chunk {i+1} ({num_tokens(chunk)} tokens):\\n{chunk[:400]}...\\n\")\n",
        "\n",
        "# Allow download\n",
        "# from google.colab import files\n",
        "# files.download(chunks_file)\n"
      ],
      "metadata": {
        "id": "0TX77N_c5gxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93540d7f-78db-4e6e-9513-677a79469df4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Total Chunks: 105\n",
            "üìä Avg tokens per chunk: 286\n",
            "üìä Min tokens: 120, Max tokens: 301\n",
            "\n",
            "üîç Chunk 1 (261 tokens):\n",
            "1\n",
            "October 2024 edition\n",
            "A quick-start handbook \n",
            "for effective prompts\n",
            "\n",
            "2\n",
            "Writing effective prompts \n",
            "From the very beginning, Google Workspace was built to allow you to collaborate in real time with other people. Now, you can also collaborate with AI using Gemini for Google Workspace to help boost your productivity and \n",
            "creativity without sacrificing privacy or security. The embedded generative AI-p...\n",
            "\n",
            "\n",
            "üîç Chunk 2 (293 tokens):\n",
            "This guide provides you with the foundational skills to write effective prompts when using Gemini for Workspace. You can think of a prompt as a conversation starter with your AI-powered assistant. You might write several \n",
            "prompts as the conversation progresses. While the possibilities are virtually endless, you can put consistent \n",
            "best practices to work today. The four main areas to consider when ...\n",
            "\n",
            "\n",
            "üîç Chunk 3 (300 tokens):\n",
            "Express complete thoughts in  \n",
            "full sentences. 2. Be specific and iterate. Tell Gemini what you need it to do (summarize, write, change the tone, create). Provide as much context as possible. 3. Be concise and avoid complexity. State your request in brief ‚Äî but specific ‚Äî language. Avoid jargon. 4. Make it a conversation. Fine-tune your prompts if the results don‚Äôt meet your expectations or if you...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üîë Create Embeddings for Chunks\n",
        "# ================================\n",
        "\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "print(\"‚è≥ Generating embeddings...\")\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    response = client.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=chunk\n",
        "    )\n",
        "    vector = response.data[0].embedding\n",
        "    embeddings.append({\n",
        "        \"id\": f\"chunk_{i+1}\",\n",
        "        \"text\": chunk,\n",
        "        \"embedding\": vector\n",
        "    })\n",
        "\n",
        "    if (i+1) % 10 == 0 or i == len(chunks)-1:\n",
        "        print(f\"‚úÖ Processed {i+1}/{len(chunks)} chunks\")\n",
        "\n",
        "# Save to JSONL file\n",
        "embeddings_file = \"chunk_embeddings.jsonl\"\n",
        "with open(embeddings_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for e in embeddings:\n",
        "        f.write(json.dumps(e) + \"\\n\")\n",
        "\n",
        "print(f\"\\n‚úÖ Saved embeddings to {embeddings_file} (total {len(embeddings)})\")\n",
        "\n",
        "# Allow download\n",
        "# from google.colab import files\n",
        "# files.download(embeddings_file)\n"
      ],
      "metadata": {
        "id": "t8O03AMV5guB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc08c461-1468-4d78-89f7-f79932fbf81c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Generating embeddings...\n",
            "‚úÖ Processed 10/105 chunks\n",
            "‚úÖ Processed 20/105 chunks\n",
            "‚úÖ Processed 30/105 chunks\n",
            "‚úÖ Processed 40/105 chunks\n",
            "‚úÖ Processed 50/105 chunks\n",
            "‚úÖ Processed 60/105 chunks\n",
            "‚úÖ Processed 70/105 chunks\n",
            "‚úÖ Processed 80/105 chunks\n",
            "‚úÖ Processed 90/105 chunks\n",
            "‚úÖ Processed 100/105 chunks\n",
            "‚úÖ Processed 105/105 chunks\n",
            "\n",
            "‚úÖ Saved embeddings to chunk_embeddings.jsonl (total 105)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üóÑÔ∏è Insert Embeddings into ChromaDB\n",
        "# ================================\n",
        "\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "# Create Chroma client (in-memory for now, can persist later)\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "# Delete collection (for reruns)\n",
        "try:\n",
        "    collection = chroma_client.delete_collection(name=\"prompting_guide\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create collection\n",
        "collection = chroma_client.get_or_create_collection(name=\"prompting_guide\", metadata={\"hnsw:space\": \"cosine\"})\n",
        "\n",
        "# Insert embeddings\n",
        "ids = [e[\"id\"] for e in embeddings]\n",
        "documents = [e[\"text\"] for e in embeddings]\n",
        "vectors = [e[\"embedding\"] for e in embeddings]\n",
        "\n",
        "collection.add(\n",
        "    ids=ids,\n",
        "    documents=documents,\n",
        "    embeddings=vectors\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Inserted {len(ids)} chunks into Chroma collection 'prompting_guide'\")\n",
        "\n",
        "# Quick check: count items\n",
        "print(\"üìä Collection count:\", collection.count())\n",
        "\n",
        "# Sample\n",
        "sample = collection.peek()\n",
        "print(\"\\nüîç Sample:\", sample)\n"
      ],
      "metadata": {
        "id": "hNDt5cn95gqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8941d9e8-3d82-4f39-a4ee-e24f755dcda9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Inserted 105 chunks into Chroma collection 'prompting_guide'\n",
            "üìä Collection count: 105\n",
            "\n",
            "üîç Sample: {'ids': ['chunk_1', 'chunk_2', 'chunk_3', 'chunk_4', 'chunk_5', 'chunk_6', 'chunk_7', 'chunk_8', 'chunk_9', 'chunk_10'], 'embeddings': array([[ 0.03427482,  0.0243559 ,  0.03334006, ..., -0.0135022 ,\n",
            "        -0.00385592, -0.00366117],\n",
            "       [ 0.01721646,  0.03215443,  0.01425211, ..., -0.0180767 ,\n",
            "        -0.00821298, -0.00122207],\n",
            "       [ 0.03158743,  0.03273606,  0.0329449 , ..., -0.03111753,\n",
            "        -0.00375264,  0.02439541],\n",
            "       ...,\n",
            "       [-0.00027881,  0.02642701,  0.06068236, ..., -0.0209523 ,\n",
            "        -0.00974704,  0.01115409],\n",
            "       [-0.01558568,  0.03539576,  0.0568851 , ..., -0.01010839,\n",
            "         0.00940651, -0.01378834],\n",
            "       [-0.00827033,  0.04495667,  0.03185001, ..., -0.00583506,\n",
            "         0.01812769, -0.01005574]]), 'documents': ['1\\nOctober 2024 edition\\nA quick-start handbook \\nfor effective prompts\\n\\n2\\nWriting effective prompts \\nFrom the very beginning, Google Workspace was built to allow you to collaborate in real time with other people. Now, you can also collaborate with AI using Gemini for Google Workspace to help boost your productivity and \\ncreativity without sacrificing privacy or security. The embedded generative AI-powered features can help you \\nwrite, organize information, create images, accelerate workflows, have richer meetings, and much more, all \\nwhile using your favorite apps like Gmail, Google Docs, Google Drive, Google Sheets, Google Meet, Google \\nSlides, and Gemini Advanced (the standalone chat experience available at gemini.google.com with enterprise-\\ngrade security). Gemini is accessible right where you are doing your work ‚Äî with access to your personal \\nknowledge base in Drive, Docs, Gmail, and more ‚Äî so you can enhance and create powerful workflows across \\nthe Workspace apps with less tab switching and interruption. This guide provides you with the foundational skills to write effective prompts when using Gemini for Workspace. You can think of a prompt as a conversation starter with your AI-powered assistant. You might write several \\nprompts as the conversation progresses. While the possibilities are virtually endless, you can put consistent \\nbest practices to work today.', 'This guide provides you with the foundational skills to write effective prompts when using Gemini for Workspace. You can think of a prompt as a conversation starter with your AI-powered assistant. You might write several \\nprompts as the conversation progresses. While the possibilities are virtually endless, you can put consistent \\nbest practices to work today. The four main areas to consider when writing an effective prompt are:\\n‚Ä¢  Persona \\n‚Ä¢  Task \\n‚Ä¢  Context \\n‚Ä¢  Format \\n \\nHere is an example of a prompt using all four areas that could work well in Gmail and Google Docs: \\n You are a program manager in [industry]. Draft an executive summary email to  [persona] based on [details \\n about relevant program docs]. Limit to bullet points. You don‚Äôt need to use all four in every prompt, but using a few will help! Always remember to include a verb or \\ncommand as part of your task; this is the most important component of a prompt. Contact sales to get started with \\nGemini for Workspace today. 3\\nHere are quick tips to get you started with Gemini for Google Workspace: \\n1. Use natural language. Write as if you‚Äôre speaking to another person. Express complete thoughts in  \\nfull sentences. 2. Be specific and iterate. Tell Gemini what you need it to do (summarize, write, change the tone, create). Provide as much context as possible. 3. Be concise and avoid complexity.', 'Express complete thoughts in  \\nfull sentences. 2. Be specific and iterate. Tell Gemini what you need it to do (summarize, write, change the tone, create). Provide as much context as possible. 3. Be concise and avoid complexity. State your request in brief ‚Äî but specific ‚Äî language. Avoid jargon. 4. Make it a conversation. Fine-tune your prompts if the results don‚Äôt meet your expectations or if you believe \\nthere‚Äôs room for improvement. Use follow-up prompts and an iterative process of review and refinement to \\nyield better results. 5. Use your documents. Personalize Gemini‚Äôs output with information from your own files in Google Drive. 6. Make Gemini your prompt editor. When using Gemini Advanced, start your prompts with: ‚ÄúMake this a \\npower prompt: [original prompt text here].‚Äù Gemini will make suggestions on how to improve your prompt. Ensure it says what you need, and then paste it back into Gemini Advanced to get an output. Prompting is a skill we can all learn. You don‚Äôt have to be a prompt \\nengineer to use generative AI. However, you will likely need to try a few \\ndifferent approaches for your prompt if you don‚Äôt get your desired \\noutcome the first time. Based on what we‚Äôve learned from our users \\nso far, the most fruitful prompts average around 21 words with relevant \\ncontext, yet the prompts people try are usually less than nine words.', 'However, you will likely need to try a few \\ndifferent approaches for your prompt if you don‚Äôt get your desired \\noutcome the first time. Based on what we‚Äôve learned from our users \\nso far, the most fruitful prompts average around 21 words with relevant \\ncontext, yet the prompts people try are usually less than nine words. Generative AI and all of its possibilities are exciting, but it‚Äôs still new. Even \\nthough our models are getting better every day, prompts can sometimes \\nhave unpredictable responses. Before putting an output from Gemini for Workspace into action, review it \\nto ensure clarity, relevance, and accuracy. And of course, keep the most \\nimportant thing in mind: Generative AI is meant to help humans but the \\nfinal output is yours. The example prompts in this guide are meant for illustrative purposes. 4\\nTable of contents\\nWriting effective prompts .................................................... Page 2 \\nIntroduction .................................................................. Page 5\\nAdministrative support ....................................................... Page 7\\nCommunications ............................................................. Page 11\\nCustomer service ........................................................... Page 15\\nExecutives ................................................................... Page 20\\nFrontline management ...................................................... Page 28\\nHuman resources ........................................................... Page 32\\nMarketing .................................................................... Page 37\\nProject management ........................................................ Page 46\\nSales ......................................................................... Page 50\\nSmall business owners and entrepreneurs ................................. Page 58\\nStartup leaders  ..............................................................', 'Page 28\\nHuman resources ........................................................... Page 32\\nMarketing .................................................................... Page 37\\nProject management ........................................................ Page 46\\nSales ......................................................................... Page 50\\nSmall business owners and entrepreneurs ................................. Page 58\\nStartup leaders  .............................................................. Page 62\\nLeveling up your prompt writing ............................................ Page 67\\n\\n5\\nIntroduction\\nGemini for Google Workspace: Prompting 101\\nGemini for Workspace is your AI-powered assistant integrated into the apps you use every day ‚Äî Gmail, Google \\nDocs, Google Sheets, Google Meet, Google Slides, and Gemini Advanced (the standalone chat experience \\navailable at gemini.google.com with enterprise-grade security). This means the apps you know and use will work \\ntogether smoothly so you can collaborate with Gemini right where you are. You can have fewer interruptions to \\nyour focus and workflow, helping you complete tasks and do things you might not have initially known how to do. You can access the features of Gemini for Workspace in multiple ways. Engaging with Gemini in the side panel \\nof your Workspace apps allows you to create highly personalized generative AI outputs that are based on your \\nown files and documents ‚Äî even if they aren‚Äôt Google Docs. You can generate personalized emails in seconds \\nreferencing your own Docs to pull in relevant context, generate Slides that are based on information directly  \\nfrom your own briefs or reports, and so much more.', 'Engaging with Gemini in the side panel \\nof your Workspace apps allows you to create highly personalized generative AI outputs that are based on your \\nown files and documents ‚Äî even if they aren‚Äôt Google Docs. You can generate personalized emails in seconds \\nreferencing your own Docs to pull in relevant context, generate Slides that are based on information directly  \\nfrom your own briefs or reports, and so much more. Understanding what makes an effective prompt and learning to craft prompts on the fly can boost your \\nproductivity and creativity. Gemini for Workspace can help you:\\n‚Ä¢ Improve your writing\\n‚Ä¢ Organize data\\n‚Ä¢ Create original images\\n‚Ä¢ Summarize information and surface insights \\n‚Ä¢ Have better meetings with automatic note taking\\n‚Ä¢ Research unfamiliar topics easily\\n‚Ä¢ Spot trends, synthesize information, and identify business opportunities \\nFor 25 years, Google has built helpful, secure products that give users choice and control over their data. It‚Äôs a \\nbedrock principle for us. This was the case back when we first launched Gmail in 2004, and it remains true in the \\nera of generative AI. This means your data is your data and does not belong to Google. Your data stays in your \\nWorkspace environment. Your privacy is protected. Your content is never used for targeting ads or to train or \\nimprove Gemini or any other generative AI models. 6\\nHow to use this prompt guide\\nThis guide introduces you to prompting with Gemini for Workspace.', 'Your privacy is protected. Your content is never used for targeting ads or to train or \\nimprove Gemini or any other generative AI models. 6\\nHow to use this prompt guide\\nThis guide introduces you to prompting with Gemini for Workspace. It includes \\nstrong prompt design examples to help you get started. Additionally, it covers \\nscenarios for different personas, use cases, and potential prompts. You will notice a variety of prompt styles. Some prompts have brackets, which \\nindicate where you would fill in specific details or tag your own personal files by \\ntyping @file name. Other prompts are presented without variables highlighted to \\nshow you what a full prompt could look like. All of the prompts in this guide are \\nmeant to inspire you, but ultimately they will need to be customized to help you \\nwith your specific work. To get started, use the role-specific suggested prompts as inspiration to help you \\nunlock a new and powerful way of working. Next, learn how you can get \\nstarted with different features  \\nby visiting g.co/gemini/features. 7\\nAdministrative \\nsupport\\nAs an administrative support professional, you \\nare responsible for keeping teams on track. You‚Äôre required to stay organized and efficient \\n‚Äî even under pressure ‚Äî while juggling many \\npriority tasks. This section provides you with simple ways to \\nintegrate prompts in your daily tasks.', '7\\nAdministrative \\nsupport\\nAs an administrative support professional, you \\nare responsible for keeping teams on track. You‚Äôre required to stay organized and efficient \\n‚Äî even under pressure ‚Äî while juggling many \\npriority tasks. This section provides you with simple ways to \\nintegrate prompts in your daily tasks. Getting started \\nFirst, review the general prompt-writing tips on page 2 and the Prompting 101 section at the beginning  \\nof this guide. Each prompt below is presented with an accompanying scenario to serve as inspiration for how you can \\ncollaborate with Gemini for Google Workspace. The prompt iteration example shows how you could write  \\nfollow-up prompts to build on the initial generated response. Prompt iteration example\\nUse case: Plan agendas (offsite, meetings, and more)\\nYou‚Äôre planning a three-day offsite meeting. To build an agenda, you brainstorm with Gemini Advanced. You type:\\n I am an executive administrator to a team director. Our newly formed team now consists of content  \\n marketers, digital marketers, and product marketers. We are gathering for the first time at a three-day  \\n offsite in Washington, DC. Plan activities for each day that include team bonding activities and time for  \\n deeper strategic work. Create a sample agenda for me. (Gemini Advanced)\\nNEW\\n‚Ä¢  Persona ‚Ä¢  Task ‚Ä¢  Context ‚Ä¢  Format \\n\\n8\\nThis is a helpful start to your planning. You need to generate specific ideas for the team bonding activities.', 'Create a sample agenda for me. (Gemini Advanced)\\nNEW\\n‚Ä¢  Persona ‚Ä¢  Task ‚Ä¢  Context ‚Ä¢  Format \\n\\n8\\nThis is a helpful start to your planning. You need to generate specific ideas for the team bonding activities. You type:\\n Suggest three different icebreaker activities  that encourage people to learn about their teammates‚Äô  \\n preferred working styles, strengths, and goals. Make sure the icebreaker ideas are engaging and can be  \\n completed by a group of 25 people in 30 minutes or less. (Gemini Advanced)\\nYou are happy with the agenda as a starting point. You now want to reformat Gemini‚Äôs response into a table. You type:\\n Organize this agenda  in a table format. Include one of your suggested icebreakers for each day. (Gemini Advanced)\\nGemini Advanced\\nGemini Advanced\\n\\n9\\nYou select Export to Docs. You open the newly created Doc. Now, you want to bring in detailed summaries \\nfor the strategy sessions using your existing files in Google Drive to provide more context for what will be \\ndiscussed. You prompt Gemini in Docs and tag your relevant files by typing @file name. Use @[2024 H2 Team Vision]  to generate a summary for the opening remarks on Day 1 of this agenda.', 'Now, you want to bring in detailed summaries \\nfor the strategy sessions using your existing files in Google Drive to provide more context for what will be \\ndiscussed. You prompt Gemini in Docs and tag your relevant files by typing @file name. Use @[2024 H2 Team Vision]  to generate a summary for the opening remarks on Day 1 of this agenda. (Gemini in Docs)\\nExample use cases\\nExecutive administrators and executive business partners\\nUse case: Manage multiple email inboxes\\nAfter returning from vacation, you have many unread, unsorted emails. You prompt Gemini in the Gmail side \\npanel. You type:\\nSummarize emails from [manager] from the last week. (Gemini in Gmail)\\nGemini returns short summaries of each message. To directly access a message, you click on Sources and see \\ntiles that bring you to specific emails. You select the most important one. Once the email thread opens, you see \\nthat many messages were exchanged. You prompt Gemini in Gmail:\\nSummarize this email thread and list all action items and deadlines. (Gemini in Gmail)\\nNEW\\nGemini Advanced\\n\\n10\\nYou owe a response to a question, which you believe is best answered by a document in your Drive. You prompt \\nGemini in the Gmail side panel.'], 'uris': None, 'included': ['metadatas', 'documents', 'embeddings'], 'data': None, 'metadatas': [None, None, None, None, None, None, None, None, None, None]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üîç Vector Search Function\n",
        "# ================================\n",
        "\n",
        "def search_chunks(query, n_results=TOP_N_RESULTS):\n",
        "    # Embed the query\n",
        "    response = client.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=query\n",
        "    )\n",
        "    query_embedding = response.data[0].embedding\n",
        "\n",
        "    # Query Chroma\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    # Format results\n",
        "    retrieved = []\n",
        "    for i in range(len(results[\"ids\"][0])):\n",
        "        retrieved.append({\n",
        "            \"id\": results[\"ids\"][0][i],\n",
        "            \"text\": results[\"documents\"][0][i],\n",
        "            \"distance\": results[\"distances\"][0][i]\n",
        "        })\n",
        "\n",
        "    return retrieved\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ianJyYu95gkX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Quick test\n",
        "test_query = \"Which Google apps integrate with Gemini?\"\n",
        "results = search_chunks(test_query, n_results=3)\n",
        "\n",
        "print(f\"üîé Query: {test_query}\\n\")\n",
        "for r in results:\n",
        "    print(f\"üìå {r['id']} (distance: {r['distance']:.4f})\")\n",
        "    print(r[\"text\"][:400] + \"...\\n\")"
      ],
      "metadata": {
        "id": "-DmK6emx5gcn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e02b495-cf39-412f-fd39-d52691c9db24"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Query: Which Google apps integrate with Gemini?\n",
            "\n",
            "üìå chunk_5 (distance: 0.3471)\n",
            "Page 28\n",
            "Human resources ........................................................... Page 32\n",
            "Marketing .................................................................... Page 37\n",
            "Project management ........................................................ Page 46\n",
            "Sales ......................................................................... Page 50\n",
            "Small business owners and entrepreneurs ............\n",
            "\n",
            "üìå chunk_6 (distance: 0.3484)\n",
            "Engaging with Gemini in the side panel \n",
            "of your Workspace apps allows you to create highly personalized generative AI outputs that are based on your \n",
            "own files and documents ‚Äî even if they aren‚Äôt Google Docs. You can generate personalized emails in seconds \n",
            "referencing your own Docs to pull in relevant context, generate Slides that are based on information directly  \n",
            "from your own briefs or report...\n",
            "\n",
            "üìå chunk_64 (distance: 0.3976)\n",
            "(Gemini in Docs)\n",
            "Content Marketing Manager\n",
            "Use case: Deliver personalized content to customers at scale\n",
            "You want to create copy for a five-step email nurture cadence for your new product. You open a new Google \n",
            "Doc and prompt Gemini in the Docs side panel and tag relevant files by typing @file name. You type:\n",
            "Create a 5-step nurture email cadence to [prospective customers] who have signed up for [...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üß™ Debug Search Similarity\n",
        "# ================================\n",
        "\n",
        "debug_query = \"Which Google apps integrate with Gemini?\"\n",
        "\n",
        "# Embed the query\n",
        "response = client.embeddings.create(\n",
        "    model=EMBEDDING_MODEL,\n",
        "    input=debug_query\n",
        ")\n",
        "query_vector = response.data[0].embedding\n",
        "\n",
        "# Search top 10 matches\n",
        "results = collection.query(\n",
        "    query_embeddings=[query_vector],\n",
        "    n_results=10\n",
        ")\n",
        "\n",
        "print(f\"üîé Query: {debug_query}\\n\")\n",
        "for i in range(len(results[\"ids\"][0])):\n",
        "    dist = results[\"distances\"][0][i]\n",
        "    txt = results[\"documents\"][0][i][:300].replace(\"\\n\", \" \")\n",
        "    print(f\"üìå {results['ids'][0][i]} | distance={dist:.4f}\")\n",
        "    print(f\"   {txt}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPcfQgQi7tHq",
        "outputId": "a3ee295e-f859-4401-a485-8dfd5c40e78f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Query: Which Google apps integrate with Gemini?\n",
            "\n",
            "üìå chunk_5 | distance=0.3471\n",
            "   Page 28 Human resources ........................................................... Page 32 Marketing .................................................................... Page 37 Project management ........................................................ Page 46 Sales ...............................\n",
            "\n",
            "üìå chunk_6 | distance=0.3484\n",
            "   Engaging with Gemini in the side panel  of your Workspace apps allows you to create highly personalized generative AI outputs that are based on your  own files and documents ‚Äî even if they aren‚Äôt Google Docs. You can generate personalized emails in seconds  referencing your own Docs to pull in relev\n",
            "\n",
            "üìå chunk_64 | distance=0.3977\n",
            "   (Gemini in Docs) Content Marketing Manager Use case: Deliver personalized content to customers at scale You want to create copy for a five-step email nurture cadence for your new product. You open a new Google  Doc and prompt Gemini in the Docs side panel and tag relevant files by typing @file name.\n",
            "\n",
            "üìå chunk_91 | distance=0.3983\n",
            "   (Gemini in Docs) ‚Ä¢  Persona ‚Ä¢  Task ‚Ä¢  Context ‚Ä¢  Format   59 Gemini in Docs Gemini returns a formatted table comparing the two proposals. After you make your decision, you go to your  email and prompt Gemini in the Gmail side panel. You type:  Write an email to Company A  thanking them for their ti\n",
            "\n",
            "üìå chunk_21 | distance=0.4009\n",
            "   (Gemini in Docs)  Gemini in Docs: [List of alternative solutions]  Gemini in Docs Gemini in Docs These 10 suggestions are helpful. You click Insert to add the text into your draft. 17 Example use cases Customer Service Manager or Representative  Use case: Respond to complex customer issues using FAQ\n",
            "\n",
            "üìå chunk_82 | distance=0.4065\n",
            "   You need to create a schedule, so you open a new Google Doc and   55 prompt Gemini in Docs by selecting Help me write. You type:  Create a half-day agenda for an educational session on our latest technology [products] for sales teams. Include time for the product development team to present and incl\n",
            "\n",
            "üìå chunk_1 | distance=0.4108\n",
            "   1 October 2024 edition A quick-start handbook  for effective prompts  2 Writing effective prompts  From the very beginning, Google Workspace was built to allow you to collaborate in real time with other people. Now, you can also collaborate with AI using Gemini for Google Workspace to help boost you\n",
            "\n",
            "üìå chunk_58 | distance=0.4198\n",
            "   You type:  Create a blog draft announcing that [name] is joining [company] as [position]. [Share two or three  details from their bio, such as their previous position and company, their involvement in professional  organizations, etc.]. (Gemini in Docs) You also want a way to efficiently track how a\n",
            "\n",
            "üìå chunk_63 | distance=0.4213\n",
            "   (Gemini in Gmail)  NEW  43 Use case: Generate inbound marketing campaigns  The team created a new ebook on best practices for executives using our new solution. You‚Äôre creating a landing  page to house the gated asset, and you need engaging copy. You open a new Google Doc and select Help me  write. \n",
            "\n",
            "üìå chunk_23 | distance=0.4246\n",
            "   (Gemini in Docs)  NEW  18 You also want to support the team with standardized language that they can use when interacting with  customers on phone calls. You prompt Gemini Advanced: I am a [customer service manager]. I am trying to create standardized language that the team can  use when interacting\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hCDZd28L7w9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Cases (Final Cell)\n",
        "\n",
        "The final cell must contain your **test cases**.  \n",
        "When executed, the AI should provide correct answers to the given questions **based on the PDF file**.\n"
      ],
      "metadata": {
        "id": "b2S9Kbftik_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AI Query Function\n",
        "\n",
        "In this cell, you must implement the function **ask_ai(query)**.  \n",
        "This function will be the final execution point of your pipeline (RAG / LLM).  \n"
      ],
      "metadata": {
        "id": "r7v46jYqjg3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ‚ùì AI Query Function (with Debug Mode)\n",
        "# ================================\n",
        "\n",
        "DEBUG = False  # üîé Toggle evidence printing\n",
        "\n",
        "def ask_ai(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Executes the final RAG / LLM pipeline with optional debug mode.\n",
        "    Input:\n",
        "        query (str): The question you want to ask the AI.\n",
        "    Output:\n",
        "        str: The AI's answer based on the PDF file.\n",
        "    \"\"\"\n",
        "    # Step 1: Retrieve top-N chunks\n",
        "    retrieved = search_chunks(query, n_results=TOP_N_RESULTS)\n",
        "    context = \"\\n\\n\".join([r[\"text\"] for r in retrieved])\n",
        "\n",
        "    # Debug mode: show retrieved evidence\n",
        "    if DEBUG:\n",
        "        print(f\"\\nüîé DEBUG: Retrieved {len(retrieved)} chunks for query ‚Üí {query}\\n\")\n",
        "        for r in retrieved:\n",
        "            print(f\"üìå {r['id']} (distance={r['distance']:.4f})\")\n",
        "            print(r[\"text\"][:300].replace(\"\\n\", \" \") + \"...\\n\")\n",
        "\n",
        "    # Step 2: Build QA prompt\n",
        "    system_prompt = PROMPT\n",
        "\n",
        "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    # Step 3: Call GPT model\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        max_tokens=OUTPUT_LENGTH,\n",
        "        temperature=TEMPERATURE\n",
        "    )\n",
        "\n",
        "    # Step 4: Return the model's answer\n",
        "    return response.choices[0].message.content.strip()\n"
      ],
      "metadata": {
        "id": "GhBbP3s8imoU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Queries\n",
        "\n",
        "Use this cell to test your function with different queries.  \n",
        "The answers must be generated correctly based on the PDF file.  \n"
      ],
      "metadata": {
        "id": "3s4yDNiGjmV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üîç Example Queries for Testing\n",
        "# ================================\n",
        "\n",
        "queries = [\n",
        "    \"Which Google apps integrate with Gemini?\",\n",
        "    \"What are two benefits of using natural language in prompts?\",\n",
        "    \"How should executives use prompts differently than frontline workers?\",\n",
        "    \"What is the purpose of giving constraints in prompts?\",\n",
        "    \"–ö–∞–∫ —Å–µ –ø—Ä–∞–≤–∏ –±–æ–±–µ–Ω–∞ —á–æ—Ä–±–∞? –û—Ç –∫–æ–π –∏–∑—Ç–æ—á–Ω–∏–∫ –µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è—Ç–∞?\"\n",
        "]\n",
        "\n",
        "# Call the AI with each query\n",
        "for q in queries:\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {ask_ai(q)}\\n\")\n"
      ],
      "metadata": {
        "id": "dw5VE1YSjmHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a7d444-42a1-4cbd-ea18-92fed722af2b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Which Google apps integrate with Gemini?\n",
            "A: The Google apps that integrate with Gemini are Gmail, Google Docs, Google Sheets, Google Meet, Google Slides, and Gemini Advanced.\n",
            "\n",
            "Q: What are two benefits of using natural language in prompts?\n",
            "A: 1. It allows for clearer communication, making it easier for the AI to understand the user's intent.  \n",
            "2. It creates a more conversational tone, enhancing the interaction between the user and the AI.\n",
            "\n",
            "Q: How should executives use prompts differently than frontline workers?\n",
            "A: Executives should use prompts to integrate them into their daily tasks efficiently, focusing on strategic decision-making and urgent tasks while on the go. They may require prompts that are concise and tailored to their specific roles, whereas frontline workers might focus more on direct customer interactions and operational tasks.\n",
            "\n",
            "Q: What is the purpose of giving constraints in prompts?\n",
            "A: The purpose of giving constraints in prompts is to generate specific results by including details such as character count limits or the number of options you‚Äôd like to generate.\n",
            "\n",
            "Q: –ö–∞–∫ —Å–µ –ø—Ä–∞–≤–∏ –±–æ–±–µ–Ω–∞ —á–æ—Ä–±–∞? –û—Ç –∫–æ–π –∏–∑—Ç–æ—á–Ω–∏–∫ –µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è—Ç–∞?\n",
            "A: Not in the guide.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}